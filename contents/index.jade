---
template: layout.jade
---

- // Get month and year
- var d = new Date();
- var year = d.getFullYear();
- var months = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]; 
- var month = months[d.getMonth()];
- console.log(month)

.wrap#index
  h1.page-header From the phenomenology to the mechanisms of consciousness: Integrated Information Theory 3.0
  <p><span><strong>From the phenomenology to the mechanisms of consciousness: Integrated Information Theory 3.0</strong></span><br />Masafumi Oizumi<span class="math">\(^{1,2,\dagger}\)</span>, Larissa Albantakis<span class="math">\(^{1,\dagger}\)</span>, Giulio Tononi<span class="math">\(^{1,\ast}\)</span><br /><strong><span>1</span> Department of Psychiatry, University of Wisconsin, Madison, WI, USA<br /><strong><span>2</span> RIKEN Brain Science Institute, Wako-shi, Saitama, Japan<br /><strong><span><span class="math">\(\dagger\)</span></span> These authors equally contributed to this work.<br /><span class="math">\(\ast\)</span> E-mail: Corresponding gtononi@wisc.edu</strong></strong></strong></p>
  <h1 id="abstract" class="unnumbered">Abstract</h1>
  <p>This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific – it is what it is by how it differs from alternative experiences; integration says that it is unified – irreducible to non-interdependent components; exclusion says that it has definite borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ``differences that make a difference&quot; within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (<em>MICS</em>, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a <em>complex</em>. According to IIT, a MICS specifies the quality of an experience and its integrated information <span class="math">\(\Phi^{\rm Max}\)</span> its quantity. From the theory follow several results, including: a system of mechanisms may condense into a main complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true “zombies” – unconscious feed-forward systems that are functionally equivalent to conscious complexes.</p>
  <h1 id="author-summary" class="unnumbered">Author Summary</h1>
  <p>Integrated information theory (IIT) approaches the relationship between consciousness and its physical substrate by first identifying the fundamental properties of experience itself: composition, information, integration, and exclusion. IIT then postulates that the physical substrate of consciousness must satisfy these very properties. We develop a detailed mathematical framework in which composition, information, integration, and exclusion are defined precisely and made operational. This allows us to establish to what extent simple systems of mechanisms, such as logic gates or neuron-like elements, can form complexes that can account for the fundamental properties of consciousness. Based on this principled approach, we show that IIT can explain many known facts about consciousness and the brain, leads to specific predictions, and allows us to infer, at least in principle, both the quantity and quality of consciousness for systems whose causal structure is known. For example, we show that some simple systems can be minimally conscious, some complicated systems can be completely unconscious, and two different systems can be functionally equivalent, yet one is conscious and the other one is not.</p>
  <h1 id="introduction" class="unnumbered">Introduction</h1>
  <p>Understanding consciousness requires not only empirical studies of its neural correlates, but also a principled theoretical approach that can provide explanatory, inferential, and predictive power. For example, why is consciousness generated by the corticothalamic system – or at least some parts of it, but not by the cerebellum, despite the latter having even more neurons? Why does consciousness fade early in sleep, although the brain remains active? Why is it lost during generalized seizures, when neural activity is intense and synchronous? And why is there no direct contribution to consciousness from neural activity within sensory and motor pathways, or within neural circuits looping out of the cortex into subcortical structures and back, despite their manifest ability to influence the content of experience? Explaining these facts in a parsimonious manner calls for a theory of consciousness. (Below, consciousness, experience, and phenomenology are taken as being synonymous).</p>
  <p>A theory is also needed for making inferences in difficult or ambiguous cases. For example, is a newborn baby conscious, how much, and of what? Or an animal like a bat, a lizard, a fruit fly? In such cases, one cannot resort to verbal reports to establish the presence and nature of consciousness, or to the neural correlates of consciousness as established in healthy adults. The inadequacy of behavioral assessments of consciousness is also evident in many brain damaged patients, who cannot communicate, and whose brain may be working in ways that are hard to interpret. Is a clinically vegetative patient showing an island of residual, near-normal brain activity in just one region of the cortex conscious, how much, and of what? Or is nobody home? Or again, consider machines, which are becoming more and more sophisticated at reproducing human cognitive abilities and at interacting profitably with us. Some such machines can learn to categorize objects such as faces, places, animals, and so on, as well if not better than humans <span class="citation"></span>, or can answer difficult questions better than humans <span class="citation"></span>. Are such machines approaching our level of consciousness? If not, what are they missing, and what does it take to build a machine that is actually conscious? Clearly, only a theory - one that says what consciousness is and how it can be generated - can hope to offer a combination of explanatory, inferential, and predictive power starting from a few basic principles, and provide a way to quantify both the level of consciousness and its content.</p>
  <p>Integrated information theory (IIT) is an attempt to characterize consciousness mathematically both in quantity and in quality <span class="citation"></span>. IIT starts from the fundamental properties of the phenomenology of consciousness, which are identified as <em>axioms</em> of consciousness. Then, IIT translates these axioms into <em>postulates</em>, which specify which conditions must be satisfied by physical mechanisms, such as neurons and their connections, to account for the phenomenology of consciousness. It must be emphasized that taking the phenomenology of consciousness as primary, and asking how it can be implemented by physical mechanisms, is the opposite of the approach usually taken in neuroscience: start from neural mechanisms in the brain, and ask under what conditions they give rise to consciousness, as assessed by behavioral reports <span class="citation"></span>. While identifying the ``neural correlates of consciousness&quot; is undoubtedly important <span class="citation"></span>, it is hard to see how it could ever lead to a satisfactory explanation of what consciousness is and how it comes about <span class="citation"></span>.</p>
  <p>As will be illustrated below, IIT offers a way to analyze systems of mechanisms to determine if they are properly structured to give rise to consciousness, how much of it, and of which kind. As reviewed previously <span class="citation"></span>, the fundamental principles of IIT, such as integration and differentiation, can provide a parsimonious explanation for many neuroanatomical, neurophysiological, and neuropsychological findings concerning the neural substrate of consciousness. Moreover, IIT leads to experimental predictions, for instance that the loss and recovery of consciousness should be associated with the breakdown and recovery of information integration. This prediction has been confirmed using transcranial magnetic stimulation in combination with high-density electroencephalography in several different conditions characterized by loss of consciousness, such as deep sleep, general anesthesia obtained with several different agents, and in brain damaged patients (vegetative, minimally conscious, emerging from minimal consciousness, locked-in <span class="citation"></span>). Furthermore, IIT has inspired theoretically motivated measures of the level of consciousness that have been applied to human and animal data (e.g. <span class="citation"></span>).</p>
  <p>While the central assumptions of IIT have remained the same, its theoretical apparatus has undergone various developments over the years. The original formulation, which may be called IIT 1.0, introduced the essential notions including causal measures of the quantity and quality of consciousness. However, to simplify the analysis, IIT 1.0 dealt exclusively with stationary systems <span class="citation"></span> (see also <span class="citation"></span>). The next formulation, which will be called IIT 2.0 <span class="citation"></span> applied the same notions on a state-dependent basis: it showed how integrated information could be calculated in a top-down manner for a system of mechanisms in a state <span class="citation"></span> and suggested a way to characterize the quality of an experience by considering its sub-mechanisms <span class="citation"></span>. The formulation presented below, and the new results that follow from it, represent a substantial advance at several different levels, hence IIT 3.0 (see also <span class="citation"></span>). Nevertheless, this article is presented independently of previous “releases” for readers new to IIT. For those readers who may have followed the evolution of IIT, the main advances are summarized in the Supplementary Material (Text S1).</p>
  <p>In what follows, we first present the axioms and the postulates of IIT. We then provide the mathematical formalism and motivating examples for each of the postulates. The key constructs of IIT are introduced first at the level of individual mechanisms, which can be taken to represent physical objects such as logic gates or neurons, then at the level of systems of mechanisms, such as computers or neural architectures. The Theory and Models section ends by presenting the central identity proposed by IIT, according to which the quality and quantity of an experience is completely specified by a maximally irreducible conceptual structure (MICS) and the associated value of integrated information <span class="math">\(\Phi^{\rm Max}\)</span>. The Results and Discussion section presents several new results that follow directly from IIT, including the condensation of systems of mechanisms into main complexes and minor complexes; examples of simple systems that are minimally conscious and of complicated systems that are not; an example of an unconscious feed-forward system that is functionally equivalent to a conscious complex; and finally, an example showing that concepts within a complex are self-referential and relate only indirectly to the external environment.</p>
  <h1 id="theory-and-models" class="unnumbered">Theory and Models</h1>
  <h2 id="axioms-postulates-and-identities" class="unnumbered">Axioms, postulates, and identities</h2>
  <p>The main tenets of IIT can be presented as a set of phenomenological axioms, ontological postulates, and identities.</p>
  <h3 id="axioms" class="unnumbered">Axioms</h3>
  <p>The central axioms, which are taken to be immediately evident, are as follows:</p>
  <dl>
  <dt>Existence.</dt>
  <dd><p>Consciousness exists - it is an undeniable aspect of reality. Paraphrasing Descartes, “I experience therefore I am”.</p>
  </dd>
  <dt>Composition.</dt>
  <dd><p>Consciousness is structured: each experience consists of multiple aspects in various combinations. Within the same experience, one can see, for example, left and right, red and blue, a triangle and a square, a red triangle on the left, a blue square on the right, and so on.</p>
  </dd>
  <dt>Information.</dt>
  <dd><p>Consciousness is informative: each experience differs in its particular way from other possible experiences. Thus, an experience of pure darkness is what it is by differing, in its particular way, from an immense number of other possible experiences. A small subset of these possible experiences includes, for example, all the frames of all possible movies.</p>
  </dd>
  <dt>Integration.</dt>
  <dd><p>Consciousness is integrated: each experience is (strongly) irreducible to non-interdependent components. Thus, experiencing the word “SONO” written in the middle of a blank page is irreducible to an experience of the word “SO” at the right border of a half-page, plus an experience of the word “NO” on the left border of another half page – the experience is whole. Similarly, seeing a red triangle is irreducible to seeing a triangle but no red, plus a red patch but no triangle.</p>
  </dd>
  <dt>Exclusion.</dt>
  <dd><p>Consciousness is exclusive: each experience excludes all others - at any given time there is only one experience having its full content, rather than a superposition of multiple partial experiences; each experience has definite borders - certain things can be experienced and others cannot; each experience has a particular spatial and temporal grain - it flows at a particular speed, and it has a certain resolution such that some distinctions are possible and finer or coarser distinctions are not.</p>
  </dd>
  </dl>
  <h3 id="postulates" class="unnumbered">Postulates</h3>
  <p>To parallel the phenomenological axioms, IIT posits ontological postulates. These list the set of properties physical systems must satisfy in order to generate experience.</p>
  <dl>
  <dt>Existence.</dt>
  <dd><p>Mechanisms in a state exist. A system is a set of mechanisms.</p>
  </dd>
  <dt>Composition.</dt>
  <dd><p>Elementary mechanisms can be combined into higher order ones.</p>
  </dd>
  </dl>
  <p>The next three postulates, information, integration, and exclusion, apply first to individual mechanisms and then to systems of mechanisms.</p>
  <h3 id="mechanisms" class="unnumbered">Mechanisms</h3>
  <dl>
  <dt>Information.</dt>
  <dd><p>A mechanism can contribute to consciousness only if it specifies “differences that make a difference” within a system. That is, a mechanism in a state generates information only if it constrains the states of a system that can be its possible causes and effects – its <em>cause-effect repertoire</em>. The more selective the possible causes and effects, the higher the <em>cause-effect information cei</em> specified by the mechanism.</p>
  </dd>
  <dt>Integration.</dt>
  <dd><p>A mechanism can contribute to consciousness only if it specifies a cause-effect repertoire (information) that is <em>irreducible</em> to independent components. <em>Integration/irreducibility</em> <span class="math">\(\varphi\)</span> is assessed by partitioning the mechanism and measuring what difference this makes to its cause-effect repertoire.</p>
  </dd>
  <dt>Exclusion.</dt>
  <dd><p>A mechanism can contribute to consciousness at most one cause-effect repertoire - its <em>maximally irreducible</em> cause-effect repertoire (MICE, or <em>quale sensu stricto</em> (in the narrow sense of the word, <span class="citation"></span>)). If the MICE exists, the mechanism constitutes a <em>concept</em>.</p>
  </dd>
  </dl>
  <h3 id="systems-of-mechanisms" class="unnumbered">Systems of mechanisms</h3>
  <dl>
  <dt>Information.</dt>
  <dd><p>A set of elements can be conscious only if its mechanisms specify a set of “differences that make a difference” – i.e. a <em>conceptual structure</em>. A conceptual structure is a <em>constellation</em> of points in concept space, where each axis is a possible past/future state of the set of elements, and each point is a concept specifying differences that make a difference within the set. The higher the number of different concepts and their <span class="math">\(\varphi^{\rm Max}\)</span> value, the higher the <em>conceptual information CI</em> that specifies a particular constellation and distinguishes it from other possible constellations.</p>
  </dd>
  <dt>Integration.</dt>
  <dd><p>A set of elements can be conscious only if its mechanisms specify a conceptual structure that is <em>irreducible</em> to independent components (strong integration). <em>Strong integration/irreducibility</em> <span class="math">\(\Phi\)</span> is assessed by partitioning the set of elements into subsets with unidirectional cuts.</p>
  </dd>
  <dt>Exclusion.</dt>
  <dd><p>Of all overlapping sets of elements, only one set can be conscious – the one whose mechanisms specify a conceptual structure that is <em>maximally irreducible (MICS)</em> to independent components. A local maximum of integrated information <span class="math">\(\Phi^{\rm Max}\)</span> (over elements, space, and time) is called a <em>complex</em>.</p>
  </dd>
  </dl>
  <h3 id="identities" class="unnumbered">Identities</h3>
  <p>Finally, IIT posits identities between phenomenological aspects and informational/causal aspects of physical systems. The central identity is the following:</p>
  <p>The maximally irreducible conceptual structure (<em>MICS</em>) generated by a complex of elements is identical to its experience. The constellation of concepts of the MICS completely specifies the quality of the experience (its <em>quale ``sensu lato&quot;</em> (in the broad sense of the term <span class="citation"></span>)). Its irreducibility <span class="math">\(\Phi^{\rm Max}\)</span> specifies its quantity. The maximally irreducible cause-effect repertoire (MICE) of each concept within a MICS specifies what the concept is about (what it contributes to the quality of the experience, i.e. its <em>quale sensu stricto</em> (in the narrow sense of the term)), while its value of irreducibility <span class="math">\(\varphi^{\rm Max}\)</span> specifies how much the concept is present in the experience. An experience is thus an <em>intrinsic property</em> of a complex of mechanisms in a state. In other words, the maximally irreducible conceptual structure specified by a complex exists intrinsically (from its own intrinsic perspective), without the need for an external observer.</p>
  <h2 id="mechanisms-1" class="unnumbered">Mechanisms</h2>
  <p>In what follows, we consider simple systems that can be used to illustrate the postulates of IIT. In the first part, we apply the postulates of IIT at the level of <em>individual mechanisms</em>. We show that an individual mechanism generates information by specifying both selective causes and effects (information), that it needs to be irreducible to independent components (integration), and that only the most irreducible cause and effect of each mechanism should be considered (exclusion). This allows us to introduce the notion of a <em>concept</em>: the maximally irreducible cause-effect repertoire of a mechanism.</p>
  <p>In the next part, we consider the postulates of IIT at the level of <em>systems of mechanisms</em>, and show how the requirements for information, integration, and exclusion can be satisfied at the system level. This allows us to introduce the notion of a <em>complex</em> – a maximally integrated set of elements – and of a <em>quale</em> – the maximally irreducible conceptual structure (MICS) it generates. Altogether, these two sections show how to assess in a step by step, bottom up manner, whether a system generates a maximally integrated conceptual structure and how the latter can be characterized in full. A summary of the key concepts and associated measures is provided as a reference in Table 1.</p>
  <p>[!h]</p>
  <p><span>|l|l|</span> &amp;<br /><br /><br />&amp;<br /><strong>Cause-effect information (<span><em>cei</em></span>):</strong>&amp; <strong>Conceptual information (<span><em>CI</em></span>):</strong><br />How a mechanism in a state specifies &amp; How a set of mechanisms specifies<br />the probability of past and future states &amp; the constellation of concepts<br />of a set of elements (cause-effect repertoires) &amp; of the set (conceptual structure)<br />&amp;<br /><br /><br />&amp;<br /><strong>Integrated information (<span class="math">\(\varphi\)</span>, “small phi”):</strong> &amp; <strong>Integrated conceptual information (<span class="math">\(\Phi\)</span>, “big phi”):</strong><br />How irreducible the cause-effect repertoire &amp; How irreducible the conceptual structure<br />specified by a mechanism is compared to its &amp; specified by a set of mechanisms is compared to its<br />minimum information partition (MIP) &amp; minimum information partition (MIP)<br />&amp;<br /></p>
  <p><br /><br />&amp;<br /><strong>Concept (<span class="math">\( \varphi^{Max}\)</span>):</strong> &amp; <strong>Complex (<span class="math">\( \Phi^{Max}\)</span>):</strong><br />A mechanism that specifies &amp; A set of elements whose mechanisms specify<br />a maximally irreducible cause-effect repertoire &amp; a maximally irreducible conceptual structure<br />(MICE or quale ``sensu stricto“) &amp; (MICS or quale ``sensu lato”)<br />&amp;<br /></p>
  <h3 id="existence" class="unnumbered">Existence</h3>
  <p><embed src="Figures/IIT3_Fig1.pdf" /></p>
  <p>[fig:existence]</p>
  <p>The existence postulate, the “zeroth” postulate of IIT, claims that mechanisms in a state exist. Within the present framework, “mechanism” simply denotes anything having a causal role within a system, for example, a neuron in the brain, or a logic gate in a computer. In principle, mechanisms might be characterized at various spatio-temporal scales, down to the micro-physical level, although for any given system there will be a scale at which causal interactions are strongest <span class="citation"></span>. In what follows, we consider systems in which the elementary mechanisms are discrete logic gates or linear threshold units (Methods S1) and assume that these mechanisms are the ones mediating the strongest causal interactions.</p>
  <p>Fig. [fig:existence]A shows the example system <span class="math">\(ABCDEF\)</span>, which includes three logic gate mechanisms, OR, AND, XOR, which will be used to illustrate the postulates of IIT throughout the Model section. The dotted circle indicates that the particular set of elements <span class="math">\(ABC\)</span> is going to be considered as a “candidate set” for IIT analysis, whereas the remaining elements <span class="math">\(D\)</span>,<span class="math">\(E\)</span>,<span class="math">\(F\)</span> are considered external and treated as boundary conditions (Methods S1).</p>
  <p>The mechanisms of <span class="math">\(ABC\)</span> determine the transition probability matrix of the candidate set (TPM), which specifies the probability with which any state of the set <span class="math">\(ABC\)</span> transitions into any other state (Fig. [fig:existence]B). In this case, since the system is deterministic, the values in the TPM are 0 or 1, but non-deterministic systems can also be considered. In this example, at the current time step <span class="math">\(t_0\)</span>, the mechanisms are in state <span class="math">\(ABC=100\)</span>. The TPM specifies which past states could have led to the current state <span class="math">\(ABC=100\)</span> (the shaded column in Fig. [fig:existence]B) and which future states it could go to (shaded row in Fig. [fig:existence]B), out of all possible states of the set.</p>
  <h3 id="composition" class="unnumbered">Composition</h3>
  <p><embed src="Figures/IIT3_Fig2.pdf" /></p>
  <p>[fig:composition]</p>
  <p>Composition states that elementary mechanisms can be structured, forming higher order mechanisms in various combinations. In Fig. [fig:composition], <span class="math">\(A\)</span>, <span class="math">\(B\)</span>, and <span class="math">\(C\)</span> are the elementary (first-order) mechanisms. By combining them, higher order mechanisms can be constructed. Pairs of elements form second-order mechanisms (<span class="math">\(AB\)</span>, <span class="math">\(AC\)</span>, <span class="math">\(BC\)</span>), and all elements together form the third-order mechanism <span class="math">\(ABC\)</span>. A red area highlights the respective mechanisms in Fig. [fig:composition]. The elements inside the candidate set, but outside the mechanism under consideration, are treated as independent noise sources (Methods S1). Altogether, the elementary mechanisms and their combinations form the <em>power set</em> of possible mechanisms.</p>
  <h3 id="information-cause-effect-repertoires-and-cause-effect-information-cei" class="unnumbered">Information: cause-effect repertoires and cause-effect information (<span class="math">\(cei\)</span>)</h3>
  <p><embed src="Figures/IIT3_Fig3.pdf" /></p>
  <p>[fig:selective]</p>
  <p>In IIT, information is meant to capture the “differences that make a difference” from the perspective of the system itself – and is therefore both causal and intrinsic. These and other features distinguish this ``intrinsic“ notion of information from the ``extrinsic”, Shannon notion (see Text S2; cf. <span class="citation"></span> for related approaches to information and causation in networks).</p>
  <p>Information as “differences that make a difference” to a system from its intrinsic perspective can be quantified by considering how a mechanism in its current state <span class="math">\(s_0\)</span> constrains the system’s potential past and future states. Fig. [fig:selective] illustrates how a mechanism <span class="math">\(A\)</span> constrains the past states of <span class="math">\(BCD\)</span> more or less <em>selectively</em> depending on its input/output function and state. <span class="math">\(A\)</span> is an AND gate of the inputs from <span class="math">\(BCD\)</span>. The constrained distribution of past states is called <span class="math">\(A\)</span>’s <em>cause repertoire</em>. In Fig. [fig:selective]A the connections between <span class="math">\(A\)</span> and <span class="math">\(BCD\)</span> are substituted by noise. Therefore, the current state of <span class="math">\(A\)</span> cannot specify anything about the past state of <span class="math">\(BCD\)</span>, the cause repertoire is identical to the unconstrained distribution (unselective), and <span class="math">\(A\)</span> generates no information. By contrast, when the connections between <span class="math">\(A\)</span> and <span class="math">\(BCD\)</span> are deterministic and <span class="math">\(A\)</span> is on (<span class="math">\(A=1\)</span>), the past state of <span class="math">\(BCD\)</span> is fully constrained, since the only compatible past state is <span class="math">\(BCD=111\)</span> (Fig. [fig:selective]B). In this case, the cause repertoire is maximally selective, corresponding to high information. On the other hand, when <span class="math">\(A\)</span> is off (<span class="math">\(A=0\)</span>, Fig. [fig:selective]C), the cause repertoire is less selective, because only <span class="math">\(BCD=111\)</span> is ruled out, corresponding to less information.</p>
  <p><embed src="Figures/IIT3_Fig4.pdf" /></p>
  <p>[fig:information]</p>
  <p>Fig. [fig:information] illustrates how element <span class="math">\(A\)</span> in state 1 constrains the past states (left) and future states (right) of the candidate set <span class="math">\(ABC\)</span>. The probability distribution of past states that could have been potential causes of <span class="math">\(A=1\)</span> is its cause repertoire <span class="math">\(p(ABC^p|A^c=1)\)</span>. The probability distribution of future states that could be potential effects of <span class="math">\(A=1\)</span> is called <em>effect repertoire</em> <span class="math">\(p(ABC^f|A^c=1)\)</span>. Here, the superscripts <span class="math">\(^p\)</span>, <span class="math">\(^c\)</span>, and <span class="math">\(^f\)</span> stand for past, current, and future, respectively. The set of elements over which the cause and effect repertoires of a mechanism are calculated is called its <em>purview</em>. Fig. [fig:information] shows the cause/effect repertoire of mechanism <span class="math">\(A=1\)</span> over its purview <span class="math">\(ABC\)</span> (the full set) in the past and future, labeled <span class="math">\(A^c/ABC^p\)</span> and <span class="math">\(A^c/ABC^f\)</span>. If the purview is not over the full set, the elements outside of the purview are unconstrained (see Methods S1 for details on the calculation).</p>
  <p>The amount of information that <span class="math">\(A=1\)</span> specifies about the past, its cause information (<span class="math">\(ci\)</span>), is measured as the distance <span class="math">\(D\)</span> between the cause repertoire <span class="math">\(p(ABC^p|A^c=1)\)</span> and the unconstrained repertoire <span class="math">\(p^{\rm{uc}}\)</span>. For the purview <span class="math">\(ABC^p\)</span>:</p>
  <p><span class="math">\[ci(ABC^p|A^c=1) = D(p(ABC^p|A^c=1)|| p^{ {\rm uc}}(ABC^p)) = 0.33. \label{eq:ci}\]</span></p>
  <p><span class="math">\(p^{\rm{uc}}(ABC^p)\)</span> corresponds to the cause repertoire in the absence of any constraints on the set’s output states due to its mechanisms (unconstrained past distribution), which is the uniform distribution.</p>
  <p>Just like cause information (<span class="math">\(ci\)</span>), effect information (<span class="math">\(ei\)</span>) of <span class="math">\(A=1\)</span> is quantified as the distance between the effect repertoire of <span class="math">\(A\)</span> and the unconstrained future distribution <span class="math">\(p^{\rm{uc}}(ABC^f)\)</span>:</p>
  <p><span class="math">\[ei(ABC^f|A^c=1)  =  D(p(ABC^f|A^c=1)|| p^{{\rm uc}}(ABC^f)) = 0.25.\]</span></p>
  <p>As can be seen in Fig. [fig:information] (right), the unconstrained future repertoire <span class="math">\(p^{{\rm uc}}(ABC^f)\)</span> is not simply the uniform distribution of future system states. While <span class="math">\(p^{{\rm uc}}(ABC^p)\)</span> corresponds to the distribution of past system states with unconstrained outputs, <span class="math">\(p^{{\rm uc}}(ABC^f)\)</span> corresponds to the distribution of future system states with unconstrained inputs. Therefore, <span class="math">\(p^{{\rm uc}}(ABC^f)\)</span> is obtained by perturbing the inputs to each element into all possible states. As an example, the unconstrained future distribution of element <span class="math">\(A\)</span>, being an OR gate, is <span class="math">\(p(A=0) = 0.25\)</span> and <span class="math">\(p(A=1) = 0.75\)</span>, which is obtained by perturbing the inputs of <span class="math">\(A\)</span> into all possible states <span class="math">\([00, 10, 01, 11]\)</span>.</p>
  <p>To quantify differences that make a difference, the distance <span class="math">\(D\)</span> between two probability distributions is evaluated using the earth mover’s distance (EMD), which quantifies how much two distributions differ by taking into account the distance between system states (see Methods S1 for details). This is important because, from the intrinsic perspective of the system, it should make a difference if two system elements differ in their state, instead of just one.</p>
  <p>Finally, having calculated <span class="math">\(ci(ABC^p|A=1)\)</span> and <span class="math">\(ei(ABC^f|A=1)\)</span>, the total amount of <em>cause-effect information</em> (<span class="math">\(cei\)</span>) specified by <span class="math">\(A=1\)</span> over the purview <span class="math">\(A/ABC^{p,f}\)</span> is the minimum of its <span class="math">\(ci\)</span> and <span class="math">\(ei\)</span>:</p>
  <p><span class="math">\[cei(ABC^{p,f}|A = 1) = \min [ci(ABC^p|A = 1),ei(ABC^f|A = 1)] = 0.25.\]</span></p>
  <p>The motivation for choosing the minimum is illustrated in Fig. [fig:inputoutput]. First, consider an element that receives inputs from the system but sends no output to it (element <span class="math">\(A\)</span> in Fig. [fig:inputoutput]A). In this case, the state of element <span class="math">\(A\)</span> constrains the past states of the system – it has selective causes within it, but not its future states – it has no selective effects on it (what <span class="math">\(A\)</span> does makes no difference to the system). Put differently, while the state of element <span class="math">\(A\)</span> does convey information about the system’s past states from the perspective of an external observer, it does not do so from the intrinsic perspective of the system itself, because the system is not affected by <span class="math">\(A\)</span> (it cannot “observe” it).</p>
  <p>Similarly, consider an element that only outputs to the system but does not receive inputs from it, being controlled exclusively by external causes (element <span class="math">\(A\)</span> in Fig. [fig:inputoutput]B). In this case, the state of element <span class="math">\(A\)</span> constrains the future states of the system – it has selective effects on it, but not its past states – it has no selective causes within it (what the system might have done makes no difference to <span class="math">\(A\)</span>). Put differently, while the state of element <span class="math">\(A\)</span> does convey information about the system’s future states from the perspective of an external observer, it does not do so from the intrinsic perspective of the system, because the system cannot affect <span class="math">\(A\)</span> (it cannot “control” it).</p>
  <p><embed src="Figures/IIT3_Fig5.pdf" /></p>
  <p>[fig:inputoutput]</p>
  <h3 id="integration-irreducible-cause-effect-repertoires-and-integrated-information-varphi" class="unnumbered">Integration: irreducible cause-effect repertoires and integrated information (<span class="math">\(\varphi\)</span>)</h3>
  <p>At the level of an individual mechanism, the integration postulate says that only mechanisms that specify integrated information can contribute to consciousness. Integrated information is information that is generated by the whole mechanism above and beyond the information generated by its parts. This means that, with respect to information, the mechanism is irreducible. Similar to cause-effect information, integrated information <span class="math">\(\varphi\)</span> (“small phi”) is calculated as the distance <span class="math">\(D\)</span> between two probability distributions: the cause-effect repertoire specified by the whole mechanism is compared against the cause-effect repertoire of the partitioned mechanism. Of the many possible ways to partition a mechanism, integrated information is evaluated across the minimum information partition (MIP), the partition that makes the least difference to the cause and effect repertoires (in other words, the minimum “difference” partition). In Fig. [fig:integration] this is demonstrated for the <span class="math">\(3^{\rm rd}\)</span> order mechanism <span class="math">\(ABC\)</span>. The MIP for the purview <span class="math">\(ABC^c/ABC^p, ABC^f\)</span> is <span class="math">\(ABC^c/ABC^p \to AB^c/C^p \times C^c/AB^p\)</span> in the past and <span class="math">\(ABC^c/ABC^f \to ABC^c/AC^f \times [ ]/B^f\)</span> in the future, where <span class="math">\([ ]\)</span> denotes the empty set. The cause and effect repertoire specified by the partitioned mechanisms can be calculated as:</p>
  <p><span class="math">\(p(ABC^p|ABC^c=100 / {\rm MIP}) = p(C^p|AB^c=10) \times p(AB^p|C^c=0),\)</span></p>
  <p>and</p>
  <p><span class="math">\[p(ABC^f|ABC^c=100 / {\rm MIP}) = p(AC^f|ABC^c=100) \times p(B^f),\]</span></p>
  <p>where the connections between the parts are “injected” with independent noise (Methods S1).</p>
  <p>The distance <span class="math">\(D\)</span> between the cause-effect repertoire specified by the whole mechanism and its MIP is quantified again using the EMD, taken separately for the past and the future (cause and the effect repertoires):</p>
  <p><span class="math">\[\varphi_{ {\rm MIP} }(ABC^p|ABC^c=100) = D(p(ABC^p|ABC^c=100) || p(ABC^p|ABC^c=100 / {\rm MIP})) = 0.5,\]</span></p>
  <p><span class="math">\[\varphi_{ {\rm MIP} }(ABC^f|ABC^c=100) = D(p(ABC^f|ABC^c=100) || p(ABC^f|ABC^c=100 / {\rm MIP})) = 0.25,\]</span></p>
  <p>As with information, the total amount of integrated information of mechanism <span class="math">\(ABC\)</span> in its current state <span class="math">\(100\)</span> over the purview <span class="math">\(A/ABC^{p,f}\)</span> is the minimum of its past and future integrated information:</p>
  <p><span class="math">\[\varphi_{ {\rm MIP} }(ABC^{p,f}|ABC^c=100)= \min [\varphi_{ {\rm MIP} }(ABC^p|ABC^c = 100), \varphi_{ {\rm MIP} }(ABC^f|ABC^c = 100)] = 0.25,\]</span></p>
  <p>In what follows, integrated information <span class="math">\(\varphi\)</span> is always evaluated for the MIP, so the <span class="math">\(_{\rm MIP}\)</span> subscript is dropped for readability.</p>
  <p>According to IIT, mechanisms that do not generate integrated information do not exist from the intrinsic perspective of a system, as illustrated in Fig. [fig:integration<sub>m</sub>otivation]. Suppose that <span class="math">\(A\)</span> is a non-parity gate (<span class="math">\(A\)</span> turns on when the inputs are even) and <span class="math">\(B\)</span> is a majority gate (<span class="math">\(B\)</span> turns on when the majority of its inputs are on). If <span class="math">\(A\)</span> and <span class="math">\(B\)</span> have independent causes and independent effects as shown in Fig. [fig:integration<sub>m</sub>otivation]A, a higher order mechanism <span class="math">\(AB\)</span> cannot generate integrated information, since it is possible to partition <span class="math">\(AB\)</span>’s causes and effects without any loss of information. In this case, <span class="math">\(AB\)</span> does not exist intrinsically.</p>
  <p><embed src="Figures/IIT3_Fig6.pdf" /></p>
  <p>[fig:integration]</p>
  <p>Consider instead Fig. [fig:integration<sub>m</sub>otivation]B. Here, <span class="math">\(AB=11\)</span> specifies that all inputs had to be on in the past (‘All ON’), which goes above and beyond what is specified separately by <span class="math">\(A=1\)</span> (an even number of inputs was on) and by <span class="math">\(B=1\)</span> (the majority of inputs was on). On the effect side, there is an AND gate that takes inputs from both <span class="math">\(A\)</span> and <span class="math">\(B\)</span>, so the effect of <span class="math">\(AB=11\)</span> goes above and beyond the separate effects of <span class="math">\(A=1\)</span> and <span class="math">\(B=1\)</span>. Therefore, mechanism <span class="math">\(AB\)</span> exists from the intrinsic perspective of the system, in the sense that it plays an irreducible causal role: it picks up a difference that makes a difference to the system in a way that cannot be accounted for by its parts.</p>
  <p>By contrast, in Fig. [fig:integration<sub>m</sub>otivation]C mechanism <span class="math">\(AB\)</span> does not exist from the intrinsic perspective of the system, because the information ‘All ON’ as such does not make any difference to the future state of the system. Similarly, in Fig. [fig:integration<sub>m</sub>otivation]D, <span class="math">\(A=1\)</span> and <span class="math">\(B=1\)</span> do not specify an irreducible past cause for the irreducible future effect that the AND gate will be ON.</p>
  <p><embed src="Figures/IIT3_Fig7.pdf" /></p>
  <p>[fig:integration<sub>m</sub>otivation]</p>
  <h3 id="exclusion-a-maximally-irreducible-cause-effect-repertoire-mice-specified-by-a-subset-of-elements-a-concept" class="unnumbered">Exclusion: A maximally irreducible cause-effect repertoire (MICE) specified by a subset of elements (a concept)</h3>
  <p><embed src="Figures/IIT3_Fig8.pdf" /></p>
  <p>[fig:exclusion<sub>e</sub>xample]</p>
  <p>The exclusion postulate at the level of a mechanism says that a mechanism can have only one set of causes and effects – those that are maximally irreducible – other causes and effects are excluded. The <em>core cause</em> of a mechanism is its maximally irreducible cause repertoire. Consider for example mechanism <span class="math">\(BC=00\)</span> in Fig. [fig:exclusion<sub>e</sub>xample]. To find the core cause of <span class="math">\(BC\)</span>, one needs to evaluate <span class="math">\(\phi\)</span> for the whole power set of past purviews <span class="math">\(P = \left\lbrace A^p, B^p, C^p, AB^p, AC^p, BC^p, ABC^p \right\rbrace\)</span>. In this case, the purview <span class="math">\(BC^c/AB^p\)</span> has the highest value of <span class="math">\( \varphi^{{\rm Max}}(P|BC^c = 00) = 0.33\)</span>. The corresponding maximally irreducible cause repertoire is thus the core cause of <span class="math">\(BC=00\)</span>. The <em>core effect</em> is assessed in the same way: it is the maximally irreducible effect repertoire of a mechanism with <span class="math">\(\varphi^{\rm Max}(F|BC^c = 00)\)</span>, where <span class="math">\(F\)</span> denotes the power set of future purviews. A mechanism that specifies a <em>maximally irreducible causes and effects (MICE)</em> constitutes a <em>concept</em> or, for emphasis, a <em>core concept</em>.</p>
  <p><embed src="Figures/IIT3_Fig9.pdf" /></p>
  <p>[fig:exclusion<sub>p</sub>ostulate]</p>
  <p>To understand the motivation behind the exclusion postulate as applied to a mechanism, consider a neuron with several strong synapses and many weak synapses (Figure S1). From the intrinsic perspective of the neuron, any combination of synapses could be potential causes of firing, including “strong synapses”, “strong synapses plus some weak synapses”, and so on, eventually including the potential cause “all synapses”, ``all synapses plus stray glutamate receptors“, ``all synapses plus stray glutamate receptors plus cosmic rays affecting membrane channels”, and so on, rapidly escalating to infinite regress. The exclusion postulate requires, first, that only one cause exists. This requirement represents a causal version of Occam’s razor, saying in essence that ``causes should not be multiplied beyond necessity“, i.e. that causal superposition is not allowed <span class="citation"></span>. Thus, <em>only one</em> set of synapses can be the cause for the neuron’s firing and not, for example, <em>both</em> “strong synapses S1,S2” <em>and</em> “all synapses”. Second, the exclusion postulate requires that, from the intrinsic perspective of a mechanism in a system, the only cause be the maximally irreducible one. Recall that IIT’s information postulate is based on the intuition that, for something to exist, it must make a difference. By extension, something exists all the more, the more of a difference it makes. The integration postulate further requires that, for a whole to exist, it must make a difference above and beyond its partition, i.e. it must be irreducible. Therefore, the cause that must be singled out as the only one that exists, as dictated by the exclusion postulate, should be the one that, if eliminated by a partition, makes the most difference to the neuron’s output – that is, the one that is maximally irreducible. In Figure S1, for example, the maximally irreducible cause turns out to be “the strong synapses S1,S2”. Note that the exclusion postulate appears to fit with phenomenology also at the level of mechanisms. Thus, invariant concepts such as “chair”, or “apple” seem to exclude the accidental details of particular apples and chairs, but only reflect the ``core” concept. In neural terms, this would imply that the maximally irreducible cause-effect repertoire of the neurons underlying such invariant concepts is similarly restricted to their core causes and effects.</p>
  <p>The notion of a concept is illustrated in Fig. [fig:exclusion<sub>p</sub>ostulate] for mechanism <span class="math">\(A\)</span> of the logic gate candidate set <span class="math">\(ABC\)</span>. The core cause of <span class="math">\(A\)</span> is the cause repertoire of purview <span class="math">\(A^c/BC^p\)</span>; the core effect is the effect repertoire of <span class="math">\(A^c/B^f\)</span>. These purviews generate the maximal amount of integrated information over the whole power set of purviews in the past (<span class="math">\(P\)</span>) and future (<span class="math">\(F\)</span>), respectively. The amount of integrated information generated by concept <span class="math">\(A^c/BC^p,B^f\)</span> is again the minimum between past and future:</p>
  <p><span class="math">\[\varphi^{{\rm Max}}(A^c =1) = \min [\varphi^{{\rm Max}}(P|A^c=1), \varphi^{{\rm Max}}(F|A^c =1)] = 0.17.\]</span></p>
  <p>Each concept of a mechanism in a state is thus endowed with a maximally irreducible cause and effect repertoire (MICE), which specifies what the concept is about (its <em>quale ``sensu stricto&quot;</em>), and its particular <span class="math">\(\varphi^{{\rm Max}}\)</span> value, which quantifies its amount of integration or irreducibility.</p>
  <h2 id="systems-of-mechanisms-1" class="unnumbered">Systems of mechanisms</h2>
  <p>We now turn from the level of mechanisms to the level of a system of mechanisms, and apply the postulates of IIT with the objective of deriving the experience or <em>quale</em> generated by a system in a bottom up manner, from the set of all its concepts.</p>
  <h3 id="information-conceptual-structure-constellation-of-concepts-in-concept-space-and-conceptual-information-ci" class="unnumbered">Information: Conceptual structure (constellation of concepts in concept space) and conceptual information (<span class="math">\(CI\)</span>)</h3>
  <p>At the system level, the information postulate says that only sets of “differences that make a difference” matter (i.e. a constellations of concepts). Fig. [fig:core<sub>c</sub>oncepts] shows all the concepts specified by the candidate set <span class="math">\(ABC\)</span> (Fig. [fig:core<sub>c</sub>oncepts]A,B). Of all the possible mechanisms of the power set of <span class="math">\(ABC\)</span>, only <span class="math">\(AC\)</span> does not give rise to a concept, since its integrated information <span class="math">\(\varphi^{\rm Max}=0\)</span> (Fig. [fig:core<sub>c</sub>oncepts]B). All other mechanisms generate non-zero integrated information and thus specify concepts (Fig. [fig:core<sub>c</sub>oncepts]C). The set of all concepts of a candidate set constitutes its <em>conceptual structure</em>, which can be represented in <em>concept space</em>.</p>
  <p><embed src="Figures/IIT3_Fig10.pdf" /></p>
  <p>[fig:core<sub>c</sub>oncepts]</p>
  <p>Concept space is a high dimensional space, with one axis for each possible past and future state of the system. In this space, each concept is symbolized as a point, or “star”: its coordinates are given by the probability of past and future states in its cause-effect repertoire, and its size is given by its <span class="math">\( \varphi^{{\rm Max}}(P,F|s_0)\)</span> value. If <span class="math">\(\varphi^{{\rm Max}}\)</span> is zero, the concept simply does not exist, and if its <span class="math">\( \varphi^{{\rm Max}}\)</span> is small, it exists to a minimal amount.</p>
  <p>In the case of the candidate set <span class="math">\(ABC\)</span>, the dimension of concept space is 16 (8 axes for the past states and 8 for the future states). For ease of representation, in the figures past and future subspaces are plotted separately, with only three axes each (corresponding to the states at which the concepts have the highest variance in probability). Therefore the 6 concepts in Fig. [fig:core<sub>c</sub>oncepts]D are displayed twice, once in the past subspace and once in the future subspace. In the full 16-dimensional concept space, however, each concept is a single star.</p>
  <p><embed src="Figures/IIT3_Fig11.pdf" /></p>
  <p>[fig:CIcalculation]</p>
  <p>At the system level, the equivalent of the cause-effect information (<span class="math">\(cei\)</span>) at the level of mechanisms is called conceptual information (<span class="math">\(CI\)</span>). Just like <span class="math">\(cei\)</span>, <span class="math">\(CI\)</span> is quantified by the distance <em><span class="math">\(D\)</span></em> from the unconstrained distribution of past and future states <span class="math">\(p^{uc}\)</span>, which corresponds to the ``null&quot; concept (a concept that specifies nothing):</p>
  <p><span class="math">\[CI(C|ABC^c=100) = D(C|ABC^c=100)\|p^{uc}(ABC^{p,f})) = 2.11.\]</span></p>
  <p>The distance <em><span class="math">\(D\)</span></em> from a constellation <span class="math">\(C\)</span> to the ``null&quot; concept can be measured using an extension of the EMD (see Methods S1), which can be understood as the cost of transporting the amount of <span class="math">\( \varphi^{{\rm Max}}\)</span> of each concept from its location in concept space to <span class="math">\(p^{uc}\)</span>. <span class="math">\(CI\)</span> is thus the sum of the distances between the cause-effect repertoire of each concept and <span class="math">\(p^{uc}\)</span>, multiplied by the concept’s <span class="math">\( \varphi^{{\rm Max}}\)</span> value (Fig. [fig:CIcalculation]). Thus, a rich constellation with many different elementary and higher order concepts generates a high amount of conceptual information <span class="math">\(CI\)</span> (Fig. [fig:CIcalculation]A). By contrast, a system comprised of a single elementary mechanism generates a minimal amount of conceptual information (Fig. [fig:CIcalculation]B).</p>
  <p>In sum, concepts are considered (metaphorically) as stars in concept space. The conceptual structure <span class="math">\(C\)</span> generated by a set of mechanisms is thus a constellation of concepts - a particular shape in concept space spanned by the set’s concepts. The more stars, the further away they are from the ``null&quot; concept, and the larger their size, the greater the conceptual information <em>CI</em> generated by the constellation <span class="math">\(C\)</span>.</p>
  <h3 id="integration-irreducible-conceptual-structure-and-integrated-conceptual-information-phi" class="unnumbered">Integration: irreducible conceptual structure and integrated conceptual information (<span class="math">\(\Phi\)</span>)</h3>
  <p><embed src="Figures/IIT3_Fig12.pdf" /></p>
  <p>[fig:big<sub>p</sub>hi<sub>M</sub>IP]</p>
  <p>At the system level, the integration postulate says that only conceptual structures that are integrated can give rise to consciousness. As for mechanisms, the integration or irreducibility of the constellation of concepts <span class="math">\(C\)</span> specified by a set of mechanisms can be assessed by partitioning a set of elements and measuring <em>integrated conceptual information</em> <span class="math">\(\Phi\)</span> as the difference made by the partition (“big phi”, as opposed to “small phi” <span class="math">\(\varphi\)</span> at the level of mechanisms).</p>
  <p>Partitioning at the system level amounts to noising the connections from one subset <span class="math">\(S1\)</span> of <span class="math">\(S\)</span> to its complement <span class="math">\(S\setminus S1\)</span>. As for mechanisms, whether and how much the constellation of concepts generated by a set of mechanisms is irreducible can be assessed with respect to the minimum information partition (MIP) of the set of elements <span class="math">\(S\)</span>. This corresponds to the unidirectional partition that makes the least difference to the constellation of concepts (in other words, the minimum “difference” partition; Fig. [fig:big<sub>p</sub>hi<sub>M</sub>IP]). To find the unidirectional MIP, for each subset <span class="math">\(S1\)</span> one must evaluate both the connections from <span class="math">\(S1\)</span> to <span class="math">\(S\setminus S1\)</span> <span><em>and</em></span> the connections from <span class="math">\(S\setminus S1\)</span> to <span class="math">\(S1\)</span> and take the minimum MIP. This corresponds, at the level of mechanisms, to finding the minimum of the MIPs with respect to the cause <span><em>and</em></span> the effect repertoires. Therefore a set of elements <span class="math">\(S\)</span> and its associated constellation is integrated if and only if each subset of elements specifies both selective causes and selective effects about its complement in <span class="math">\(S\)</span>. Similar to integrated information <span class="math">\(\varphi\)</span> for a mechanism, integrated conceptual information <span class="math">\(\Phi\)</span> for a set of elements is defined as the distance <em>D</em> between the constellation of the whole set and that of the partitioned set:</p>
  <p><span class="math">\[\Phi_{\rm MIP}(C|s_0) = D(C\| C_{\rm MIP}^{\rightarrow}),\]</span></p>
  <p>where <span class="math">\(C_{\rm MIP}^\rightarrow\)</span> denotes the constellation of the partitioned set of elements.</p>
  <p>The extended EMD between the whole and the partitioned constellation corresponds to the minimal cost of transforming <span class="math">\(C\)</span> into <span class="math">\(C_{\rm MIP}^{\rightarrow}\)</span> in concept space. Through the partition, concepts of <span class="math">\(C\)</span> may change location, lose <span class="math">\(\varphi^{{\rm Max}}(P,F|s_0)\)</span>, or disappear. Their <span class="math">\(\varphi^{{\rm Max}}(P,F|s_0)\)</span> has to be allocated to fill the concepts in <span class="math">\(C_{\rm MIP}^{\rightarrow}\)</span> with an associated cost of transportation that is proportional to the distance in concept space and the amount of <span class="math">\(\varphi^{{\rm Max}}\)</span> that is moved. Any residual <span class="math">\( \varphi^{{\rm Max}}\)</span> is transported to the ``null&quot; concept (<span class="math">\(p^{uc}\)</span>) under the same cost of transportation.</p>
  <p>Fig. [fig:big<sub>p</sub>hi<sub>M</sub>IP] shows the conceptual structure for the candidate system <span class="math">\(ABC\)</span> and its MIP (see Methods S1 for a calculation of <span class="math">\(\Phi_{\rm MIP}(C(ABC)|100)\)</span>). In this case, 4 of the 6 concepts of <span class="math">\(ABC\)</span> are lost through the partition; their <span class="math">\( \varphi^{{\rm Max}}(P,F|s_0)\)</span> is thus transported to the location of the ``null&quot; concept (<span class="math">\(p^{uc}\)</span>). Since <span class="math">\(\Phi\)</span> will always be evaluated over the MIP, in what follows the subscript <span class="math">\(_{\rm MIP}\)</span> is dropped, as it was for <span class="math">\(\varphi\)</span>.</p>
  <p><embed src="Figures/IIT3_Fig13.pdf" /></p>
  <p>[fig:integration<sub>s</sub>ystem]</p>
  <p>The motivation for integration at the system level is illustrated in Fig. [fig:integration<sub>s</sub>ystem] (as was done for mechanisms in Fig. [fig:integration]). The set of 6 elements shown in Fig. [fig:integration<sub>s</sub>ystem]A can be subdivided into two independent subsets of 3 elements, each with its independent set of concepts. Therefore, a minimum partition between the two subsets makes no difference and integrated conceptual information <span class="math">\(\Phi = 0\)</span>. Since the set is reducible without any loss, it does not exist intrinsically - it can only be treated as ``one“ system from the extrinsic perspective of an observer. By contrast, the set in Fig. [fig:integration<sub>s</sub>ystem]B is irreducible because each part specifies both causes and effects in the other part. Two other possibilities are that a subset specifies causes, but not effects, in the rest of the set (Fig.[fig:integration<sub>s</sub>ystem]C), or only effects, but not causes (Fig. [fig:integration<sub>s</sub>ystem]D). In the case of unidirectional connections the subset is integrated ``weakly” rather than ``strongly“ (in analogy with weak and strong connectedness in graph theory, e.g. <span class="citation"></span>). Therefore, the subset is not really an ``integral” part of the set, but merely an ``appendix“. As an analogy, take the executive board of a company. An employee who transcribes the recording of a board meeting is obviously affected by the board, but if he has no way to provide any feed-back, he should not be considered an ``integral” part of the board, which has no way of knowing that he exists and what he does. The same obtains for an employee who prints the agenda for the board meeting, if the board has no way of giving him feedback about the agenda.</p>
  <h3 id="exclusion-a-maximally-irreducible-conceptual-structure-mics-specified-by-a-set-of-elements-a-complex" class="unnumbered">Exclusion: a maximally irreducible conceptual structure (MICS) specified by a set of elements (a complex)</h3>
  <p>The exclusion postulate at the level of systems of mechanisms says that only a conceptual structure that is <span><em>maximally</em></span> irreducible can give rise to consciousness – other constellations generated by overlapping elements are excluded. A <em>complex</em> is thus defined as a set of elements within a system of mechanisms that generates a local maximum of integrated conceptual information <span class="math">\(\Phi^{\rm Max}\)</span> (meaning that it has maximal <span class="math">\(\Phi\)</span> as compared to all overlapping sets of elements). Only a complex exists as an entity from the intrinsic perspective. Consequently, complexes cannot overlap and at each point in time, an element/mechanism can belong to one complex only (complexes should be evaluated as maxima of integrated information not only over elements, but also over spatial and temporal grains <span class="citation"></span>, but here it is assumed that the binary elements and time intervals considered in the examples are optimal). Once a complex has been identified, concept space can be called “<em>qualia space</em>,” and the constellation of concepts can be called a ``<em>quale ‘sensu lato’</em>&quot;. A quale in the broad sense of the word is therefore a <em>maximally irreducible conceptual structure (MICS)</em> or, alternatively, an <em>integrated information structure</em>.</p>
  <p>To determine whether an integrated set of elements is a complex, <span class="math">\(\Phi\)</span> must be evaluated for all possible candidate sets (subsets of the system) (Fig. [fig:complex]). As mentioned above, when a set of elements within the system is assessed, the other elements are treated as boundary conditions (see Methods S1). Fig. [fig:complex] shows the values of <span class="math">\(\Phi(C|s_0)\)</span> for all possible candidate sets that are subsets of <span class="math">\(ABC\)</span> (<span class="math">\(AB\)</span>,<span class="math">\(AC\)</span>,<span class="math">\(BC\)</span>,<span class="math">\(ABC\)</span>) and for one superset (<span class="math">\(ABCD\)</span>). The latter, and all other sets that include elements <span class="math">\(D\)</span>, <span class="math">\(E\)</span>, or <span class="math">\(F\)</span>, have <span class="math">\(\Phi=0\)</span>. This is because <span class="math">\(D\)</span>, <span class="math">\(E\)</span>, and <span class="math">\(F\)</span> are not strongly integrated with the rest of the system. Single elements are not taken into account as candidate sets since they cannot be partitioned and thus cannot be complexes by definition. In this example, the set of elements <span class="math">\(ABC\)</span> generates the highest value of <span class="math">\(\Phi^{\rm Max}\)</span> and is therefore the complex. By the exclusion postulate (“of all overlapping sets of elements, only one set can be conscious”), only <span class="math">\(ABC\)</span> “exists” intrinsically, and other overlapping sets of elements within the system cannot ``exist&quot; intrinsically at the same time (they are excluded).</p>
  <p><embed src="Figures/IIT3_Fig14.pdf" /></p>
  <p>[fig:complex]</p>
  <h3 id="identity-between-an-experience-and-a-maximally-irreducible-conceptual-structure-mics-or-quale-sensu-lato-generated-by-a-complex" class="unnumbered">Identity between an experience and a maximally irreducible conceptual structure (MICS or quale ``sensu lato&quot;) generated by a complex</h3>
  <p>The notions and measures related to the information, integration, and exclusion postulates, both at the level of mechanisms and at the level of systems of mechanisms, are summarized in Table 1. On this basis, it is possible to formulate the central identity proposed by IIT: <em>an experience is identical with the maximally irreducible conceptual structure (MICS, integrated information structure, or quale ``sensu lato&quot;) specified by the mechanisms of a complex in a state</em>. Subsets of elements within the complex constitute the concepts that make up the MICS. The maximally irreducible cause-effect repertoire (MICE) of each concept specifies what the concept is about (what it contributes to the quality of the experience, i.e. its <em>quale ``sensu stricto&quot;</em> (in the narrow sense of the term)). The value of irreducibility <span class="math">\(\varphi^{\rm Max}\)</span> of the concepts specifies how much the concept is present in the experience. An experience (i.e. consciousness) is thus an <em>intrinsic property</em> of a complex of elements in a state: how they constrain – in a compositional manner – its space of possibilities, in the past and in the future.</p>
  <p>In Fig. [fig:FancyQuale], this identity is illustrated by showing an isolated system of physical mechanisms <span>ABC</span> in a particular state (bottom left). The above analysis allows one to determine that in this case the system does constitute a complex, and that it specifies a MICS or quale (top right). As before, the constellation of concepts in qualia space is plotted over 3 representative axes separately for past and future states of the system. For clarity, the concepts are also represented as probability distributions over all 16 past and future states (cause-effect repertoires, bottom right).</p>
  <p>The central identity of IIT can also be formulated to express the classic distinction between <em>level</em> and <em>content</em> of consciousness <span class="citation"></span>: the quantity or level of consciousness corresponds to the <span class="math">\(\Phi^{{\rm Max}}\)</span> value of the quale; the quality or content of the experience corresponds to the particular constellation of concepts that constitutes the quale - a particular shape in qualia space. Note that, by specifying the quality of an experience, the particular shape of each constellation also distinguishes it from other possible experiences, just like the particular shape of a tetrahedron is what makes it a tetrahedron and distinguishes it from a cube, an icosahedron, and so on.</p>
  <p>As indicated by the figure, once a phenomenological analysis of the essential properties (axioms) of consciousness has been translated into a set of postulates that the physical mechanisms generating consciousness must satisfy, it becomes possible to invert the process: One can now ask, for any set of physical mechanisms, whether it is associated with phenomenology (is there ``something it is like to be it,&quot; from its own intrinsic perspective), how much of it (the quantity or level of consciousness), and of which kind (the quality or content of the experience). As also indicated by the figure, these phenomenological properties should be considered as intrinsic properties of physical mechanisms arranged in a certain way, meaning that a complex of physical mechanisms in a certain state is necessarily associated with its quale.</p>
  <p><embed src="Figures/IIT3_Fig15.pdf" /></p>
  <p>[fig:FancyQuale]</p>
  <h1 id="results-and-discussion" class="unnumbered">Results and Discussion</h1>
  <p>The Theory and Models section presented a way of constructing the experience or quale generated by a system of mechanisms in a state in a step-by-step, bottom up manner. The next section explores several implications of the postulates and concepts introduced above using example systems of mechanisms and the conceptual structures they generate.</p>
  <h2 id="a-system-may-condense-into-a-main-complex-and-several-minor-complexes" class="unnumbered">A system may condense into a main complex and several minor complexes</h2>
  <p><embed src="Figures/IIT3_Fig16.pdf" /></p>
  <p>[fig:minor<sub>c</sub>omplex]</p>
  <p>In Fig. [fig:minor<sub>c</sub>omplex], the previous example system <span class="math">\(ABC\)</span> has been embedded within a larger network. In the larger system, elements <span class="math">\(I\)</span>, <span class="math">\(J\)</span>, and <span class="math">\(L\)</span> cannot be a part of the complex because they lack either inputs or outputs, or both. <span class="math">\(H\)</span> and <span class="math">\(K\)</span> also cannot be part of the complex, since they are connected to the rest of the system in a strictly feed-forward manner. Nevertheless, elements <span class="math">\(H\)</span> and <span class="math">\(K\)</span> act as boundary conditions for the rest of the system. The remaining elements <span class="math">\(ABCDEFG\)</span> cannot form a complex as a whole, since the subset of elements <span class="math">\(FG\)</span> is not connected to the rest of the system. The subset of elements <span class="math">\(ABCDE\)</span> does generate a small amount of integrated conceptual information <span class="math">\(\Phi\)</span> and could thus potentially form a complex. Among the power set of elements <span class="math">\(ABCDE\)</span>, however, it is the smaller subset <span class="math">\(ABC\)</span> that generates the local maximum of <span class="math">\(\Phi^{\rm Max}\)</span>. This excludes <span class="math">\(ABCDE\)</span> from being a complex, since an element can participate to only one complex at each point in time. The remaining elements <span class="math">\(DE\)</span>, however, can still form a <em>minor complex</em>, with lower <span class="math">\(\Phi^{{\rm Max}}\)</span> than <span class="math">\(ABC\)</span>. Thus, <span class="math">\(ABCDE\)</span> condenses down to the major complex <span class="math">\(ABC\)</span>, the minor complex <span class="math">\(DE\)</span>, and their residual interactions. Finally, <span class="math">\(FG\)</span> forms a minor complex that does not interact with the rest of the system.</p>
  <p>This simple example of ``condensation“ into major and minor complexes may be relevant also for much more complicated systems of interconnected elements. For example, IIT predicts that in the human brain there should be a dominant ``main” complex of high <span class="math">\(\Phi^{{\rm Max}}\)</span>, constituted of neural elements within the cortical system, which satisfies the postulates described above and generates the changing qualia of waking consciousness <span class="citation"></span>. The set of neuronal elements constituting this main complex is likely to be dynamic <span class="citation"></span>, at times including and at times excluding particular subsets of neurons. Through its interface elements (called “ports-in” and “ports-out”), this main complex receives inputs and provides outputs to a vast number of smaller systems involved in parsing inputs and planning and executing outputs. While interacting with the main complex in both directions, many of these smaller systems may constitute minor complexes specifying little more than a few concepts, which would qualify them as ``minimally conscious&quot; (see below). In the healthy, adult human brain the qualia and <span class="math">\(\Phi^{\rm Max}\)</span> generated by the dominant main complex are likely to dwarf those specified by the minimally conscious minor complexes. In addition to the fully conscious main complex and minimally conscious minor complexes, there will be a multitude of unconscious processes mediated by purely feed-forward systems (see below) or by the residual interactions between main complex and minor complexes, as in Fig. [fig:minor<sub>c</sub>omplex].</p>
  <p>Under special circumstances, such as after split brain surgery, the main complex may split into two main complexes, both having high <span class="math">\(\Phi^{{\rm Max}}\)</span>. There is solid evidence that in such cases consciousness itself splits in two individual consciousnesses that are unaware of each other <span class="citation"></span>. A similar situation may occur in dissociative and conversion disorders, where splits of the main complex may be functional and reversible rather than structural and permanent <span class="citation"></span>.</p>
  <p>An intriguing dilemma is posed by behaviors that would seem to require a substantial amount of cognitive integration, such as semantic judgments (e.g. <span class="citation"></span>). Such behaviors are usually assumed to be mediated by neural systems that are unconscious, because they can be shown to occur under experimental conditions, such as continuous flash suppression, where the speaking subject is not aware of them and cannot report about them. If such behaviors were carried out in a purely feed-forward manner, they would indeed qualify as unconscious in IIT (see below). However, at least some of these behaviors may constitute the output of minor complexes separated from the main one. According to IIT such minor complexes, if endowed with non-trivial values of <span class="math">\(\Phi^{{\rm Max}}\)</span>, should be considered <em>paraconscious</em> (i.e. conscious ``on the side&quot; of the conscious subject) rather than unconscious. In principle, the presence of paraconscious minor complexes could be demonstrated by developing experimental paradigms of dual report.</p>
  <p>In brains substantially different from ours many other scenarios may occur. For example, the nervous system of highly intelligent invertebrates such as the octopus contains a central brain as well as large populations of neurons distributed in the nerve cords of its arms. It is an open question whether such a brain would give rise to a large, distributed main complex, or to multiple major complexes that generate separate consciousnesses. Similar issues apply to systems composed of non-neural elements, such as ant colonies, computer architectures, and so on. While determining rigorously how such systems condense in terms of major and minor complexes, and what kind of MICS they may generate, is not practically feasible, the predictions of IIT are in principle testable and should lead to definite answers.</p>
  <h2 id="consciousness-and-connectivity-modular-homogeneous-and-specialized-networks" class="unnumbered">Consciousness and connectivity: modular, homogeneous, and specialized networks</h2>
  <p>Whether a set of elements as a whole constitutes a complex or decomposes into several complexes depends first of all on the connectivity among its elementary mechanisms. In Fig. [fig:examples] we show the complexes and the associated MICS of three simple networks, representative of a modular, homogeneous, and specialized system architecture.</p>
  <p>Fig. [fig:examples]A (top) shows a ``modular&quot; network of 3 COPY (<span class="math">\(ACE\)</span>) and 3 AND (<span class="math">\(BDF\)</span>) logic gates. In this network, the system as a whole is not a complex, despite being integrated due to the presence of inter-connections among all elements. Instead, each of the three modules (<span class="math">\(AB\)</span>, <span class="math">\(CD\)</span>, and <span class="math">\(EF\)</span>) that consist of 1 COPY and 1 AND gate constitutes a complex, because each generates more <span class="math">\(\Phi\)</span> than the whole system, although each module has just two concepts. The purviews of module <span class="math">\(AB\)</span>’s concepts are shown in Fig. [fig:examples]A (middle), and their representation in qualia space is displayed in Fig. [fig:examples]A (bottom).</p>
  <p>Fig. [fig:examples]B shows a ``homogeneous&quot; network of 5 OR gates (<span class="math">\(ABCDE\)</span>), in which every element is connected to every other element including itself. Since all elements in the network specify the same cause-effect repertoire, their 5 <span class="math">\(1^{st}\)</span> order (elementary) concepts are identical. Moreover, there are no higher order concepts, since combining elements yields nothing above the cause-effect information generated by each elementary mechanism. In qualia space, the 5 identical concepts are concentrated on a single point (Fig. [fig:examples]B, bottom). Accordingly, the homogeneous network has a low value of CI and <span class="math">\(\Phi^{\rm Max}\)</span>.</p>
  <p>Fig. [fig:examples]C shows a ``specialized&quot; network consisting of 5 majority gates, which turn on when the majority of inputs is on. However, each gate has only 3 afferent and efferent connections, which differ for every element. Therefore, each elementary concept specifies a different cause-effect repertoire. For the same reason, there are many higher order concepts (all but the highest order concept of the power set). The specialized network thus gives rise to a rich constellation in qualia space (Fig. [fig:examples]C, bottom) with a high value of CI and <span class="math">\(\Phi^{\rm Max}\)</span>.</p>
  <p>The example in Fig. [fig:examples]A showing that a network can be interconnected, either directly or indirectly, yet condense into a number of mini-complexes of low <span class="math">\(\Phi^{\rm Max}\)</span> if its architecture is primarily modular, is potentially consistent with neuropsychological evidence. As mentioned in the Introduction, the cerebellum is a paramount example of a complicated neuronal network, comprising even more neurons than the cerebral cortex, that does not give rise to consciousness or contribute to it <span class="citation"></span>. This paradox could be explained by its anatomical and physiological organization, which seems to be such that small cerebellar modules process inputs and produce outputs largely independent of each other <span class="citation"></span>. By contrast, a prominent feature of the cerebral cortex, which instead can generate consciousness, is that it is comprised of elements that are functionally specialized and at the same time can interact rapidly and effectively <span class="citation"></span>. This is the kind of organization that yields a comparatively high value of <span class="math">\(\Phi^{\rm Max}\)</span> in the simple example of Fig.[fig:examples]C. Finally, the example in Fig.[fig:examples]B, where connections are abundant but are organized in a homogeneous manner, may also have neurobiological counterparts. For instance, during deep slow wave sleep or in certain states of general anesthesia, the interactions among different cortical regions become highly stereotypical. Due to the characteristic bistability between on and off states of most neurons in the cerebral cortex, even though the anatomical connectivity is unchanged, functional and effective connectivity become virtually homogeneous <span class="citation"></span>. Under such conditions, consciousness invariably fades <span class="citation"></span>. The examples of Fig.[fig:examples]B and C also suggest that both the richness of concepts and the level of consciousness should increase with the refinement of cortical connections during neural development and the associate increase in functional specialization (e.g. <span class="citation"></span>).</p>
  <p><embed src="Figures/IIT3_Fig17.pdf" /></p>
  <p>[fig:examples]</p>
  <h2 id="consciousness-and-activity-inactive-systems-can-be-conscious" class="unnumbered">Consciousness and activity: inactive systems can be conscious</h2>
  <p>The conceptual structure generated by a complex depends not only on the connectivity among its elements and the input/output function they perform, but also on their current state. An important corollary of IIT is that both active and inactive elements can contribute to its conceptual structure. In general, elements that are on (1) specify ``positive“ elementary concepts, whereas elements that are off (0) specify ``negative” elementary concepts, both of which contribute to the shape of the MICS. Moreover, high-order concepts will often be specified by subsets including both active and inactive elements.</p>
  <p>In Fig. [fig:physiol], the system ABCD, comprised of 4 COPY gates, illustrates that a set of elements can form a complex and specify a MICS even though <span><em>all</em></span> of its elements are in state ‘0’ (off). This is because inactive elements, too, can selectively constrain past and future states of the system (as opposed to “inactivated” or non-functional elements, which cannot change state and thus cannot generate information). For example, element <span class="math">\(A=0\)</span> specifies an irreducible cause (<span class="math">\(D\)</span> had to be off at <span class="math">\(t_{-1}\)</span>) and an irreducible effect (<span class="math">\(B\)</span> will be on at <span class="math">\(t_{+1}\)</span>) within the complex. Thus, IIT predicts that, even if all the neurons in a main complex were inactive (or active at a low baseline rate), they would still generate consciousness as long as they are ready to respond to incoming spikes. An intriguing possibility is that a neurophysiological state of near-silence may be approximated through certain meditative practices that aim at reaching a state of ``pure“ awareness without content <span class="citation"></span>. This corollary of IIT contrasts with the common assumption that neurons can only contribute to consciousness if they are active in such a way that they can ``signal” or ``broadcast“ the information they represent and ``ignite” fronto-parietal networks <span class="citation"></span>. This is because, in IIT, information is not in the message that is broadcasted by an element, but in the shape of the MICS that is specified by a complex.</p>
  <p>Another corollary of IIT that is relevant to neuroscience is that it is not necessary for the firing state of neurons to percolate or be ``broadcasted“ globally through the entire system generating consciousness for it to contribute to experience. For example, in the system in Fig. [fig:physiol], element <span class="math">\(A\)</span> does not connect directly to element <span class="math">\(C\)</span>. As a consequence, the activity (or inactivity) of <span class="math">\(A\)</span> cannot affect <span class="math">\(C\)</span>, and vice versa, within one time step. Nevertheless, <span class="math">\(ABCD\)</span> still forms a complex and gives rise to a MICS at time <span class="math">\(t_0\)</span>. Thus, according to IIT, the activation or deactivation of a neuron (over the time scale at which integrated information reaches a maximum <span class="citation"></span>) can modify an experience as long as it affects the shape of the MICS specified by the complex to which the neuron belongs, without requiring any global ``broadcast” of signals.</p>
  <p><embed src="Figures/IIT3_Fig18.pdf" /></p>
  <p>[fig:physiol]</p>
  <h2 id="simple-systems-can-be-conscious-a-minimally-conscious-photodiode" class="unnumbered">Simple systems can be conscious: a ``minimally conscious&quot; photodiode</h2>
  <p>The previous section showed that activations and direct interactions between elements are not necessary to generate a MICS. Taking into account the axioms and postulates of IIT, we can now summarize what it takes to be conscious and give an example of a ``minimally conscious system,“ which will be called a ``minimally conscious” photodiode.</p>
  <p><embed src="Figures/IIT3_Fig19.pdf" /></p>
  <p>[fig:photo]</p>
  <p>The ``photodiode“ in Fig. [fig:photo]A consists of two elements: the detector <span class="math">\(D\)</span> and the predictor <span class="math">\(P\)</span>. <span class="math">\(D\)</span> receives two external light inputs (and is thus a port-in) and one internal input from <span class="math">\(P\)</span>, all with input strength 1. As illustrated in Fig. [fig:photo]B, <span class="math">\(D\)</span> turns on if it receives at least two inputs from internal and/or external sources. If <span class="math">\(D\)</span> has switched on due to sufficiently strong external inputs, it activates element <span class="math">\(P\)</span>, which serves as a ``memory”. At the next time step, <span class="math">\(P\)</span> acts as a ``predictor&quot; of the next external input to <span class="math">\(D\)</span> by increasing its sensitivity to light.</p>
  <p>Simple as it is, the photodiode system satisfies the postulates of IIT: both of its elements specify selective causes and effects within the system (each element about the other one), their cause-effect repertoires are maximally irreducible, and the conceptual structure specified by the two elements is also maximally irreducible. Consequently, the system <span class="math">\(DP=11\)</span> forms a complex that gives rise to a MICS, albeit one having just two concepts and a <span class="math">\(\Phi^{\rm Max}\)</span> value of 1 (Fig. [fig:photo]C). <span class="math">\(DP\)</span> is therefore conscious, albeit minimally so.</p>
  <p>It is instructive to consider the quality of experience specified by such a minimally conscious photodiode. From an observer’s perspective, the photodiode detects light, but from the intrinsic perspective, the experience is only minimally specified, and in no way can convey the meaning ``light“: <span class="math">\(D\)</span> says something about <span class="math">\(P\)</span>’s past and future, and <span class="math">\(P\)</span> about <span class="math">\(D\)</span>’s, and that is all. Accordingly, the shape in qualia space is a constellation having just two stars, and is thus minimally specific. This aspect is further emphasized if one considers that different physical systems, say a photodiode activated by blue light (a ``blue” detector), or even a binary thermistor (a ``temperature“ detector) would generate the exact same MICS (Fig.[fig:photo]D) and thus the same minimal experience. Moreover, the symmetry of the MICS implies that the quality of the experience would be the same regardless of the system’s state: the photodiode in state <span class="math">\(DP=00\)</span>, <span class="math">\(01\)</span>, or <span class="math">\(10\)</span>, receiving one external input, generates exactly the same MICS as <span class="math">\(DP=11\)</span>. In all the above cases, the experience might be described roughly as “it is like this rather than not like this”, with no further qualifications. The photodiode’s experience is thus both quantitatively and qualitatively minimal. Only additional mechanisms that create new concepts and break the symmetries in the shape of the MICS can generate additional meaning. Ultimately, only a set of concepts comparable to that of our main complex can specify the shape of the experience ``light” as it appears to us, and distinguish it from countless other shapes corresponding to different experiences <span class="citation"></span>.</p>
  <h2 id="complex-systems-can-be-unconscious-a-zombie-feed-forward-network" class="unnumbered">Complex systems can be unconscious: a ``zombie&quot; feed-forward network</h2>
  <p>Another corollary of IIT is that certain structures do not give rise to consciousness even though they may perform complicated functions. Consider first an ``unconscious&quot; photodiode (Fig.[fig:feedf]A), comprising again the two elements <span class="math">\(D\)</span> and <span class="math">\(P\)</span>. In this case, however, whether <span class="math">\(D\)</span> is on or off is determined by external inputs only, and the output of <span class="math">\(P\)</span> does not feed back into the system. Therefore, <span class="math">\(D\)</span>’s response to light is just passed through the system, but never comes back to it. Although an observer may describe the two elements <span class="math">\(DP\)</span> as a system, <span class="math">\(D\)</span> and <span class="math">\(P\)</span> do not have both causes and effects within the system <span class="math">\(DP\)</span>, which is thus not a complex, and generates no quale.</p>
  <p>The same lack of feed-back that disqualifies the unconscious photodiode can be extended, by recursion, to any feed-forward system, no matter how numerous its elements and complicated its connectivity (Fig.[fig:feedf]B). From the viewpoint of an extrinsic observer, the system’s borders can be set arbitrarily. However, the input layer is always determined entirely by external inputs and the output layer does not affect the rest of the system. Consequently, from the intrinsic perspective, both input and output layer cannot be part of the complex. Drawing the system boundaries closer and closer together in a recursive manner, one eventually ends up with just one input and output layer, made up of many “unconscious photodiodes”, and thus generating no quale. Therefore, systems with a purely feed-forward architecture cannot generate consciousness.</p>
  <p>The idea that ``feed-back“, ``reentry”, or ``recursion“ of some kind may be an essential ingredient of consciousness has many proponents <span class="citation"></span>. Recently, it has been suggested that the presence or absence of feed-back could be directly equated with the presence or absence of consciousness <span class="citation"></span>. Moreover, several recent studies indicate that an impairment of reentrant interactions over feed-back connections is associated with loss of consciousness during anesthesia <span class="citation"></span> and in brain-damaged patients <span class="citation"></span>. However, it has been pointed out that the brain (and many other systems) is full of reentrant circuits, many of which do not seem to contribute to consciousness <span class="citation"></span>. IIT offers some specific insights with respect to these issues. First, the need for reciprocal interactions within a complex is not merely an empirical observation, but it has theoretical validity because it is derived directly from the phenomenological axiom of (strong) integration. Second, (strong) integration is by no means the only requirement for consciousness, but must be complemented by information and exclusion. Third, for IIT it is the potential for interactions among the parts of a complex that matters and not the actual occurrence of ``feed-back” or ``reentrant“ signaling, as is usually assumed. As was discussed above, a complex can be conscious, at least in principle, even though none of its neurons may be firing, no feed-back or reentrant loop may be activated, and no ``ignition” may have occurred.</p>
  <p><embed src="Figures/IIT3_Fig20.pdf" /></p>
  <p>[fig:feedf]</p>
  <h2 id="conscious-complexes-and-unconscious-zombie-systems-can-be-functionally-equivalent" class="unnumbered">Conscious complexes and unconscious ``zombie&quot; systems can be functionally equivalent</h2>
  <p>The last section showed that according to IIT feed-forward systems cannot give rise to a quale. However, without restrictions on the number of nodes, feed-forward networks with multiple layers can in principle approximate almost any given function to an arbitrary (but finite) degree <span class="citation"></span>. Therefore, it is conceivable that an unconscious system could show the same input-output behavior as a ``conscious&quot; system.</p>
  <p>An example is shown in Fig.[fig:equivalent]A. A strongly integrated system is compared to a feed-forward network that produces the same input-output behavior over at least 4 time steps (<span class="math">\(9^4\)</span> input states, Fig.[fig:equivalent]B). To achieve a memory of <span class="math">\(x\)</span> past time steps in the feed-forward system, the relevant elements were unfolded over time: the state of each element is passed on through a chain of <span class="math">\(x\)</span> nodes, one node for each of the <span class="math">\(x\)</span> time steps <span class="citation"></span>. In this way, the states of upstream elements in previous time steps can be combined (converge) in a feed-forward manner to determine the state of elements downstream, but can never feed back on elements upstream. As illustrated in the figure, while the recurrent system gives rise to a complex with <span class="math">\(\Phi^{\rm Max} &gt; 0\)</span> in every state, and would therefore be conscious, the feed-forward system is not a complex and is thus necessarily unconscious.</p>
  <p>This comparison highlights an important corollary of IIT: whether a system is conscious or not cannot be decided based on its input-output behavior only. In neuroscience, the ability to report is usually considered as the gold standard for assessing the presence of consciousness. Behavior and reportability can be reliable guides under ordinary conditions (typically adult awake humans) and can be employed to evaluate neural correlates of consciousness <span class="citation"></span> and to validate theoretical constructs <span class="citation"></span>. However, behavior and reportability become problematic for evaluating consciousness in pathological conditions, during development, in animals very different from us, and in machines that may perform sophisticated behaviors <span class="citation"></span>. For example, programs running on powerful computers can not only play chess better than humans, but win in difficult question games such as ``Jeopardy“ <span class="citation"></span>. Moreover, recent advances in machine learning have made it possible to construct simulated networks, primarily feed-forward, that can learn to recognize natural categories such as cats, dogs <span class="citation"></span>, pedestrians <span class="citation"></span>, and/or faces <span class="citation"></span>. Hence, if behavior is the gold standard, it is not clear on what grounds we should deny consciousness to a phone ``assistant” program that can answer many difficult questions, and can even be made to report about her internal feelings, or to a chip that recognizes thousands of different objects as well or better than we do, while granting it to a human who can barely follow an object with his eyes. IIT claims, by contrast, that input-output behavior is not always a reliable guide: one needs to investigate not just ``what“ functions are being performed by a system, but also ``how” they are performed within the system. Thus, IIT admits the possibility of true ``zombies&quot;, which may behave more and more like us while lacking subjective experience <span class="citation"></span>.</p>
  <p>The examples of Fig.[fig:equivalent] also suggest that, while it may be possible to build unconscious systems that perform many complex functions, there is an evident evolutionary advantage towards the selection of integrated architectures that can perform the same functions consciously. Among the benefits of integrated architectures are economy of units and wiring, speed, compositionality, context-dependency, memory, and the ability to learn adaptive functions rapidly, flexibly, and building upon previous knowledge <span class="citation"></span>. Moreover, in a feed-forward network all system elements are entirely determined by the momentary external input passing through the system. By contrast, a (strongly) integrated system is autonomous, since it can act and react based on its internal states and goals.</p>
  <p><embed src="Figures/IIT3_Fig21.pdf" /></p>
  <p>[fig:equivalent]</p>
  <h2 id="the-concepts-within-a-complex-are-self-generated-self-referential-and-holistic" class="unnumbered">The concepts within a complex are self-generated, self-referential, and holistic</h2>
  <p><embed src="Figures/IIT3_Fig22.pdf" /></p>
  <p>[fig:segment]</p>
  <p>The final example (Fig. [fig:segment]A) considers a simple perceptual system - a recurrent segment/dot system. The segment/dot system consists of 10 heavily interconnected elements that, in their current state, form a complex (Fig. [fig:segment]A, blue circle). Elements <span class="math">\(A, B\)</span>, and <span class="math">\(C\)</span> are the ports-in of the complex: they each receive 2 inputs from an external source in addition to feed-back inputs from within the complex. Elements <span class="math">\(F\)</span> and <span class="math">\(J\)</span> are the ports-out of the complex: they output to the external elements <span class="math">\(O1\)</span> and <span class="math">\(O2\)</span>, respectively, in addition to their outputs within the complex. In this example, the ports-out are XOR logic gates. All other elements inside the segment/dot system are linear threshold units (LTUs). Connections within the complex are excitatory (<span class="math">\(+1\)</span>, black) or inhibitory (<span class="math">\(-1\)</span>, red).</p>
  <p>The elementary mechanisms comprising the segment/dot system have specialized functions and generate elementary concepts. If an elementary mechanism is in state ``on“(1), its concept is termed “positive”. If the mechanism is in state “off”(0), its concept is termed “negative”. In the segment/dot system, the negative concepts tend to have lower <span class="math">\(\varphi^{\rm Max}\)</span> values, because the mechanisms tend to be more selective in their ``on” state (see Fig. [fig:selective]). As listed in Fig. [fig:segment]B, in addition to first order concepts, the segment-dot system gives rise to many higher order concepts. Dependent on the state of the system, certain higher order concepts may or may not exist. For instance, in the current state of the segment/dot system, the <span class="math">\(2^{nd}\)</span> order concept <span class="math">\(DI\)</span> exists, while <span class="math">\(EG\)</span> does not because it is reducible (<span class="math">\(\varphi^{\rm Max}=0\)</span>). If the segment/dot system were presented instead with a “right”-segment (inputs 022), <span class="math">\(DI\)</span> would disappear and <span class="math">\(EG\)</span> would emerge.</p>
  <p>From the perspective of an external observer (e.g. a neuroscientist recording the activity of ``neurons“ <span class="math">\(A-J\)</span>), the function of a mechanism is typically described with respect to external inputs (e.g. a ``segment” detector). In the segment/dot system, mechanisms at different hierarchical levels correspond to increasing levels of invariance: element <span class="math">\(D\)</span>, for example, turns on if the two contiguous pixels on the left have been on persistently (with inputs of strength 2); higher up in the system, element <span class="math">\(F\)</span> turns on if two contiguous pixels have been on either on the left or on the right, thus indicating the presence of the invariant ``segment“. Element <span class="math">\(J\)</span>, on the other hand, detects the invariant ``dot”, either left, right, or center. The excitatory and inhibitory feed-back connections in the segment/dot system serve a predictive function: they temporarily increase/decrease the sensitivity to similar/opposed stimuli, allowing weaker inputs (with a value of 1) to be detected as segments and dots if the weaker external input is in accordance with the feed-back from within the complex.</p>
  <p>From the intrinsic perspective of the system, instead, the function of each mechanism is given by its concept. Each concept is <em>self-generated</em>, because it must be specified exclusively by a subset of elements belonging to the complex. It is also <em>self-referential</em>, because its cause-effect repertoire refers exclusively to elements within the complex. Therefore, each concept is related only indirectly to external inputs. For example, the positive concept of <span class="math">\(D\)</span>, in its current state 1, is about the purview <span class="math">\(D/ABEFJ^p,A^f\)</span>. From the intrinsic perspective, the function of <span class="math">\(D=1\)</span> is thus to constrain the possible past states of <span class="math">\(A,B,E,F\)</span> and <span class="math">\(J\)</span>, and to constrain the possible future state of <span class="math">\(A\)</span> (Fig.[fig:segment]C). Therefore, <span class="math">\(D=1\)</span> specifies a concept that is exclusively self-referential to the complex to which <span class="math">\(D\)</span> belongs (note that, in this simple version of a recurrent segment/dot system, feed-forward and feed-back connections have the same absolute strength of 1. In a more realistic neural network, in which the function of the recurrent connections is mostly modulatory, a concept’s past and future purviews would be modified accordingly). Nevertheless, in this case there is a good correspondence between the intrinsic and the extrinsic perspective, since the cause repertoire of <span class="math">\(D=1\)</span> specifies as potential causes those states in which both ports-in <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are 1, which happens when two contiguous pixels on the left are on. Importantly, the concept of <span class="math">\(D=1\)</span> additionally takes into account the internal context <span class="math">\(E,F,J\)</span> (blue shaded states in Fig.[fig:segment]C). However, the correspondence between intrinsic and extrinsic perspective breaks down for the ports-in (<span class="math">\(A,B,C\)</span>): even though their state is partly determined by the external inputs, their concept specifies constraints about past and future states of elements higher up in the system, rather than about the environment (Fig.[fig:segment]D).</p>
  <p>The self-referential property of the concepts specified by ports-in may have some implications with respect to the role of primary areas in consciousness. An influential hypothesis by Crick and Koch <span class="citation"></span> suggests that primary visual cortex (V1) and perhaps other primary cortical areas may not contribute directly to consciousness, a hypothesis that is now supported by a large number of experimental results. For example, during binocular rivalry neurons in V1 may fire selectively to horizontal bars that are shown to one eye, even though the subject does not see them and is conscious of a different stimulus presented to the other eye <span class="citation"></span>. On the other hand, the firing of units higher up in the visual system correlates tightly with the experience. While these results are compelling, other interpretations are possible if, as illustrated in the segment/dot system, V1 neurons were to constitute ports-in of the main complex. Under this assumption, V1 units would have to specify concepts about other units in the complex – either other V1 units or units in higher areas – rather than about their feed-forward inputs, which would remain outside the complex. V1 concepts could relate for example to Gestalt properties such as spatial continuity, rather than to oriented bars. In that case, what V1 contributes to consciousness during binocular rivalry – namely spatial continuity – would not change substantially between the two rivalrous percepts. Instead, concepts corresponding to oriented bars would be specified by units in higher areas, whose firing is sensitive to perceptual rivalry, <em>over</em> units in V1. In sum, V1 units would contribute to consciousness not only by generating their own concepts (such as spatial continuity), but also by providing the cause repertoire for concepts specified by units higher up (such as oriented bars). While this possibility may be far-fetched and counterintuitive, it would not be inconsistent with lesion studies that highlight the importance of V1 for most aspects of visual consciousness <span class="citation"></span>.</p>
  <p>The self-referential nature of concepts within a complex has implications with respect to how concepts obtain their <em>meaning</em> (Albantakis et al., in preparation). As mentioned above, a (conscious) external observer ``knows“ that element <span class="math">\(F\)</span> in Fig. [fig:segment]E turns on whenever there is a ``segment” in the input from the environment. However, from the intrinsic perspective of the complex, that meaning cannot be specified by <span class="math">\(F=1\)</span> in isolation. This is because, while the cause-repertoire of <span class="math">\(F=1\)</span> specifies that either <span class="math">\(D\)</span> or <span class="math">\(E\)</span> must have been on, by itself it cannot specify what <span class="math">\(D\)</span> and <span class="math">\(E\)</span> mean in turn. In fact, the full meaning of ``segment“ can only be synthesized through the interlocking of cause-effect repertoires of multiple concepts within a MICS (such as that of element <span class="math">\(F\)</span> interlocked with those of elements <span class="math">\(D\)</span>, <span class="math">\(E\)</span>, and so on). In this view, the meaning of a concept depends on the <em>context</em> provided by the entire MICS to which it belongs, and corresponds to how it constrains the overall ``shape” of the MICS. Meaning is thus both self-referential (internalistic) and <em>holistic</em>.</p>
  <p>While emphasizing the self-referential nature of concepts and meaning, IIT naturally recognizes that in the end most concepts owe their origin to the presence of regularities in the environment, to which they ultimately must refer, albeit only indirectly. This is because the mechanisms specifying the concepts have themselves been honed under selective pressure from the environment during evolution, development, and learning <span class="citation"></span>. Nevertheless, at any given time, environmental input can only act as a boundary condition, helping to ``select“ which particular concepts within the MICS will be positive or negative, and their meaning will be defined entirely within the quale. Every waking experience should then be seen as an ``awake dream” selected by the environment. And indeed, once the architecture of the brain has been built and refined, having an experience – with its full complement of intrinsic meaning – does not require the environment at all, as demonstrated every night by the dreams that occur when we are asleep and disconnected from the world.</p>
  <h2 id="limitations-and-future-directions" class="unnumbered">Limitations and future directions</h2>
  <p>In finishing, we point out some limitations and unfinished business. IIT 3.0 starts from key properties of consciousness – the phenomenological axioms – and translates them into postulates that lay out how a system of mechanisms must be constructed to satisfy those axioms and thus generate consciousness. To be able to formulate the postulates in explicit, computable terms, we considered small systems of interconnected mechanisms that are fully characterized by their transition probability matrix (TPM). For each system, mechanisms are discrete in time and space (see also Methods S1) and transition probabilities are available for every possible state. Directly applying this approach to physical systems of interest, such as brains, is unfeasible for several reasons: i) One would need either to discretize the variables of interest or to extend the theoretical treatment to continuous variables. ii) For biological systems, one is usually limited to observable system states, and the exhaustive perturbation of a system as the brain across all its possible states is unfeasible. Nevertheless, systematic perturbations of brain states using naturalistic stimuli such as movies can provide useful approximations. Also, circumscribed regions of the cerebral cortex could be perturbed systematically using optogenetic methods coupled with calcium imaging. iii) Variables recorded in most neurophysiological experiments may not correspond to the spatial and temporal grain at which integrated information reaches a maximum, which is the appropriate level of analysis <span class="citation"></span>. iv) The present analysis is unfeasible for systems of more than a dozen elements or so. This is because, to calculate <span class="math">\(\Phi^{\rm max}\)</span> exhaustively, all possible partitions of every mechanism and of every system of mechanisms should be evaluated, which leads to a combinatorial explosion, not to mention that the analysis should be performed at every spatio-temporal grain. For these reasons, the primary aim of IIT 3.0 is simply to begin characterizing, in a self-consistent and explicit manner, the fundamental properties of consciousness and of the physical systems that can support it. Hopefully, heuristic measures and experimental approaches inspired by this theoretical framework will make it possible to test some of the predictions of the theory <span class="citation"></span>. Deriving bounded approximations to the explicit formalism of IIT 3.0 is also crucial for establishing in more complex networks how some of the properties described here scale with system size and as a function of system architecture.</p>
  <p>The above formulation of IIT 3.0 is also incomplete: i) We did not discuss the relationship between MICS and specific aspects of phenomenology, such as the clustering into modalities and submodalities, and the characteristic ``feel“ of different aspects of experience (space, shape, color and so on; but see <span class="citation"></span>). ii) In the examples above, we assumed that the ``micro” spatio-temporal grain size of elementary logic gates updating every time step was optimal. In general, however, for any given system the optimal grain size needs to be established by examining at which spatio-temporal level integrated information reaches a maximum <span class="citation"></span>. In terms of integrated information, then, the macro may emerge over the micro, just like the whole may emerge above the parts. iii) While emphasizing that meaning is always internal to a complex (it is self-generated and self-referential), we did not discuss in any detail how meaning originates through the nesting of concepts within MICS (its holistic nature). This issue will begin to be addressed in future work (Albantakis et al., in preparation). iv) In IIT, the relationship between the MICS generated by a complex of mechanisms, such as a brain, and the environment to which it is adapted, is not one of ``information processing“, but rather one of ``matching” between internal and external causal structures <span class="citation"></span>. Matching can be quantified as the distance between the set of MICS generated when a system interacts with its typical environment and those generated when it is exposed to a structureless (``scrambled“) version of it <span class="citation"></span>. The notion of matching, and the prediction that adaptation to an environment should lead to an increase in matching and thereby to an increase in consciousness, will be investigated in future work, both by evolving simulated agents in virtual environments (``animats” <span class="citation"></span>), and through neurophysiological experiments (Boly et al., submitted). v) IIT 3.0 explicitly treats integrated information and causation as one and the same thing, but the many implications of this approach need to be explored in depth in future work. For example, IIT implies that each individual consciousness is a local maximum of causal power. Hence, if having causal power is a requirement for existence, then consciousness is maximally real. Moreover, it is real in and of itself - from its own intrinsic perspective - without the need for an external observer to come into being.</p>
  <h1 id="supplementary-material" class="unnumbered">Supplementary Material</h1>
  <h2 id="main-differences-between-iit-3.0-and-earlier-versions" class="unnumbered">Main differences between IIT 3.0 and earlier versions</h2>
  <ol>
  <li><p>The axioms and postulates of the theory are presented explicitly. This clarifies many issues and highlights the link between the starting point of IIT, which is phenomenology itself (axioms that are assumed to be self-evident from the intrinsic perspective of a conscious entity) and the postulates that must be satisfied by physical systems in order to support consciousness. Moreover, the postulates are applied explicitly both at the level of individual mechanisms and at that of systems of mechanisms. As illustrated in the main text, axioms and postulates include existence, composition, information, integration, and exclusion.</p></li>
  <li><p>Mechanisms specify both causes and effects. Unlike IIT 2.0 <span class="citation"></span>, IIT 3.0 considers how mechanisms in a state constrain both the past and the future of a system. In a way this is a return to the state-independent framework of IIT 1.0 <span class="citation"></span>, which considered both causes and effects, but only for a stationary system at equilibrium. It is also a return to IIT 1.0 in capturing the idea that information is “a difference that makes a difference”, and not simply “a difference”. Indeed, IIT 3.0 postulates that both cause and effect are necessary to generate information intrinsically. This emphasis on both the causes and the effects of mechanisms in a state becomes a starting point for exploring the relationship between information and causation (see Albantakis et. al in prep), which in IIT 3.0 are one and the same thing.</p></li>
  <li><p>The elements of a system are mechanisms in a state. In IIT 3.0, the basic elements that define concepts and constellations of concepts within concept space are mechanisms in a state, rather than the connections among them, as was the case in IIT 2.0. This is because mechanisms in a state (e.g. on/off) can specify ``<em>given</em> this cause - <em>then</em> that effect&quot; conditions, i.e. specify concepts.</p></li>
  <li><p>Complexes are identified by assessing the effects of partitions on their entire conceptual structure. For computational expediency, in IIT 1.0 and 2.0, complexes were identified by assessing the irreducibility of a set of elements through partitions of its highest-order concept only - the concept specified by all its elements together. Only then would one establish the full conceptual structure specified by the set. In IIT 3.0 the irreducibility of a set of elements is assessed by considering how a partition affects its entire conceptual structure - all the concepts specified by its elements in all combinations (power set). In this way all the concepts that are changed or lost due to the partition contribute to <span class="math">\(\Phi\)</span>. For example, even a partition between a single element <span class="math">\(A\)</span> and the rest of the set can destroy or modify not only the elementary concept specified by <span class="math">\(A\)</span> by itself, but also the higher-order concepts <span class="math">\(A\)</span> specifies together with elements on the other side of the partition, as well as all the concepts specified by other elements that include <span class="math">\(A\)</span> in their purview.</p></li>
  <li><p>The minimum information partition (MIP) is evaluated without normalization. In IIT 1.0 and 2.0, normalization was used to avoid certain inappropriate consequences of identifying complexes based exclusively on partitions at the level of the highest-order concept. In IIT 3.0 this is no longer necessary.</p></li>
  <li><p>Mechanisms specify concepts only if they are irreducible. IIT 3.0 recognizes that concepts can only exist intrinsically if they are irreducible. This important requirement had been overlooked in IIT 2.0.</p></li>
  <li><p>Concept space has a proper metric. In IIT 2.0, the effect of partitions was measured by the Kullback-Leibler divergence (KLD) between distributions, which only takes into account differences in selectivity. IIT 3.0 recognizes the need for a true metric (the earth mover’s distance, EMD) that also takes into account the similarity or dissimilarity of states between whole and partitioned distributions. Moreover, an extended version of EMD is applied to measure the distance between whole and partitioned constellations of concepts in concept space. This development makes all the more explicit the distinction between the notion of information in IIT as “differences that make a difference” from the intrinsic perspective of a system, and the classic notion of information from the perspective of an external observer (see <span>Some differences between integrated information and Shannon information</span>).</p></li>
  <li><p>The exclusion postulate is applied not only to systems of mechanisms but also to causes and effects specified by individual mechanisms in a system.</p></li>
  <li><p>Elements outside the candidate set under consideration are treated as boundary conditions. Their states are fixed at their ``actual&quot; values, rather than noised (see Boundary conditions).</p></li>
  <li><p>A user-friendly program for calculating exhaustively all the quantities required by IIT in discrete systems is made available alongside the paper <span class="citation"></span>.</p></li>
  </ol>
  <h2 id="conditionally-independent-mechanisms" class="unnumbered">Conditionally independent mechanisms</h2>
  <p>In this paper, it is assumed that mechanisms are conditionally independent. Consider a system consisting of three elements <span class="math">\(ABC\)</span>. Mathematically, conditional independence of mechanisms <span class="math">\(A\)</span>, <span class="math">\(B\)</span>, and <span class="math">\(C\)</span> is represented as</p>
  <p><span class="math">\[p(ABC^{t}|ABC^{t-1}) = p(A^t|ABC^{t-1}) \times p(B^t|ABC^{t-1}) \times p(C^t|ABC^{t-1}).\]</span></p>
  <p>In words, given a system state at time <span class="math">\(t_{-1}\)</span>, the probability of <span class="math">\(A\)</span>, <span class="math">\(B\)</span>, and <span class="math">\(C\)</span> at time <span class="math">\(t\)</span> can be determined independently. This corresponds to the assumption that there is no instantaneous interaction between mechanisms and causes must precede their effects.</p>
  <h2 id="boundary-conditions" class="unnumbered">Boundary conditions</h2>
  <p>Choosing a ``candidate set&quot; for IIT analysis means defining a precise border between elements that are inside the candidate set and elements that are outside. From the intrinsic perspective of the candidate set, the outside elements are treated as <em>boundary conditions</em>. This means that they are not considered as variables internal to the set over which to perform perturbations, but rather as fixed, external constraints. For the purpose of IIT analysis, this means that the connections from the outside elements are not noised and their state is kept fixed. Specifically, when evaluating a cause repertoire in the candidate set, the outside elements are fixed at their past state at <span class="math">\(t_{-1}\)</span>. Similarly, when evaluating an effect repertoire, the outside elements are fixed at their present states at <span class="math">\(t_0\)</span>.</p>
  <p>As an example, in the candidate set of Fig. 1 (main text), the state of element <span class="math">\(D\)</span> is taken to be 0 at <span class="math">\(t_{-1}\)</span> and <span class="math">\(t_0\)</span>. Given these boundary condition, the candidate set <span class="math">\(ABC\)</span> performs as if <span class="math">\(D\)</span> did not exist. On the other hand, the transition probability matrix (TPM) of <span class="math">\(ABC\)</span> would be different for <span class="math">\(D(t_{-1})=1\)</span>. Thus, the conceptual structure (<span class="math">\(C\)</span>) of the candidate set may differ, depending on the boundary conditions, even though the state of the elements within the candidate set is the same (Note that, for all example systems in the Results and Discussion section, the system state at <span class="math">\(t_{-1}\)</span> is the same as the current state at <span class="math">\(t_0\)</span>. Also, when considering other candidate sets within <span class="math">\(ABC\)</span>, it is assumed that <span class="math">\(ABC(t_{-1})=110\)</span>.)</p>
  <p>A neurological example of boundary conditions would be the sensory input to the ports-in of a cortical main complex. As was illustrated in the case of the segment/dot system (Fig. 22, main text), the main complex does not include the ``sensory“ elements providing feed-forward input to it. However, their input - some on and some off - constitutes a boundary condition over which cause-effect repertoires internal to the main complex are evaluated. As a more extreme example, the activity of subcortical activating systems with diffuse projections that maintain the excitability of the cortex is likely to constitute an essential boundary or ``enabling” condition <span class="citation"></span> for the existence of a cortical main complex. Without the activating input they provide, cortical neurons become bistable and consciousness disintegrates <span class="citation"></span>. Nevertheless, the neural elements that provide this essential activating input to cortex are themselves likely to be excluded from the main complex <span class="citation"></span>.</p>
  <h2 id="cause-effect-repertoire-unconstrained-repertoire-puc-and-partitions" class="unnumbered">Cause-effect repertoire, unconstrained repertoire <span class="math">\(p^{uc}\)</span>, and partitions</h2>
  <p>To calculate the cause-effect information of a mechanism in a state over a purview, its cause-effect repertoire is compared against the unconstrained repertoire <span class="math">\(p^{uc}\)</span>. The integrated information <span class="math">\(\phi\)</span> of a mechanism in a state over a purview is assessed by comparing its cause-effect repertoire against that of the partitioned purview. How these probability distributions are derived is illustrated using the example of mechanism <span class="math">\(A=1\)</span> over the purview <span class="math">\(ABC\)</span> from Fig. 4 (main text), as well as other mechanisms from the candidate set <span class="math">\(ABC\)</span> (Fig. 1, main text).</p>
  <h3 id="cause-repertoire" class="unnumbered">Cause repertoire</h3>
  <p>The cause repertoire <span class="math">\(p(ABC^p|A^c=1)\)</span> is obtained via Bayes’ rule by perturbing the set of elements into all its states with equal likelihood, i.e. assuming a uniform prior distribution <span class="math">\(p^{\rm{per}}(ABC^p)\)</span> of past states <span class="math">\(ABC^p\)</span>, where the superscript <span class="math">\(^{\rm per}\)</span> stands for “perturbed”:</p>
  <p><span class="math">\[p(ABC^p|A^c=1) = \frac{p(A^c=1|ABC^p) p^{\rm{per}}(ABC^p)} {p(A^c=1)}. \label{eq:causerep}\]</span></p>
  <p>Here, <span class="math">\(p^{\rm{per}}(ABC^p) = 1/8\)</span> for each past state, since the set is perturbed into each state with equal probability, <span class="math">\(p(A^c=1)=3/4\)</span>, and <span class="math">\(p(A^c=1|ABC^p)\)</span> is either 0 (for states 000 and 100) or 1 (for all other states).</p>
  <p>In general, the cause repertoire can also be assessed over a subset of the candidate set, e.g. <span class="math">\(p(C^p|A^c=1)\)</span>. In this case, one has to <em>marginalize</em> over the elements outside of the purview, which remain unconstrained:</p>
  <p><span class="math">\[p(C^p|A^c=1) =  \frac{ \left( \sum_{AB^p} p(A^c=1|C^p, AB^p) \: p^{\rm{per}}(AB^p) \right)  \: p^{\rm{per}}(C^p)} {p(A^c=1)}, \label{eq:causerep2}\]</span></p>
  <p>where <span class="math">\(p^{\rm{per}}(AB^c)\)</span> denotes the perturbed (uniform) distribution over the past states of the elements <span class="math">\(AB\)</span>. The sum in equation [eq:causerep2] is effectively the average over the probabilities <span class="math">\(p(A^c=1|ABC^p)\)</span> calculated for <span class="math">\(AB^p = [00, 10, 01, 11]\)</span>.</p>
  <p>If the cause-repertoire of a higher order mechanism such as <span class="math">\(AB=10\)</span> is determined over a limited purview, e.g <span class="math">\(AB^p\)</span>, marginalizing over the remaining elements (<span class="math">\(C^p\)</span>) can lead to correlations in <span class="math">\(AB^c\)</span> if <span class="math">\(C\)</span> provides common input to both <span class="math">\(A\)</span> and <span class="math">\(B\)</span>. Since the aim is to assess the cause-information of <span class="math">\(AB^c\)</span> about <span class="math">\(AB^p\)</span> independent of <span class="math">\(C^p\)</span>, <span class="math">\(C^p\)</span> has to be replaced by ``virtual elements&quot; with independent output to every element, as indicated by the subscript <span class="math">\(_V\)</span>:</p>
  <p><span class="math">\[p(AB^p|AB^c=10) =  \frac{ \left(  \sum_{C^p_V} p(AB^c=10|AB^p, C^p_V) \: p^{\rm{per}}(C^p_V) \right)  \: p^{\rm{per}}(AB^p)} {p(AB^c=10)}.\]</span></p>
  <p>The virtual element <span class="math">\(C^p_V\)</span> means that the states 0 and 1 are imposed independently over every output connection from <span class="math">\(C\)</span> (Fig. [fig:virtualnodes]A). The mechanisms considered here are <span class="math">\(1^{st}\)</span> order Markov functions and are thus conditionally independent given their respective inputs. Therefore, the cause repertoire of a higher order mechanism such as <span class="math">\(AB=01\)</span> can be calculated as the product of the cause-repertoires of its elementary mechanisms, <span class="math">\(A=1\)</span> and <span class="math">\(B=0\)</span>, obviating the need to add virtual elements in the actual calculations:</p>
  <p><span class="math">\[p(AB^p|AB^c=10) = p(AB^p|A^c=1) \times p(AB^p|B^c=0).\]</span></p>
  <h3 id="unconstrained-cause-repertoire" class="unnumbered">Unconstrained cause repertoire</h3>
  <p>As described in the main text (Fig. 4), the amount of cause information that <span class="math">\(A=1\)</span> specifies about the past, its cause information (<span class="math">\(ci\)</span>), is measured as the distance <span class="math">\(D\)</span> between the cause repertoire (eq. [eq:causerep]) and the unconstrained repertoire <span class="math">\(p^{\rm{uc}}\)</span>. For the purview <span class="math">\(ABC^p\)</span>:</p>
  <p><span class="math">\[ci(ABC^p|A^c=1) = D(p(ABC^p|A^c=1)|| p^{ {\rm uc}}(ABC^p)) = 0.33. \label{eq:ci2}\]</span></p>
  <p><span class="math">\(p^{\rm{uc}}(ABC^p)\)</span> corresponds to the cause repertoire in the absence of any mechanism. The unconstrained past distributions is thus the uniform distribution, since without the constraints of a mechanism in a state <span class="math">\(p^{\rm{uc}}(ABC^p)\)</span> = <span class="math">\(p^{\rm{per}}(ABC^p)\)</span>.</p>
  <h3 id="effect-repertoire" class="unnumbered">Effect repertoire</h3>
  <p>The effect repertoire <span class="math">\(p(ABC^f|A^c=1)\)</span> (Fig. 4, main text) is computed by fixing the current state of <span class="math">\(A\)</span> to 1, while the remaining elements <span class="math">\(B\)</span> and <span class="math">\(C\)</span> are independently perturbed into all their possible states with equal likelihood:</p>
  <p><span class="math">\[p(ABC^f|A^c=1) = \sum_{BC^c_V} p(ABC^f|A^c=1, BC^c_V) p^{\rm{per}}(BC^c_V). \label{eq:effectrep}\]</span></p>
  <p>Again, common inputs from <span class="math">\(B\)</span> or <span class="math">\(C\)</span> can lead to correlations between <span class="math">\(A^f\)</span>, <span class="math">\(B^f\)</span>, and <span class="math">\(C^f\)</span>. To avoid counting these correlations as effects of element <span class="math">\(A\)</span>, it is important to replace elements <span class="math">\(B\)</span> and <span class="math">\(C\)</span> by virtual elements with independent outputs to every element (Fig. [fig:virtualnodes]B). Since all mechanisms under consideration are conditionally independent, in practice the effect repertoire can be calculated as:</p>
  <p><span class="math">\[p(ABC^f|A^c=1) = p(A^f|A^c=1) \times p(B^f|A^c=1) \times p(C^f|A^c=1),\]</span></p>
  <p>where the effect repertoire of a single future element, e.g. <span class="math">\(p(A^f|A^c=1)\)</span> is simply:</p>
  <p><span class="math">\[p(A^f|A^c=1) = \sum_{BC^c} p(ABC^f|A^c=1, BC^c) p^{\rm{per}}(BC^c). \label{eq:effectrep2}\]</span></p>
  <h3 id="unconstrained-effect-repertoire" class="unnumbered">Unconstrained effect repertoire</h3>
  <p>Like the cause information (<span class="math">\(ci\)</span>), the effect information (<span class="math">\(ei\)</span>) of <span class="math">\(A=1\)</span> is quantified as the distance <em>D</em> between the effect repertoire of <span class="math">\(A\)</span> and the unconstrained future distribution <span class="math">\(p^{\rm{uc}}(ABC^f)\)</span>:</p>
  <p><span class="math">\[ei(ABC^f|A^c=1)  =  D(p(ABC^f|A^c=1)|| p^{{\rm uc}}(ABC^f)) = 0.25.\]</span></p>
  <p>Without any constrains from a mechanism in a state, the unconstrained effect repertoire <span class="math">\(p^{{\rm uc}}(ABC^f)\)</span> is given by:</p>
  <p><span class="math">\[p^{{\rm uc}}(ABC^f) = \sum_{ABC^c_V} p(ABC^f|ABC^c_V) p^{{\rm per}}(ABC^c_V), \label{eq:future_ref}\]</span></p>
  <p>where virtual elements are again used to avoid including effects arising from correlations due to common inputs. For conditionally independent mechanisms, this is identical to:</p>
  <p><span class="math">\[\begin{aligned}
  p^{{\rm uc}}(ABC^f) =&amp; \sum_{ABC^c} p(A^f|ABC^c) p^{{\rm per}}(ABC^c) \times \sum_{ABC^c} p(B^f|ABC^c) p^{{\rm per}}(ABC^c) \times \\ \times &amp; \sum_{ABC^c} p(C^f|ABC^c) p^{{\rm per}}(ABC^c), \nonumber \label{eq:future_ref2}\end{aligned}\]</span></p>
  <p>the product of the effect probability distributions of each element given unconstrained inputs. <span class="math">\(p^{{\rm uc}}(ABC^f)\)</span> is thus not simply the uniform distribution of future states of the candidate set. In the example set <span class="math">\(ABC\)</span>, the unconstrained effect repertoire for the OR-gate <span class="math">\(A\)</span> is <span class="math">\(p(A=0)=0.25\)</span> and <span class="math">\(p(A=1)=0.75\)</span>, for the AND-gate <span class="math">\(B\)</span>: <span class="math">\(p(B=0)=0.75\)</span> and <span class="math">\(p(B=1)=0.25\)</span>, and for the XOR-gate <span class="math">\(C\)</span>: <span class="math">\(p(C=0)=0.5\)</span> and <span class="math">\(p(C=1)=0.5\)</span>, obtained by perturbing their inputs to the states [00, 01, 10, 11] with equal probability.</p>
  <p><embed src="Figures/IIT3_FigS1.pdf" /></p>
  <p>[fig:virtualnodes]</p>
  <h3 id="partitions" class="unnumbered">Partitions</h3>
  <p>As illustrated above, when the cause/effect repertoire of a mechanism is computed over a particular purview, the elements outside of the purview, but within the candidate set, remain unconstrained. Similarly, if a purview is partitioned, elements outside of the part under consideration become unconstrained and thus effectively act as independent noise sources (they are ``injected with noise&quot;). This renders the connections across the partition causally inactive.</p>
  <p>In the example shown in Fig. [fig:partition], the purview <span class="math">\(ABC^c/ABC^f\)</span> is partitioned into <span class="math">\(AB^c/AB^f \times C^c/C^f\)</span>. To obtain the partitioned effect repertoire, the effect repertoires of the purviews <span class="math">\(AB^c/AB^f\)</span> and <span class="math">\(C^c/C^f\)</span> are calculated independently and then multiplied. Considering the purview of <span class="math">\(AB^c/AB^f\)</span>, the element <span class="math">\(C\)</span> is outside of the purview and thus unconstrained:</p>
  <p><span class="math">\[p(AB^f|AB^c=10) = \sum_{C^c_V} p(AB^f|AB^c=10, C^c_V) p^{\rm{per}}(C^c_V).\]</span></p>
  <p>To causally disconnect <span class="math">\(C\)</span> from <span class="math">\(AB\)</span>, it is again important to introduce virtual elements, which ensure independent noise in the connections across the partition. Even if <span class="math">\(A\)</span> and <span class="math">\(B\)</span> receive a common input from <span class="math">\(C\)</span>, such a common input is ignored in the purview of <span class="math">\(AB^c/AB^f\)</span>. Since <span class="math">\(A\)</span>, <span class="math">\(B\)</span>, and <span class="math">\(C\)</span> are conditionally independent <span class="math">\(p(AB^f|AB^c=10)\)</span> can be calculated without explicitly introducing virtual units:</p>
  <p><span class="math">\[p(AB^f|AB^c=10) = \sum_{C^c} p(A^f|AB^c=10, C^c) p^{\rm{per}}(C^c) \times \sum_{C^c} p(B^f|AB^c=10, C^c) p^{\rm{per}}(C^c).\]</span></p>
  <p>Similarly, the purview of <span class="math">\(C^c/C^f\)</span> is the computed as:</p>
  <p><span class="math">\[p(C^f|C^c=0) = \sum_{AB^c} p(C^f|C^c=0, AB^c) p^{\rm{per}}(AB^c).\]</span></p>
  <p>The partitioned effect repertoire, <span class="math">\(AB^c/AB^f \times C^c/C^f\)</span>, is the product of <span class="math">\(p(AB^f|AB^c)\)</span> and <span class="math">\(p(C^f|C^c)\)</span>. Partitioned cause repertoires are calculated in the same way.</p>
  <p><embed src="Figures/IIT3_FigS2.pdf" /></p>
  <p>[fig:partition]</p>
  <h2 id="earth-movers-distance" class="unnumbered">Earth mover’s distance</h2>
  <p><embed src="Figures/IIT3_FigS3.pdf" /></p>
  <p>[fig:KLDEMD]</p>
  <h3 id="distance-for-probability-distributions" class="unnumbered">Distance for probability distributions</h3>
  <p>Integrated information <span class="math">\(\varphi\)</span> measures the difference between two probability distributions, a partitioned distribution and an unpartitioned distribution. In previous work, the Kullback-Leibler divergence (KLD) was used to compare distributions. KLD has several useful properties, but it is not a true metric (it is not symmetric) and it is unbounded. Moreover, KLD only measures how ``sharp&quot; a distribution is compared to the other, without taking into account whether some states of the system are closer than others (e.g. that [0 0] is closer to [1 0] than to [1 1]).</p>
  <p>A more appropriate measure that corresponds better to the IIT notion of information as ``differences that make a difference“ is the earth mover’s distance (EMD). As indicated by its name, an intuitive interpretation of the EMD is that it measures the minimum cost of transportation that arises when one probability distribution has to be transformed into another <span class="citation"></span>. In this view, a probability value is associated with a certain amount of ``earth” that is moved across a certain distance, the distance from one state to another. The cost of transportation is then the amount of ``earth&quot; moved times the distance by which it is moved. The distance between binary states is measured by the Hamming distance, which counts the number of places by which two strings differ. For instance, the Hamming distance between the states <span class="math">\(ABC=000\)</span> and <span class="math">\(ABC=111\)</span> is 3; the distance between <span class="math">\(ABC=010\)</span> and <span class="math">\(ABC=100\)</span> is 2. The EMD is in principle extendable to account for non-binary states, as long as a distance between the individual states is given, which is an intrinsic property of the mechanisms under consideration.</p>
  <p>EMD is symmetric, is bounded (by the number of elements <span class="math">\(N\)</span> for binary mechanisms) and takes into account the distance between states. Fig. [fig:KLDEMD] shows a cause repertoire over two elements with two of its possible partitioned repertoires. In the intact cause repertoire, the only possible cause is <span class="math">\(00\)</span>. Partitions 1 and 2 both have state <span class="math">\(00\)</span> as a possible cause, but add state <span class="math">\(10\)</span> (Partition 1) and state <span class="math">\(11\)</span> (Partition 2), with equal probability. Since state <span class="math">\(00\)</span> is closer to <span class="math">\(10\)</span> than to <span class="math">\(11\)</span>, the intact distribution is closer to Partition 1 than to the Partition 2, as captured with EMD. From the intrinsic perspective of the system, then, Partition 2 makes more of a difference, since the state of <em>two</em> system mechanisms becomes undetermined instead of just one. By contrast, KLD assigns the same distance to both partitions, since it only measures the “reduction of uncertainty.”</p>
  <h3 id="distance-for-constellations-of-concepts" class="unnumbered">Distance for constellations of concepts</h3>
  <p><embed src="Figures/IIT3_FigS4.pdf" /></p>
  <p>[fig:genEMD]</p>
  <p>Integrated conceptual information <span class="math">\(\Phi^{\rm Max}\)</span> measures the difference between the intact constellation of a set of elements <span class="math">\(C\)</span> and that of its minimum information partition <span class="math">\(C^{\rm MIP}_\rightarrow\)</span>. Like the difference between probability distributions <span class="math">\(\varphi^{\rm}\)</span>, the difference between constellations is assessed by an extended version of the earth mover’s distance (EMD). The extended EMD measures the minimal cost of transforming one constellation into another. Instead of probabilities, in the extended EMD it is the <span class="math">\(\varphi^{\rm}\)</span> value of the concepts that corresponds to the “earth” that is redistributed from constellation <span class="math">\(C\)</span> to <span class="math">\(C^{\rm MIP}_\rightarrow\)</span>. Instead of the Hamming distance, the distance between the concepts of <span class="math">\(C\)</span> and <span class="math">\(C^{\rm MIP}_\rightarrow\)</span> is given by their distance in concept space. Since <span class="math">\(\sum (\varphi^{\rm Max})\)</span> of all concepts of <span class="math">\(C\)</span> is usually higher than that of <span class="math">\(C^{\rm MIP}_\rightarrow\)</span>, any residual <span class="math">\(\varphi^{\rm Max}\)</span> is assigned to the ``null&quot; concept (the unconstrained distribution <span class="math">\(p^{uc}\)</span>), which is included as an additional location in <span class="math">\(C^{\rm MIP}_\rightarrow\)</span>.</p>
  <p>Fig. [fig:genEMD] shows how the <span class="math">\(\Phi^{\rm Max}\)</span> value of candidate set <span class="math">\(ABC\)</span> (Fig. 12, main text) is calculated. To illustrate the analogy between the standard EMD for probability distributions and the extended EMD for constellations, the <span class="math">\(\varphi^{\rm Max}\)</span> values of the concepts of <span class="math">\(C\)</span> and <span class="math">\(C^{\rm MIP}_\rightarrow\)</span> are displayed as two distributions. In this example, the concept of <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are unaffected by the partition, while the other 4 concepts are destroyed. The optimal way to transform <span class="math">\(C\)</span> into <span class="math">\(C^{\rm MIP}_\rightarrow\)</span> is thus to move the <span class="math">\(\varphi^{\rm Max}\)</span> values of the concepts <span class="math">\(C\)</span>, <span class="math">\(AB\)</span>, <span class="math">\(BC\)</span>, and <span class="math">\(ABC\)</span> to the ``null“ concept <span class="math">\(p^{uc}\)</span>. The distance between two concepts in concept space is measured by the EMD distance of their cause-effect repertoires. The distance from concept <span class="math">\(C\)</span> to the ``null” concept, for example (Fig. [fig:genEMD]B), is the sum of the standard EMD distance of the two cause repertoires and the two effect repertoires of <span class="math">\(C\)</span> and the <span class="math">\(p^{uc}\)</span>. To obtain <span class="math">\(\Phi^{\rm Max}\)</span> for this example, the distances of concepts <span class="math">\(C\)</span>, <span class="math">\(AB\)</span>, <span class="math">\(BC\)</span>, and <span class="math">\(ABC\)</span> to the ``null&quot; concept are multiplied by the <span class="math">\(\varphi^{\rm Max}\)</span> value of each concept and then summed up. In the general case, the optimal way to redistribute <span class="math">\(\varphi^{\rm Max}\)</span> form <span class="math">\(C\)</span> to <span class="math">\(C^{\rm MIP}_\rightarrow\)</span> must be found using an optimization algorithm <span class="citation"></span>.</p>
  <h4 id="section"><span class="header-section-number">0.0.0.1</span> </h4>
  <p>Custom-made MATLAB software was used for all calculations. The program to calculate the complex of a small system of logic gates and its constellation of concepts is available under: <span class="citation"></span>. EMD calculations were performed using the open source fast MATLAB code of Pele and Werman <span class="citation"></span>.</p>
  <h2 id="motivation-for-exclusion-at-the-level-of-mechanisms" class="unnumbered">Motivation for exclusion at the level of mechanisms</h2>
  <p>Fig. S5 is described in the main text, section ``Exclusion: A maximally irreducible cause-effect repertoire (MICE) specified by a subset of elements (a concept)&quot;.</p>
  <p><embed src="Figures/IIT3_FigS5.pdf" /></p>
  <p>[fig:Neuron]</p>
  <h2 id="some-differences-between-integrated-information-and-shannon-information" class="unnumbered">Some differences between integrated information and Shannon information</h2>
  <p>While the notion of information plays a central role in IIT, it is distinctively different from that developed in Shannon’s communication theory <span class="citation"></span>.</p>
  <p>Information as defined in information theory quantifies how accurately input signals can be decoded by knowing output signals transmitted across a (noisy) channel. (Mutual) information is the average statistical dependence between two sets of variables, such as the inputs and the outputs of a channel. In general, this depends on the entropy of the inputs and on how well the inputs predict the outputs. The (self)-information contained in a single system state is simply the logarithm of its probability of occurrence. As clearly recognized by Shannon, the information of information theory is divorced from meaning: the sender and the receiver are assumed to know what the messages mean, and information theory is only concerned with how reliably and efficiently they can be transmitted across a channel. While Shannon information can be evaluated between past states and current states of the same system, it is always assessed from the extrinsic perspective of an observer who assesses the statistical dependence between inputs and outputs. Therefore, Shannon information could be called <em>extrinsic</em> information.</p>
  <p>In IIT, by contrast, information is <em>intrinsic</em>: it is assessed from the intrinsic perspective of a system in terms of the differences that make a difference to it. Intrinsic information is <em>causal</em>, and it must be evaluated by perturbing a set of elements in all possible ways, not just by observing them. It is also <em>compositional</em>, in that different combinations of elements within a system can simultaneously specify different constraints. Crucially, in IIT information must also be <em>integrated</em>: if partitioning a system makes no difference to it, there is no system to begin with. Moreover, IIT only considers information that is maximally integrated or <em>exclusive</em> because, from the intrinsic perspective, there can only be one set of causes. None of these requirements matter from the extrinsic perspective of an observer who measures the overall statistical dependence between inputs and outputs. Note also that within the framework of Shannon information, the difference between two probability distributions is assessed based on the Kullback-Leibler divergence (KLD), which measures the loss of Shannon information from the perspective of an external observer if one probability distribution is approximated by another. In the case that a distribution is evaluated against maximum entropy (all states equally likely), KLD measures reduction of uncertainty. Since information in IIT aims to capture differences that make a difference from the intrinsic perspective of a system, it uses a true metric (EMD) that is sensitive to the relative distance among system states (Text S5).</p>
  <p>Most importantly, classical information theory does not deal with meaning, but only with how well messages are communicated and stored. In IIT, instead, information <em>is</em> meaning: more precisely, intrinsic integrated information is a maximally irreducible information structure (MICS). This integrated information structure captures all the constrains over the past and future of a complex as determined by the state of its mechanisms. It is thus a “shape” in concept space, not a message transmitted across a channel. Accordingly, the meaning is the MICS itself, and the meaning of each individual concept within the MICS is self-generated, self-referential, and holistic: it is constructed by the elements of the complex, over the elements of the complex, and in the context provided by other concepts within the same MICS.</p>
  <p>As an example, consider the firing of a “face” neuron deep inside the brain that is part of the main complex. From the extrinsic perspective of an observer, such a face neuron can certainly convey extrinsic, Shannon information about some events in the environment, given that its firing is tightly correlated with the presence of faces. From the intrinsic perspective, however, the firing of that neuron is meaningful because of the way it modifies the shape of the MICS generated by the main complex (which it would modify in a different way if it were silent). As a corollary, dreams have as much meaning as awake experiences: the firing of a “face” neuron in a dream specifies the same intrinsic information even though it conveys no extrinsic information about the environment.</p>
  <p>Ultimately, of course, the circuits that generate meaning originate, develop, and refine through a long process of evolution, neural development, and learning, under the selective pressure of a complex environment. According to IIT, the goal of these circuits is not so much to “process” extrinsic information from the environment. Instead, it is to generate integrated information structures that ``match“ regularities in the environment in such a way that the ``right dream” occurs at the right time <span class="citation"></span>. This matching, in turn, can act as a driving force for the evolution of consciousness <span class="citation"></span>.</p>
  <h1 id="acknowledgments" class="unnumbered">Acknowledgments</h1>
  <p>We thank Chiara Cirelli, Lice Ghilardi, Melanie Boly, Christof Koch, and Marcello Massimini for many invaluable discussions concerning the concepts presented here. We also thank Brad Postle, Barry van Veen, Virgil Griffiths, Atif Hashmi, Erik Hoel, Matteo Mainetti, Andy Nere, Umberto Olcese, and Puneet Rana. We are especially grateful to V. Griffith for his contribution to characterizing the concept of synergy and its relation to integrated information; to M. Mainetti for his help in characterizing the proper metric for conceptual spaces. For developing the software used to compute maximally irreducible integrated conceptual structures we are indebted to B. Shababo, A. Nere, A. Hashmi, U. Olcese, and P. Rana. This work was supported by a Paul Allen Family Foundation grant and by the McDonnell Foundation.</p>
  <p><span>50</span> Le Q V. et al. (2011) Building high-level features using large scale unsupervised learning. In: ICML’2012.</p>
  <p>The DeepQA Research Team (2013) Available: <span class="math">\({\rm http://researcher.ibm.com/researcher/view\_project.php?id=2099}\)</span>. Accessed October 21, 2013.</p>
  <p>Thompson C (2010) Smarter Than You Think - I.B.M.’s Supercomputer to Challenge “Jeopardy!” Champions. N Y Times Mag.</p>
  <p>Tononi G (2004) An information integration theory of consciousness. BMC Neurosci 5: 42.</p>
  <p>Tononi G (2008) Consciousness as integrated information: a provisional manifesto. Biol Bull 215: 216-242.</p>
  <p>Tononi G (2012) Integrated information theory of consciousness: an updated account. Arch Ital Biol 150: 56-90.</p>
  <p>Baars BJ (1988) A Cognitive Theory of Consciousness (Cambridge University Press).</p>
  <p>Crick F, Koch C (2003) A framework for consciousness. Nat Neurosci 6: 119-126.</p>
  <p>Koch C (2004) The Quest for Consciousness: A Neurobiological Approach (Roberts and Co.).</p>
  <p>Dehaene S, Changeux JP (2011) Experimental and theoretical approaches to conscious processing. Neuron 70: 200–27. Chalmers DJ (1996) The Conscious Mind: In Search of a Fundamental Theory (Oxford University Press).</p>
  <p>Tononi G, Koch C (2008) The neural correlates of consciousness: an update. Ann N Y Acad Sci 1124: 239-61.</p>
  <p>Tononi G, Laureys S (2009) The neurology of consciousness: an overview. The neurology of consciousness, 375-412.</p>
  <p>Casali AG, Gosseries O, Rosanova M, Boly M, Sarasso S, et al. (2013) A theoretically based index of consciousness independent of sensory processing and behavior. Science translational medicine 5(198): 198ra105-198ra105.</p>
  <p>Oizumi et al. (in prep)</p>
  <p>Tononi G (2001) Information measures for conscious experience. Arch Ital Biol 139:367-71.</p>
  <p>Balduzzi D, Tononi G (2008) Integrated information in discrete dynamical systems: Motivation and theoretical framework. PLoS Comput Biol 4: e1000091.</p>
  <p>Balduzzi D, Tononi G (2009) Qualia: the geometry of integrated information. PLoS Comput Biol 5: e1000462.</p>
  <p>Hoel E, Albantakis L, Tononi G (2013) Quantifying causal emergence shows that ``macro“ can beat ``micro”. Proc Natl Acad Sci: In press.</p>
  <p>Ay N, Polani D (2008) Information Flows in Causal Networks. Adv Complex Syst 11:17–41.</p>
  <p>Korb KB, Nyberg EP, Hope L (2011) in Causality in the Sciences (Oxford University Press, Oxford).</p>
  <p>Griffith V, Koch C (2012) Quantifying synergistic mutual information. arXiv preprint arXiv:1205.4265.</p>
  <p>Wilson RJ (1985) Introduction to Graph Theory, 3/e (Longman Scientific &amp; Technical).</p>
  <p>Plum F, Posner JB (1982) The Diagnosis of Stupor and Coma (Oxford University Press).</p>
  <p>Tononi G, Edelman GM (1998) Consciousness and complexity. Science 282: 1846-1851.</p>
  <p>Gazzaniga MS (2005) Forty-five years of split-brain research and still going strong. Nat Rev Neurosci 6:653–9.</p>
  <p>Lynn S, Rhue J (1994) Dissociation: Clinical and theoretical perspectives (Guilford Press).</p>
  <p>Mudrik L, Breska A, Lamy D, Deouell LY (2011) Integration without awareness: expanding the limits of unconscious processing. <em>Psychol Sci</em> 22: 764–70.</p>
  <p>Mudrik, L, Faivre N, Koch S (2013) Information integration in the absence of awareness. Trends in Cognitive Sciences, in press.</p>
  <p>Glickstein M (2007) What does the cerebellum really do? Curr Biol 17:R824–R827.</p>
  <p>Schmahmann JD, Weilburg JB, Sherman JC (2007) The neuropsychiatry of the cerebellum - insights from the clinic. Cerebellum 6:254–67.</p>
  <p>Boyd CAR (2010) Cerebellar agenesis revisited. Brain 133:941–4.</p>
  <p>Cohen D (1998) Patches of synchronized activity in the cerebellar cortex evoked by mossy-fiber stimulation: Questioning the role of parallel fibers. Proc Natl Acad Sci 95:15032–15036.</p>
  <p>Bower JM (2002) The Organization of Cerebellar Cortical Circuitry Revisited. Implications for Function. Ann N Y Acad Sci 978:135–155.</p>
  <p>Sporns O (2010) Networks of the Brain (MIT Press).</p>
  <p>van den Heuvel MP, Sporns O (2013) An anatomical substrate for integration among functional networks in human cortex. J Neurosci 33:14489–500.</p>
  <p>Massimini M et al. (2005) Breakdown of cortical effective connectivity during sleep. Science 309:2228–32.</p>
  <p>1. Ferrarelli F et al. (2010) Breakdown in cortical effective connectivity during midazolam-induced loss of consciousness. Proc Natl Acad Sci U S A 107:2681–6.</p>
  <p>Sanes DH, Reh TA, Harris WA (2011) Development of the Nervous System (Academic Press).</p>
  <p>Sullivan PR (1995) Contentless Consciousness and Information-Processing Theories of Mind. Philos Psychiatry, Psychol 2:51–59.</p>
  <p>Edelman GM (1989) The Remembered Present: A Biological Theory of Consciousness (Basic Books).</p>
  <p>Harth E (1993) The creative loop: How the brain makes a mind (Addison-Wesley, Reading, MA).</p>
  <p>Hofstadter DR (2007) I Am a Strange Loop (Basic Books).</p>
  <p>Lamme VAF (2003) Why visual attention and awareness are different. Trends Cogn Sci 7:12–18.</p>
  <p>Imas OA, Ropella KM, Ward BD, Wood JD, Hudetz AG (2005) Volatile anesthetics disrupt frontal-posterior recurrent information transfer at gamma frequencies in rat. Neurosci Lett 387:145–150.</p>
  <p>Boly M et al. (2012) Connectivity changes underlying spectral EEG changes during propofol-induced loss of consciousness. J Neurosci 32:7082–90.</p>
  <p>Mashour GA (2013) Cognitive unbinding: A neuroscientific paradigm of general anesthesia and related states of unconsciousness. Neurosci Biobehav Rev.</p>
  <p>Koch C, Crick F (2001) The zombie within. Nature 411: 893.</p>
  <p>Boly M et al. (2011) Preserved feedforward but impaired top-down processes in the vegetative state. Science 332:858–62.</p>
  <p>Cybenko G (1989) Approximation by superpositions of a sigmoidal function. Math Control Signals Syst 2: 303-314.</p>
  <p>Hornik K, Stinchcombe M, White H (1989) Multilayer feedforward networks are universal approximators. Neural Networks 2: 359-366.</p>
  <p>Rumelhart D, Hinton G, Williams R (1986) Learning internal representations by error propagation, Parallel distributed processing, 1986. Cambridge, MA.</p>
  <p>Goldman M (2009) Memory without feedback in a neural network. Neuron 61: 499–501.</p>
  <p>Dalal N, Triggs B (2005) in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) (IEEE), pp 886–893.</p>
  <p>Serre T, Wolf L, Bileschi S, Riesenhuber M, Poggio T (2007) Robust object recognition with cortex-like mechanisms. IEEE Trans Pattern Anal Mach Intell 29:411–26.</p>
  <p>Sung K-K, Poggio T (1998) Example-based learning for view-based human face detection. IEEE Trans Pattern Anal Mach Intell 20:39–51.</p>
  <p>Zhao W, Chellappa R, Phillips PJ, Rosenfeld A (2003) Face recognition. ACM Comput Surv 35:399–458.</p>
  <p>Poggio T, Ullman S (2013) Vision: are models of object recognition catching up with the brain? Ann N Y Acad Sci.</p>
  <p>Crick F, Koch C (1995) Are we aware of neural activity in primary visual cortex? Nature 375: 121–123.</p>
  <p>Blake R, Logothetis NK (2002) Visual competition. Nat Rev Neurosci 3: 13–21.</p>
  <p>Tong F (2003) Primary visual cortex and visual awareness. Nat Rev Neurosci 4:219–29.</p>
  <p>Pollen DA (2008) Fundamental requirements for primary visual perception. Cereb Cortex 18:1991–8.</p>
  <p>Tononi G, Sporns O, Edelman GM (1996) A complexity measure for selective matching of signals by the brain. Proc Natl Acad Sci U S A 93:3422–3427.</p>
  <p>Friston K, Kiebel S (2009) Predictive coding under the free-energy principle. Philos Trans R Soc Lond B Biol Sci 364:1211–21.</p>
  <p>Friston K (2010) The free-energy principle: a unified brain theory? Nat Rev Neurosci 11:127–38.</p>
  <p>Barrett AB, Seth AK (2011) Practical measures of integrated information for time-series data. PLoS Comput Biol 7:e1001052.</p>
  <p>Hashmi A, Nere A, Tononi G (2013) Sleep-Dependent Synaptic Down-Selection (II): Single-Neuron Level Benefits for Matching, Selectivity, and Specificity. Front Neurol 4:148.</p>
  <p>Albantakis L, Hintze A, Koch, C, Adami C, Tononi G (2013) Information Matching - Environment dependent increase in integrated information (<span class="math">\(\Phi\)</span>). European Conference on Complex Systems (ECCS13).</p>
  <p>Edlund J a et al. (2011) Integrated information increases with fitness in the evolution of animats. PLoS Comput Biol 7:e1002236.</p>
  <p>Joshi NJ, Tononi G, Koch C (2013) The minimal complexity of adapting agents increases with fitness. PLoS Comput Biol 9:e1003111.</p>
  <p>https://github.com/Albantakis/iit/tree/IIT-3.0-Program</p>
  <p>Mormann F, Koch C (2007) Neural Correlates of Consciousness. Scholarpedia 2:1740.</p>
  <p>Pele O, Werman M (2008) A linear time histogram metric for improved sift matching. Computer Vision-ECCV 2008, 495-508. Springer Berlin Heidelberg.</p>
  <p>Pele O, Werman M (2009) Fast and robust earth mover’s distances. Computer vision, 2009 IEEE 12th international conference on, 460-467. IEEE.</p>
  <p>Shannon CE, Weaver W (1949) The mathematical theory of communication (University of Illinois press, Urbana, IL).</p>
  <p>Cover TM, Thomas JA (2006) Elements of information theory (Wiley-interscience).</p>
  <h1 id="figure-legends" class="unnumbered">Figure Legends</h1>
