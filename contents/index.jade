---
template: layout.jade
---

- // Get month and year
- var d = new Date();
- var year = d.getFullYear();
- var months = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]; 
- var month = months[d.getMonth()];
- console.log(month)

.container
  .row
    .col-md-3
      .iit-sidebar.hidden-print.affix-top(role="complimentary")
        ul.nav.iit-sidenav
          li.active
            a(href="#abstract") Abstract
          li
            a(href="#author-summary") Author Summary
          li
            a(href="#introduction") Introduction
          li
            a(href="#theory-and-models") Theory and Models
            ul.nav
              li
                a(href="#axioms-postulates-and-identities") Axioms, postulates, and identities
                ul.nav
                  li
                    a(href="#axioms") Axioms
                  li
                    a(href="#postulates") Postulates
                  li
                    a(href="#mechanisms") Mechanisms
                  li
                    a(href="#systems-of-mechanisms") Systems of mechanisms
                  li
                    a(href="#identities") Identities
              li
                a(href="#mechanisms-1") Mechanisms
                ul.nav
                  li
                    a(href="#existence") Existence
                  li
                    a(href="#composition") Composition
                  li
                    a(href="#information-cause-effect-repertoires-and-cause-effect-information-cei") Information
                  li
                    a(href="#integration-irreducible-cause-effect-repertoires-and-integrated-information-varphi") Integration
                  li
                    a(href="#exclusion-a-maximally-irreducible-cause-effect-repertoire-mice-specified-by-a-subset-of-elements-a-concept") Exclusion
              li
                a(href="#systems-of-mechanisms-1") Systems of mechanisms
                ul.nav
                  li
                    a(href="#information-conceptual-structure-constellation-of-concepts-in-concept-space-and-conceptual-information-ci") Information
                  li
                    a(href="#integration-irreducible-conceptual-structure-and-integrated-conceptual-information-phi") Integration
                  li
                    a(href="#exclusion-a-maximally-irreducible-conceptual-structure-mics-specified-by-a-set-of-elements-a-complex") Exclusion
                  li
                    a(href="#identity-between-an-experience-and-a-maximally-irreducible-conceptual-structure-mics-or-quale-sensu-lato-generated-by-a-complex") Identity between an experience and a MICS
          li
            a(href="#results-and-discussion") Results and Discussion
            ul.nav
              li
                a(href="#a-system-may-condense-into-a-main-complex-and-several-minor-complexes") Main and minor complexes
              li
                a(href="#consciousness-and-connectivity-modular-homogeneous-and-specialized-networks") Consciousness and connectivity
              li
                a(href="#consciousness-and-activity-inactive-systems-can-be-conscious") Consciousness and activity
              li
                a(href="#simple-systems-can-be-conscious-a-minimally-conscious-photodiode") Simple systems can be conscious
              li
                a(href="#complex-systems-can-be-unconscious-a-zombie-feed-forward-network") Complex systems can be unconscious
              li
                a(href="#conscious-complexes-and-unconscious-zombie-systems-can-be-functionally-equivalent") Conscious and unconscious systems can be functionally equivalent
              li
                a(href="#the-concepts-within-a-complex-are-self-generated-self-referential-and-holistic") Concepts within a complex are self-generated, self-referential, and holistic
              li
                a(href="#limitations-and-future-directions") Limitations and future directions
          li
            a(href="#supplementary-material") Supplementary&#32;Material
            ul.nav
              li
                a(href="#main-differences-between-iit-3") Differences&#32;between&#32;IIT&#32;3.0&#32;and&#32;earlier&#32;versions
              li
                a(href="#conditionally-independent-mechanisms") Conditionally&#32;independent&#32;mechanisms
              li
                a(href="#boundary-conditions") Boundary&#32;conditions
              li
                a(href="#cause-effect-repertoire-unconstrained-repertoire-puc-and-partitions") Cause-effect&#32;repertoire, unconstrained&#32;repertoire <span class="math">\(p^{uc}\)</span>, and partitions
                ul.nav
                  li
                    a(href="#cause-repertoire") Cause&#32;repertoire
                  li
                    a(href="#unconstrained-cause-repertoire") Unconstrained cause&#32;repertoire
                  li
                    a(href="#effect-repertoire") Effect&#32;repertoire
                  li
                    a(href="#unconstrained-effect-repertoire") Unconstrained&#32;effect&#32;repertoire
                  li
                    a(href="#partitions") Partitions
              li
                a(href="#earth-movers-distance") Earth&#32;mover’s&#32;distance
                ul.nav
                  li
                    a(href="#distance-for-probability-distributions") Distance&#32;for&#32;probability&#32;distributions
                  li
                    a(href="#distance-for-constellations-of-concepts") Distance&#32;for&#32;constellations&#32;of&#32;concepts
              li
                a(href="#motivation-for-exclusion-at-the-level-of-mechanisms") Motivation&#32;for&#32;exclusion&#32;at&#32;the&#32;level&#32;of&#32;mechanisms
              li
                a(href="#some-differences-between-integrated-information-and-shannon-information") Differences&#32;between&#32;integrated&#32;information&#32;and&#32;Shannon&#32;information
          li
            a(href="#acknowledgments") Acknowledgments
    .col-md-9(role="main")
      .wrap#index
        h1.page-header.title From the phenomenology to the mechanisms of consciousness: Integrated Information Theory 3.0
        p
          | Masafumi Oizumi
          span.math  \(^{1,2,\dagger}\)
          | , Larissa Albantakis
          span.math  \(^{1,\dagger}\)
          | , Giulio Tononi
          span.math  \(^{1,\ast}\)
        p.footnote
          span.math  \(^1\)
          | Department of Psychiatry, University of Wisconsin, Madison, WI, USA
          br
          span.math  \(^2\)
          | RIKEN Brain Science Institute, Wako-shi, Saitama, Japan
          br
          span.math  \(^\dagger\)
          | These authors equally contributed to this work.
          br
          span.math  \(^\ast\)
          | E-mail correspondence to 
          a(href="mailto:gtononi@wisc.edu") gtononi@wisc.edu.
      section#abstract
        h1.page-header.unnumbered Abstract
        p
          | This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific---it is what it is by how it differs from alternative experiences; integration says that it is unified---irreducible to non-interdependent components; exclusion says that it has definite borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ``differences that make a difference" within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (<em>MICS, a constellation of concepts in qualia space</em>), and the set of elements that generates it constitutes a
          em  complex. 
          | According to IIT, a MICS specifies the quality of an experience and its integrated information
          span.math  \(\Phi^{\rm Max}\)
          |  its quantity. From the theory follow several results, including: a system of mechanisms may condense into a main complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true “zombies”---unconscious feed-forward systems that are functionally equivalent to conscious complexes.
      section#author-summary
        h1.page-header.unnumbered Author Summary
        p
          | Integrated information theory (IIT) approaches the relationship between consciousness and its physical substrate by first identifying the fundamental properties of experience itself: composition, information, integration, and exclusion. IIT then postulates that the physical substrate of consciousness must satisfy these very properties. We develop a detailed mathematical framework in which composition, information, integration, and exclusion are defined precisely and made operational. This allows us to establish to what extent simple systems of mechanisms, such as logic gates or neuron-like elements, can form complexes that can account for the fundamental properties of consciousness. Based on this principled approach, we show that IIT can explain many known facts about consciousness and the brain, leads to specific predictions, and allows us to infer, at least in principle, both the quantity and quality of consciousness for systems whose causal structure is known. For example, we show that some simple systems can be minimally conscious, some complicated systems can be completely unconscious, and two different systems can be functionally equivalent, yet one is conscious and the other one is not.
      section#introduction
        h1.page-header.unnumbered Introduction
        p
          | Understanding consciousness requires not only empirical studies of its neural correlates, but also a principled theoretical approach that can provide explanatory, inferential, and predictive power. For example, why is consciousness generated by the corticothalamic system---or at least some parts of it, but not by the cerebellum, despite the latter having even more neurons? Why does consciousness fade early in sleep, although the brain remains active? Why is it lost during generalized seizures, when neural activity is intense and synchronous? And why is there no direct contribution to consciousness from neural activity within sensory and motor pathways, or within neural circuits looping out of the cortex into subcortical structures and back, despite their manifest ability to influence the content of experience? Explaining these facts in a parsimonious manner calls for a theory of consciousness. (Below, consciousness, experience, and phenomenology are taken as being synonymous).
        p
          | A theory is also needed for making inferences in difficult or ambiguous cases. For example, is a newborn baby conscious, how much, and of what? Or an animal like a bat, a lizard, a fruit fly? In such cases, one cannot resort to verbal reports to establish the presence and nature of consciousness, or to the neural correlates of consciousness as established in healthy adults. The inadequacy of behavioral assessments of consciousness is also evident in many brain damaged patients, who cannot communicate, and whose brain may be working in ways that are hard to interpret. Is a clinically vegetative patient showing an island of residual, near-normal brain activity in just one region of the cortex conscious, how much, and of what? Or is nobody home? Or again, consider machines, which are becoming more and more sophisticated at reproducing human cognitive abilities and at interacting profitably with us. Some such machines can learn to categorize objects such as faces, places, animals, and so on, as well if not better than humans
          span.citation
          | , or can answer difficult questions better than humans
          span.citation
          | . Are such machines approaching our level of consciousness? If not, what are they missing, and what does it take to build a machine that is actually conscious? Clearly, only a theory---one that says what consciousness is and how it can be generated---can hope to offer a combination of explanatory, inferential, and predictive power starting from a few basic principles, and provide a way to quantify both the level of consciousness and its content.
        p
          | Integrated information theory (IIT) is an attempt to characterize consciousness mathematically both in quantity and in quality
          span.citation
          | . IIT starts from the fundamental properties of the phenomenology of consciousness, which are identified as
          em  axioms 
          | of consciousness. Then, IIT translates these axioms into
          em  postulates, 
          | which specify which conditions must be satisfied by physical mechanisms, such as neurons and their connections, to account for the phenomenology of consciousness. It must be emphasized that taking the phenomenology of consciousness as primary, and asking how it can be implemented by physical mechanisms, is the opposite of the approach usually taken in neuroscience: start from neural mechanisms in the brain, and ask under what conditions they give rise to consciousness, as assessed by behavioral reports
          span.citation
          | . While identifying the ``neural correlates of consciousness" is undoubtedly important
          span.citation
          | , it is hard to see how it could ever lead to a satisfactory explanation of what consciousness is and how it comes about
          span.citation
          | .
        p
          | As will be illustrated below, IIT offers a way to analyze systems of mechanisms to determine if they are properly structured to give rise to consciousness, how much of it, and of which kind. As reviewed previously
          span.citation
          | , the fundamental principles of IIT, such as integration and differentiation, can provide a parsimonious explanation for many neuroanatomical, neurophysiological, and neuropsychological findings concerning the neural substrate of consciousness. Moreover, IIT leads to experimental predictions, for instance that the loss and recovery of consciousness should be associated with the breakdown and recovery of information integration. This prediction has been confirmed using transcranial magnetic stimulation in combination with high-density electroencephalography in several different conditions characterized by loss of consciousness, such as deep sleep, general anesthesia obtained with several different agents, and in brain damaged patients (vegetative, minimally conscious, emerging from minimal consciousness, locked-in
          span.citation
          | ). Furthermore, IIT has inspired theoretically motivated measures of the level of consciousness that have been applied to human and animal data (e.g.
          span.citation
          | ).
        p
          | While the central assumptions of IIT have remained the same, its theoretical apparatus has undergone various developments over the years. The original formulation, which may be called IIT 1.0, introduced the essential notions including causal measures of the quantity and quality of consciousness. However, to simplify the analysis, IIT 1.0 dealt exclusively with stationary systems
          span.citation
          | (see also
          span.citation
          | ). The next formulation, which will be called IIT 2.0
          span.citation
          | applied the same notions on a state-dependent basis: it showed how integrated information could be calculated in a top-down manner for a system of mechanisms in a state
          span.citation
          | and suggested a way to characterize the quality of an experience by considering its sub-mechanisms
          span.citation
          | . The formulation presented below, and the new results that follow from it, represent a substantial advance at several different levels, hence IIT 3.0 (see also
          span.citation
          | ). Nevertheless, this article is presented independently of previous “releases” for readers new to IIT. For those readers who may have followed the evolution of IIT, the main advances are summarized in the Supplementary Material (Text S1).
        p
          | In what follows, we first present the axioms and the postulates of IIT. We then provide the mathematical formalism and motivating examples for each of the postulates. The key constructs of IIT are introduced first at the level of individual mechanisms, which can be taken to represent physical objects such as logic gates or neurons, then at the level of systems of mechanisms, such as computers or neural architectures. The Theory and Models section ends by presenting the central identity proposed by IIT, according to which the quality and quantity of an experience is completely specified by a maximally irreducible conceptual structure (MICS) and the associated value of integrated information
          span.math  \(\Phi^{\rm Max}\)
          | . The Results and Discussion section presents several new results that follow directly from IIT, including the condensation of systems of mechanisms into main complexes and minor complexes; examples of simple systems that are minimally conscious and of complicated systems that are not; an example of an unconscious feed-forward system that is functionally equivalent to a conscious complex; and finally, an example showing that concepts within a complex are self-referential and relate only indirectly to the external environment.
      section#theory-and-models
        h1.page-header.unnumbered Theory and Models
        .anchor#axioms-postulates-and-identities
          h2.unnumbered Axioms, postulates, and identities
        p
          | The main tenets of IIT can be presented as a set of phenomenological axioms, ontological postulates, and identities.
        .anchor#axioms
          h3.unnumbered Axioms
        p The central axioms, which are taken to be immediately evident, are as follows:
        dl.dl-horizontal
          dt Existence.
          dd Consciousness exists---it is an undeniable aspect of reality. Paraphrasing Descartes, “I experience therefore I am”.
          dt Composition.
          dd Consciousness is structured: each experience consists of multiple aspects in various combinations. Within the same experience, one can see, for example, left and right, red and blue, a triangle and a square, a red triangle on the left, a blue square on the right, and so on.
          dt Information.
          dd Consciousness is informative: each experience differs in its particular way from other possible experiences. Thus, an experience of pure darkness is what it is by differing, in its particular way, from an immense number of other possible experiences. A small subset of these possible experiences includes, for example, all the frames of all possible movies.
          dt Integration.
          dd Consciousness is integrated: each experience is (strongly) irreducible to non-interdependent components. Thus, experiencing the word “SONO” written in the middle of a blank page is irreducible to an experience of the word “SO” at the right border of a half-page, plus an experience of the word “NO” on the left border of another half page---the experience is whole. Similarly, seeing a red triangle is irreducible to seeing a triangle but no red, plus a red patch but no triangle.
          dt Exclusion.
          dd Consciousness is exclusive: each experience excludes all others---at any given time there is only one experience having its full content, rather than a superposition of multiple partial experiences; each experience has definite borders---certain things can be experienced and others cannot; each experience has a particular spatial and temporal grain---it flows at a particular speed, and it has a certain resolution such that some distinctions are possible and finer or coarser distinctions are not.
        .anchor#postulates
          h3.unnumbered Postulates
        p To parallel the phenomenological axioms, IIT posits ontological postulates. These list the set of properties physical systems must satisfy in order to generate experience.
        dl.dl-horizontal
          dt Existence.
          dd Mechanisms in a state exist. A system is a set of mechanisms.
          dt Composition.
          dd Elementary mechanisms can be combined into higher order ones.
        p The next three postulates, information, integration, and exclusion, apply first to individual mechanisms and then to systems of mechanisms.
        .anchor#mechanisms
          h3.unnumbered Mechanisms
        dl.dl-horizontal
          dt Information.
          dd A mechanism can contribute to consciousness only if it specifies “differences that make a difference” within a system. That is, a mechanism in a state generates information only if it constrains the states of a system that can be its possible causes and effects---its
            em  cause-effect repertoire.
            | The more selective the possible causes and effects, the higher the
            em  cause-effect information cei 
            | specified by the mechanism.
          dt Integration.
          dd A mechanism can contribute to consciousness only if it specifies a cause-effect repertoire (information) that is
            em  irreducible 
            | to independent components.
            em  Integration/irreducibility 
            span.math  \(\varphi\)
            |  is assessed by partitioning the mechanism and measuring what difference this makes to its cause-effect repertoire.
          dt Exclusion.
          dd A mechanism can contribute to consciousness at most one cause-effect repertoire---its
            em  maximally irreducible 
            | cause-effect repertoire (MICE, or
            em  quale sensu stricto 
            | (in the narrow sense of the word,
            span.citation
            | )). If the MICE exists, the mechanism constitutes a
            em  concept.
            |
        .anchor#systems-of-mechanisms
          h3.unnumbered Systems of mechanisms
        dl.dl-horizontal
          dt Information.
          dd A set of elements can be conscious only if its mechanisms specify a set of “differences that make a difference”---i.e. a
            em  conceptual structure.
            | A conceptual structure is a
            em  constellation 
            | of points in concept space, where each axis is a possible past/future state of the set of elements, and each point is a concept specifying differences that make a difference within the set. The higher the number of different concepts and their
            span.math  \(\varphi^{\rm Max}\)
            |  value, the higher the
            em  conceptual information CI 
            | that specifies a particular constellation and distinguishes it from other possible constellations.
          dt Integration.
          dd A set of elements can be conscious only if its mechanisms specify a conceptual structure that is
            em  irreducible 
            | to independent components (strong integration).
            em  Strong integration/irreducibility 
            span.math  \(\Phi\)
            |  is assessed by partitioning the set of elements into subsets with unidirectional cuts.
          dt Exclusion.
          dd Of all overlapping sets of elements, only one set can be conscious---the one whose mechanisms specify a conceptual structure that is
            em  maximally irreducible (MICS) 
            | to independent components. A local maximum of integrated information
            span.math  \(\Phi^{\rm Max}\)
            |  (over elements, space, and time) is called a
            em  complex.
            |
        .anchor#identities
          h3.unnumbered Identities
        p
          | Finally, IIT posits identities between phenomenological aspects and informational/causal aspects of physical systems. The central identity is the following:
        p
          | The maximally irreducible conceptual structure (
          em MICS) 
          | generated by a complex of elements is identical to its experience. The constellation of concepts of the MICS completely specifies the quality of the experience (its
          em  quale ``sensu lato" 
          | (in the broad sense of the term
          span.citation
          | )). Its irreducibility
          span.math  \(\Phi^{\rm Max}\)
          |  specifies its quantity. The maximally irreducible cause-effect repertoire (MICE) of each concept within a MICS specifies what the concept is about (what it contributes to the quality of the experience, i.e. its
          em  quale sensu stricto 
          | (in the narrow sense of the term)), while its value of irreducibility
          span.math  \(\varphi^{\rm Max}\)
          |  specifies how much the concept is present in the experience. An experience is thus an
          em  intrinsic property 
          | of a complex of mechanisms in a state. In other words, the maximally irreducible conceptual structure specified by a complex exists intrinsically (from its own intrinsic perspective), without the need for an external observer.
        .anchor#mechanisms-1
          h2.unnumbered Mechanisms
        p
          | In what follows, we consider simple systems that can be used to illustrate the postulates of IIT. In the first part, we apply the postulates of IIT at the level of
          em  individual mechanisms.
          | We show that an individual mechanism generates information by specifying both selective causes and effects (information), that it needs to be irreducible to independent components (integration), and that only the most irreducible cause and effect of each mechanism should be considered (exclusion). This allows us to introduce the notion of a
          em  concept 
          | : the maximally irreducible cause-effect repertoire of a mechanism.
        p
          | In the next part, we consider the postulates of IIT at the level of
          em  systems of mechanisms,
          | and show how the requirements for information, integration, and exclusion can be satisfied at the system level. This allows us to introduce the notion of a
          em  complex 
          |---a maximally integrated set of elements---and of a
          em  quale 
          |---the maximally irreducible conceptual structure (MICS) it generates. Altogether, these two sections show how to assess in a step by step, bottom up manner, whether a system generates a maximally integrated conceptual structure and how the latter can be characterized in full. A summary of the key concepts and associated measures is provided as a reference in Table 1.
        p [!h]
        p
          span |l|l|
          | &
          br
          br
          br
          | &
          br
          strong
            | Cause-effect information (
            span
              em  cei)
            |:
          | &
          strong
            | Conceptual information (
            span
              em  CI)
            |:
          br
          | How a mechanism in a state specifies & How a set of mechanisms specifies
          br
          | the probability of past and future states & the constellation of concepts
          br
          | of a set of elements (cause-effect repertoires) & of the set (conceptual structure)
          br
          | &
          br
          br
          br
          | &
          br
          strong
            | Integrated information (<span class="math">\(\varphi\)</span>, “small phi”):
          | &
          strong
            | Integrated conceptual information (<span class="math">\(\Phi\)</span>, “big phi”):
          br
          | How irreducible the cause-effect repertoire & How irreducible the conceptual structure
          br
          | specified by a mechanism is compared to its & specified by a set of mechanisms is compared to its
          br
          | minimum information partition (MIP) & minimum information partition (MIP)
          br
          | &
          br
        p
          br
          br
          | &
          br
          strong
            | Concept (<span class="math">\(\varphi^{Max}\)</span>):
          | &
          strong
            | Complex (<span class="math">\(\Phi^{Max}\)</span>):
          br
          | A mechanism that specifies & A set of elements whose mechanisms specify
          br
          | a maximally irreducible cause-effect repertoire & a maximally irreducible conceptual structure
          br
          | (MICE or quale ``sensu stricto“) & (MICS or quale ``sensu lato”)
          br
          | &
          br
        .anchor#existence
          h3.unnumbered Existence
        p
          embed(src='Figures/IIT3_Fig1.pdf')
        p [fig:existence]
        p
          | The existence postulate, the “zeroth” postulate of IIT, claims that mechanisms in a state exist. Within the present framework, “mechanism” simply denotes anything having a causal role within a system, for example, a neuron in the brain, or a logic gate in a computer. In principle, mechanisms might be characterized at various spatio-temporal scales, down to the micro-physical level, although for any given system there will be a scale at which causal interactions are strongest
          span.citation
          | . In what follows, we consider systems in which the elementary mechanisms are discrete logic gates or linear threshold units (Methods S1) and assume that these mechanisms are the ones mediating the strongest causal interactions.
        p
          | Fig. [fig:existence]A shows the example system
          span.math  \(ABCDEF\)
          | , which includes three logic gate mechanisms, OR, AND, XOR, which will be used to illustrate the postulates of IIT throughout the Model section. The dotted circle indicates that the particular set of elements
          span.math  \(ABC\)
          |  is going to be considered as a “candidate set” for IIT analysis, whereas the remaining elements
          span.math  \(D\)
          | ,
          span.math  \(E\)
          | ,
          span.math  \(F\)
          |  are considered external and treated as boundary conditions (Methods S1).
        p
          | The mechanisms of
          span.math  \(ABC\)
          |  determine the transition probability matrix of the candidate set (TPM), which specifies the probability with which any state of the set
          span.math  \(ABC\)
          |  transitions into any other state (Fig. [fig:existence]B). In this case, since the system is deterministic, the values in the TPM are 0 or 1, but non-deterministic systems can also be considered. In this example, at the current time step
          span.math  \(t_0\)
          | , the mechanisms are in state
          span.math  \(ABC=100\)
          | . The TPM specifies which past states could have led to the current state
          span.math  \(ABC=100\)
          |  (the shaded column in Fig. [fig:existence]B) and which future states it could go to (shaded row in Fig. [fig:existence]B), out of all possible states of the set.
        .anchor#composition
          h3.unnumbered Composition
        p
          embed(src='Figures/IIT3_Fig2.pdf')
        p [fig:composition]
        p
          | Composition states that elementary mechanisms can be structured, forming higher order mechanisms in various combinations. In Fig. [fig:composition],
          span.math  \(A\)
          | ,
          span.math  \(B\)
          | , and
          span.math  \(C\)
          |  are the elementary (first-order) mechanisms. By combining them, higher order mechanisms can be constructed. Pairs of elements form second-order mechanisms (<span class="math">\(AB\)</span>,
          span.math  \(AC\)
          | ,
          span.math  \(BC\)
          | ), and all elements together form the third-order mechanism
          span.math  \(ABC\)
          | . A red area highlights the respective mechanisms in Fig. [fig:composition]. The elements inside the candidate set, but outside the mechanism under consideration, are treated as independent noise sources (Methods S1). Altogether, the elementary mechanisms and their combinations form the
          em  power set 
          | of possible mechanisms.
        .anchor#information-cause-effect-repertoires-and-cause-effect-information-cei
          h3.unnumbered
          | Information: cause-effect repertoires and cause-effect information (<span class="math">\(cei\)</span>)
        p
          embed(src='Figures/IIT3_Fig3.pdf')
        p [fig:selective]
        p
          | In IIT, information is meant to capture the “differences that make a difference” from the perspective of the system itself---and is therefore both causal and intrinsic. These and other features distinguish this ``intrinsic“ notion of information from the ``extrinsic”, Shannon notion (see Text S2; cf.
          span.citation
          | for related approaches to information and causation in networks).
        p
          | Information as “differences that make a difference” to a system from its intrinsic perspective can be quantified by considering how a mechanism in its current state
          span.math  \(s_0\)
          |  constrains the system’s potential past and future states. Fig. [fig:selective] illustrates how a mechanism
          span.math  \(A\)
          |  constrains the past states of
          span.math  \(BCD\)
          |  more or less
          em  selectively 
          | depending on its input/output function and state.
          span.math  \(A\)
          |  is an AND gate of the inputs from
          span.math  \(BCD\)
          | . The constrained distribution of past states is called
          span.math  \(A\)
          | ’s
          em  cause repertoire.
          | In Fig. [fig:selective]A the connections between
          span.math  \(A\)
          |  and
          span.math  \(BCD\)
          |  are substituted by noise. Therefore, the current state of
          span.math  \(A\)
          |  cannot specify anything about the past state of
          span.math  \(BCD\)
          | , the cause repertoire is identical to the unconstrained distribution (unselective), and
          span.math  \(A\)
          |  generates no information. By contrast, when the connections between
          span.math  \(A\)
          |  and
          span.math  \(BCD\)
          |  are deterministic and
          span.math  \(A\)
          |  is on (<span class="math">\(A=1\)</span>), the past state of
          span.math  \(BCD\)
          |  is fully constrained, since the only compatible past state is
          span.math  \(BCD=111\)
          | (Fig. [fig:selective]B). In this case, the cause repertoire is maximally selective, corresponding to high information. On the other hand, when
          span.math  \(A\)
          |  is off (<span class="math">\(A=0\)</span>, Fig. [fig:selective]C), the cause repertoire is less selective, because only
          span.math  \(BCD=111\)
          |  is ruled out, corresponding to less information.
        p
          embed(src='Figures/IIT3_Fig4.pdf')
        p [fig:information]
        p
          | Fig. [fig:information] illustrates how element
          span.math  \(A\)
          |  in state 1 constrains the past states (left) and future states (right) of the candidate set
          span.math  \(ABC\)
          | . The probability distribution of past states that could have been potential causes of
          span.math  \(A=1\)
          |  is its cause repertoire
          span.math  \(p(ABC^p|A^c=1)\)
          | . The probability distribution of future states that could be potential effects of
          span.math  \(A=1\)
          |  is called
          em  effect repertoire 
          span.math  \(p(ABC^f|A^c=1)\)
          | . Here, the superscripts
          span.math  \(^p\)
          | ,
          span.math  \(^c\)
          | , and
          span.math  \(^f\)
          |  stand for past, current, and future, respectively. The set of elements over which the cause and effect repertoires of a mechanism are calculated is called its
          em  purview.
          | Fig. [fig:information] shows the cause/effect repertoire of mechanism
          span.math  \(A=1\)
          |  over its purview
          span.math  \(ABC\)
          |  (the full set) in the past and future, labeled
          span.math  \(A^c/ABC^p\)
          |  and
          span.math  \(A^c/ABC^f\)
          | . If the purview is not over the full set, the elements outside of the purview are unconstrained (see Methods S1 for details on the calculation).
        p
          | The amount of information that
          span.math  \(A=1\)
          |  specifies about the past, its cause information (<span class="math">\(ci\)</span>), is measured as the distance
          span.math  \(D\)
          |  between the cause repertoire
          span.math  \(p(ABC^p|A^c=1)\)
          |  and the unconstrained repertoire
          span.math  \(p^{\rm{uc}}\)
          | . For the purview
          span.math  \(ABC^p\)
          | :
        p
          span.math
            | \[ci(ABC^p|A^c=1) = D(p(ABC^p|A^c=1)|| p^{ {\rm uc}}(ABC^p)) = 0.33. \label{eq:ci}\]
        p
          span.math  \(p^{\rm{uc}}(ABC^p)\)
          |  corresponds to the cause repertoire in the absence of any constraints on the set’s output states due to its mechanisms (unconstrained past distribution), which is the uniform distribution.
        p
          | Just like cause information (<span class="math">\(ci\)</span>), effect information (<span class="math">\(ei\)</span>) of
          span.math  \(A=1\)
          |  is quantified as the distance between the effect repertoire of
          span.math  \(A\)
          |  and the unconstrained future distribution
          span.math  \(p^{\rm{uc}}(ABC^f)\)
          | :
        p
          span.math  \[ei(ABC^f|A^c=1)  =  D(p(ABC^f|A^c=1)|| p^{{\rm uc}}(ABC^f)) = 0.25.\]
        p
          | As can be seen in Fig. [fig:information] (right), the unconstrained future repertoire
          span.math  \(p^{{\rm uc}}(ABC^f)\)
          |  is not simply the uniform distribution of future system states. While
          span.math  \(p^{{\rm uc}}(ABC^p)\)
          |  corresponds to the distribution of past system states with unconstrained outputs,
          span.math  \(p^{{\rm uc}}(ABC^f)\)
          |  corresponds to the distribution of future system states with unconstrained inputs. Therefore,
          span.math  \(p^{{\rm uc}}(ABC^f)\)
          |  is obtained by perturbing the inputs to each element into all possible states. As an example, the unconstrained future distribution of element
          span.math  \(A\)
          | , being an OR gate, is
          span.math  \(p(A=0) = 0.25\)
          |  and
          span.math  \(p(A=1) = 0.75\)
          | , which is obtained by perturbing the inputs of
          span.math  \(A\)
          |  into all possible states
          span.math  \([00, 10, 01, 11]\)
          | .
        p
          | To quantify differences that make a difference, the distance
          span.math  \(D\)
          |  between two probability distributions is evaluated using the earth mover’s distance (EMD), which quantifies how much two distributions differ by taking into account the distance between system states (see Methods S1 for details). This is important because, from the intrinsic perspective of the system, it should make a difference if two system elements differ in their state, instead of just one.
        p
          | Finally, having calculated
          span.math  \(ci(ABC^p|A=1)\)
          |  and
          span.math  \(ei(ABC^f|A=1)\)
          | , the total amount of
          em  cause-effect information 
          | (<span class="math">\(cei\)</span>) specified by
          span.math  \(A=1\)
          |  over the purview
          span.math  \(A/ABC^{p,f}\)
          |  is the minimum of its
          span.math  \(ci\)
          |  and
          span.math  \(ei\)
          | :
        p
          //TODO: fix this math
          span.math \[ $$ \begin{eqnarray} cei\left(ABC^{p,f} | A = 1\right) &=& \min [ci\left(ABC^p | A = 1\right),ei\left(ABC^f | A = 1\right)] \\\\ &=& 0.25.\end{eqnarray} $$ \]
        p
           | The motivation for choosing the minimum is illustrated in Fig. [fig:inputoutput]. First, consider an element that receives inputs from the system but sends no output to it (element
          span.math  \(A\)
          |  in Fig. [fig:inputoutput]A). In this case, the state of element
          span.math  \(A\)
          |  constrains the past states of the system---it has selective causes within it, but not its future states---it has no selective effects on it (what
          span.math  \(A\)
          |  does makes no difference to the system). Put differently, while the state of element
          span.math  \(A\)
          |  does convey information about the system’s past states from the perspective of an external observer, it does not do so from the intrinsic perspective of the system itself, because the system is not affected by
          span.math  \(A\)
          |  (it cannot “observe” it).
        p
          | Similarly, consider an element that only outputs to the system but does not receive inputs from it, being controlled exclusively by external causes (element
          span.math  \(A\)
          | in Fig. [fig:inputoutput]B). In this case, the state of element
          span.math  \(A\)
          |  constrains the future states of the system---it has selective effects on it, but not its past states---it has no selective causes within it (what the system might have done makes no difference to
          span.math  \(A\)
          | ). Put differently, while the state of element
          span.math  \(A\)
          |  does convey information about the system’s future states from the perspective of an external observer, it does not do so from the intrinsic perspective of the system, because the system cannot affect
          span.math  \(A\)
          |  (it cannot “control” it).
        p
          embed(src='Figures/IIT3_Fig5.pdf')
        p [fig:inputoutput]
        .anchor#integration-irreducible-cause-effect-repertoires-and-integrated-information-varphi
          h3.unnumbered
          | Integration: irreducible cause-effect repertoires and integrated information (<span class="math">\(\varphi\)</span>)
        p
          | At the level of an individual mechanism, the integration postulate says that only mechanisms that specify integrated information can contribute to consciousness. Integrated information is information that is generated by the whole mechanism above and beyond the information generated by its parts. This means that, with respect to information, the mechanism is irreducible. Similar to cause-effect information, integrated information
          span.math  \(\varphi\)
          |  (“small phi”) is calculated as the distance
          span.math  \(D\)
          |  between two probability distributions: the cause-effect repertoire specified by the whole mechanism is compared against the cause-effect repertoire of the partitioned mechanism. Of the many possible ways to partition a mechanism, integrated information is evaluated across the minimum information partition (MIP), the partition that makes the least difference to the cause and effect repertoires (in other words, the minimum “difference” partition). In Fig. [fig:integration] this is demonstrated for the
          span.math  \(3^{\rm rd}\)
          |  order mechanism
          span.math  \(ABC\)
          | . The MIP for the purview
          span.math  \(ABC^c/ABC^p, ABC^f\)
          |  is
          span.math  \(ABC^c/ABC^p \to AB^c/C^p \times C^c/AB^p\)
          |  in the past and
          span.math  \(ABC^c/ABC^f \to ABC^c/AC^f \times [ ]/B^f\)
          |  in the future, where
          span.math  \([ ]\)
          |  denotes the empty set. The cause and effect repertoire specified by the partitioned mechanisms can be calculated as:
        p
          span.math  \[p(ABC^p|ABC^c=100 / {\rm MIP}) = p(C^p|AB^c=10) \times p(AB^p|C^c=0),\]
        p and
        p
          span.math  \[p(ABC^f|ABC^c=100 / {\rm MIP}) = p(AC^f|ABC^c=100) \times p(B^f),\]
        p
          | where the connections between the parts are “injected” with independent noise (Methods S1).
        p
          | The distance
          span.math  \(D\)
          |  between the cause-effect repertoire specified by the whole mechanism and its MIP is quantified again using the EMD, taken separately for the past and the future (cause and the effect repertoires):
        p
          span.math
            | \[\varphi_{ {\rm MIP} }(ABC^p|ABC^c=100) = D(p(ABC^p|ABC^c=100) || p(ABC^p|ABC^c=100 / {\rm MIP})) = 0.5,\]
        p
          span.math
            | \[\varphi_{ {\rm MIP} }(ABC^f|ABC^c=100) = D(p(ABC^f|ABC^c=100) || p(ABC^f|ABC^c=100 / {\rm MIP})) = 0.25,\]
        p
          | As with information, the total amount of integrated information of mechanism
          span.math  \(ABC\)
          |  in its current state
          span.math  \(100\)
          |  over the purview
          span.math  \(A/ABC^{p,f}\)
          |  is the minimum of its past and future integrated information:
        p
          span.math
            | \[\varphi_{ {\rm MIP} }(ABC^{p,f}|ABC^c=100)= \min [\varphi_{ {\rm MIP} }(ABC^p|ABC^c = 100), \varphi_{ {\rm MIP} }(ABC^f|ABC^c = 100)] = 0.25,\]
        p
          | In what follows, integrated information
          span.math  \(\varphi\)
          |  is always evaluated for the MIP, so the
          span.math  \(_{\rm MIP}\)
          |  subscript is dropped for readability.
        p
          | According to IIT, mechanisms that do not generate integrated information do not exist from the intrinsic perspective of a system, as illustrated in Fig. [fig:integration
          sub m
          | otivation]. Suppose that
          span.math  \(A\)
          |  is a non-parity gate (<span class="math">\(A\)</span>.turns on when the inputs are even) and
          span.math  \(B\)
          |  is a majority gate (<span class="math">\(B\)</span>.turns on when the majority of its inputs are on). If
          span.math  \(A\)
          |  and
          span.math  \(B\)
          |  have independent causes and independent effects as shown in Fig. [fig:integration
          sub m
          | otivation]A, a higher order mechanism
          span.math  \(AB\)
          |  cannot generate integrated information, since it is possible to partition
          span.math  \(AB\)
          | ’s causes and effects without any loss of information. In this case,
          span.math  \(AB\)
          |  does not exist intrinsically.
        p
          embed(src='Figures/IIT3_Fig6.pdf')
        p [fig:integration]
        p
          | Consider instead Fig. [fig:integration
          sub m
          | otivation]B. Here,
          span.math  \(AB=11\)
          |  specifies that all inputs had to be on in the past (‘All ON’), which goes above and beyond what is specified separately by
          span.math  \(A=1\)
          | (an even number of inputs was on) and by
          span.math  \(B=1\)
          | (the majority of inputs was on). On the effect side, there is an AND gate that takes inputs from both
          span.math  \(A\)
          |  and
          span.math  \(B\)
          | , so the effect of
          span.math  \(AB=11\)
          |  goes above and beyond the separate effects of
          span.math  \(A=1\)
          |  and
          span.math  \(B=1\)
          | . Therefore, mechanism
          span.math  \(AB\)
          |  exists from the intrinsic perspective of the system, in the sense that it plays an irreducible causal role: it picks up a difference that makes a difference to the system in a way that cannot be accounted for by its parts.
        p
          | By contrast, in Fig. [fig:integration
          sub m
          | otivation]C mechanism
          span.math  \(AB\)
          |  does not exist from the intrinsic perspective of the system, because the information ‘All ON’ as such does not make any difference to the future state of the system. Similarly, in Fig. [fig:integration
          sub m
          | otivation]D,
          span.math  \(A=1\)
          |  and
          span.math  \(B=1\)
          |  do not specify an irreducible past cause for the irreducible future effect that the AND gate will be ON.
        p
          embed(src='Figures/IIT3_Fig7.pdf')
        p
          | [fig:integration
          sub m
          | otivation]
        .anchor#exclusion-a-maximally-irreducible-cause-effect-repertoire-mice-specified-by-a-subset-of-elements-a-concept
          h3.unnumbered
          | Exclusion: A maximally irreducible cause-effect repertoire (MICE) specified by a subset of elements (a concept)
        p
          embed(src='Figures/IIT3_Fig8.pdf')
        p
          | [fig:exclusion
          sub e
          | xample]
        p
          | The exclusion postulate at the level of a mechanism says that a mechanism can have only one set of causes and effects---those that are maximally irreducible---other causes and effects are excluded. The
          em  core cause 
          | of a mechanism is its maximally irreducible cause repertoire. Consider for example mechanism
          span.math  \(BC=00\)
          |  in Fig. [fig:exclusion
          sub e
          | xample]. To find the core cause of
          span.math  \(BC\)
          | , one needs to evaluate
          span.math  \(\phi\)
          |  for the whole power set of past purviews
          span.math  \(P = \left\lbrace A^p, B^p, C^p, AB^p, AC^p, BC^p, ABC^p \right\rbrace\)
          | . In this case, the purview
          span.math  \(BC^c/AB^p\)
          |  has the highest value of
          span.math  \( \varphi^{{\rm Max}}(P|BC^c = 00) = 0.33\)
          | . The corresponding maximally irreducible cause repertoire is thus the core cause of
          span.math  \(BC=00\)
          | . The
          em  core effect 
          | is assessed in the same way: it is the maximally irreducible effect repertoire of a mechanism with
          span.math  \(\varphi^{\rm Max}(F|BC^c = 00)\)
          | , where
          span.math  \(F\)
          |  denotes the power set of future purviews. A mechanism that specifies a
          em  maximally irreducible causes and effects (MICE) 
          | constitutes a
          em  concept 
          | or, for emphasis, a
          em  core concept.
          |
        p
          embed(src='Figures/IIT3_Fig9.pdf')
        p
          | [fig:exclusion
          sub p
          | ostulate]
        p
          | To understand the motivation behind the exclusion postulate as applied to a mechanism, consider a neuron with several strong synapses and many weak synapses (Figure S1). From the intrinsic perspective of the neuron, any combination of synapses could be potential causes of firing, including “strong synapses”, “strong synapses plus some weak synapses”, and so on, eventually including the potential cause “all synapses”, ``all synapses plus stray glutamate receptors“, ``all synapses plus stray glutamate receptors plus cosmic rays affecting membrane channels”, and so on, rapidly escalating to infinite regress. The exclusion postulate requires, first, that only one cause exists. This requirement represents a causal version of Occam’s razor, saying in essence that ``causes should not be multiplied beyond necessity“, i.e. that causal superposition is not allowed
          span.citation
          | . Thus,
          em only one 
          | set of synapses can be the cause for the neuron’s firing and not, for example,
          em both 
          | “strong synapses S1,S2”
          em  and 
          | “all synapses”. Second, the exclusion postulate requires that, from the intrinsic perspective of a mechanism in a system, the only cause be the maximally irreducible one. Recall that IIT’s information postulate is based on the intuition that, for something to exist, it must make a difference. By extension, something exists all the more, the more of a difference it makes. The integration postulate further requires that, for a whole to exist, it must make a difference above and beyond its partition, i.e. it must be irreducible. Therefore, the cause that must be singled out as the only one that exists, as dictated by the exclusion postulate, should be the one that, if eliminated by a partition, makes the most difference to the neuron’s output---that is, the one that is maximally irreducible. In Figure S1, for example, the maximally irreducible cause turns out to be “the strong synapses S1,S2”. Note that the exclusion postulate appears to fit with phenomenology also at the level of mechanisms. Thus, invariant concepts such as “chair”, or “apple” seem to exclude the accidental details of particular apples and chairs, but only reflect the ``core” concept. In neural terms, this would imply that the maximally irreducible cause-effect repertoire of the neurons underlying such invariant concepts is similarly restricted to their core causes and effects.
        p
          | The notion of a concept is illustrated in Fig. [fig:exclusion
          sub p
          | ostulate] for mechanism
          span.math  \(A\)
          |  of the logic gate candidate set
          span.math  \(ABC\)
          | . The core cause of
          span.math  \(A\)
          |  is the cause repertoire of purview
          span.math  \(A^c/BC^p\)
          | ; the core effect is the effect repertoire of
          span.math  \(A^c/B^f\)
          | . These purviews generate the maximal amount of integrated information over the whole power set of purviews in the past (<span class="math">\(P\)</span>) and future (<span class="math">\(F\)</span>), respectively. The amount of integrated information generated by concept
          span.math  \(A^c / BC^p,B^f\)
          |  is again the minimum between past and future:
        p
          span.math \[\begin{eqnarray} \varphi^{{\rm Max}}(A^c =1) &=& \min \left[\varphi^{{\rm Max}}(P|A^c=1), \varphi^{{\rm Max}}(F|A^c =1)\right] \\\\ &=& 0.17. \end{eqnarray}\]
        p
          | Each concept of a mechanism in a state is thus endowed with a maximally irreducible cause and effect repertoire (MICE), which specifies what the concept is about (its
          em  quale ``sensu stricto")
          |, and its particular
          span.math  \(\varphi^{{\rm Max}}\)
          |  value, which quantifies its amount of integration or irreducibility.
        .anchor#systems-of-mechanisms-1
          h2.unnumbered Systems of mechanisms
        p
          | We now turn from the level of mechanisms to the level of a system of mechanisms, and apply the postulates of IIT with the objective of deriving the experience or
          em  quale 
          | generated by a system in a bottom up manner, from the set of all its concepts.
        .anchor#information-conceptual-structure-constellation-of-concepts-in-concept-space-and-conceptual-information-ci
          h3.unnumbered
          | Information: Conceptual structure (constellation of concepts in concept space) and conceptual information (<span class="math">\(CI\)</span>)
        p
          | At the system level, the information postulate says that only sets of “differences that make a difference” matter (i.e. a constellations of concepts). Fig. [fig:core
          sub c
          | oncepts] shows all the concepts specified by the candidate set
          span.math  \(ABC\)
          |  (Fig. [fig:core
          sub c
          | oncepts]A,B). Of all the possible mechanisms of the power set of
          span.math  \(ABC\)
          | , only
          span.math  \(AC\)
          |  does not give rise to a concept, since its integrated information
          span.math  \(\varphi^{\rm Max}=0\)
          |  (Fig. [fig:core
          sub c
          | oncepts]B). All other mechanisms generate non-zero integrated information and thus specify concepts (Fig. [fig:core
          sub c
          | oncepts]C). The set of all concepts of a candidate set constitutes its
          em  conceptual structure,
          | which can be represented in
          em  concept space.
          |
        p
          embed(src='Figures/IIT3_Fig10.pdf')
        p
          | [fig:core
          sub c
          | oncepts]
        p
          | Concept space is a high dimensional space, with one axis for each possible past and future state of the system. In this space, each concept is symbolized as a point, or “star”: its coordinates are given by the probability of past and future states in its cause-effect repertoire, and its size is given by its
          span.math  \( \varphi^{{\rm Max}}(P,F|s_0)\)
          |  value. If
          span.math  \(\varphi^{{\rm Max}}\)
          |  is zero, the concept simply does not exist, and if its
          span.math  \( \varphi^{{\rm Max}}\)
          |  is small, it exists to a minimal amount.
        p
          | In the case of the candidate set
          span.math  \(ABC\)
          | , the dimension of concept space is 16 (8 axes for the past states and 8 for the future states). For ease of representation, in the figures past and future subspaces are plotted separately, with only three axes each (corresponding to the states at which the concepts have the highest variance in probability). Therefore the 6 concepts in Fig. [fig:core
          sub c
          | oncepts]D are displayed twice, once in the past subspace and once in the future subspace. In the full 16-dimensional concept space, however, each concept is a single star.
        p
          embed(src='Figures/IIT3_Fig11.pdf')
        p [fig:CIcalculation]
        p
          | At the system level, the equivalent of the cause-effect information (<span class="math">\(cei\)</span>) at the level of mechanisms is called conceptual information (<span class="math">\(CI\)</span>). Just like
          span.math  \(cei\)
          | ,
          span.math  \(CI\)
          |  is quantified by the distance
          em
            span.math  \(D\)
          | from  the unconstrained distribution of past and future states
          span.math  \(p^{uc}\)
          | , which corresponds to the ``null" concept (a concept that specifies nothing):
        p
          span.math  \[CI(C|ABC^c=100) = D(C|ABC^c=100)\|p^{uc}(ABC^{p,f})) = 2.11.\]
        p
          | The distance
          em
            span.math  \(D\)
          | from  a constellation
          span.math  \(C\)
          |  to the ``null" concept can be measured using an extension of the EMD (see Methods S1), which can be understood as the cost of transporting the amount of
          span.math  \( \varphi^{{\rm Max}}\)
          |  of each concept from its location in concept space to
          span.math  \(p^{uc}\)
          | .
          span.math  \(CI\)
          |  is thus the sum of the distances between the cause-effect repertoire of each concept and
          span.math  \(p^{uc}\)
          | , multiplied by the concept’s
          span.math  \( \varphi^{{\rm Max}}\)
          |  value (Fig. [fig:CIcalculation]). Thus, a rich constellation with many different elementary and higher order concepts generates a high amount of conceptual information
          span.math  \(CI\)
          |  (Fig. [fig:CIcalculation]A). By contrast, a system comprised of a single elementary mechanism generates a minimal amount of conceptual information (Fig. [fig:CIcalculation]B).
        p
          | In sum, concepts are considered (metaphorically) as stars in concept space. The conceptual structure
          span.math  \(C\)
          |  generated by a set of mechanisms is thus a constellation of concepts---a particular shape in concept space spanned by the set’s concepts. The more stars, the further away they are from the ``null" concept, and the larger their size, the greater the conceptual information
          em  CI 
          | generated by the constellation
          span.math  \(C\)
          | .
        .anchor#integration-irreducible-conceptual-structure-and-integrated-conceptual-information-phi
          h3.unnumbered
          | Integration: irreducible conceptual structure and integrated conceptual information (<span class="math">\(\Phi\)</span>)
        p
          embed(src='Figures/IIT3_Fig12.pdf')
        p
          | [fig:big
          sub p
          | hi
          sub M
          | IP]
        p
          | At the system level, the integration postulate says that only conceptual structures that are integrated can give rise to consciousness. As for mechanisms, the integration or irreducibility of the constellation of concepts
          span.math  \(C\)
          |  specified by a set of mechanisms can be assessed by partitioning a set of elements and measuring
          em  integrated conceptual information 
          span.math  \(\Phi\)
          |  as the difference made by the partition (“big phi”, as opposed to “small phi”
          span.math  \(\varphi\)
          |  at the level of mechanisms).
        p
          | Partitioning at the system level amounts to noising the connections from one subset
          span.math  \(S1\)
          |  of
          span.math  \(S\)
          |  to its complement
          span.math  \(S\setminus S1\)
          | . As for mechanisms, whether and how much the constellation of concepts generated by a set of mechanisms is irreducible can be assessed with respect to the minimum information partition (MIP) of the set of elements
          span.math  \(S\)
          | . This corresponds to the unidirectional partition that makes the least difference to the constellation of concepts (in other words, the minimum “difference” partition; Fig. [fig:big
          sub p
          | hi
          sub M
          | IP]). To find the unidirectional MIP, for each subset
          span.math  \(S1\)
          |  one must evaluate both the connections from
          span.math  \(S1\)
          |  to
          span.math  \(S\setminus S1\)
          span
            em  and 
          | the connections from
          span.math  \(S\setminus S1\)
          |  to
          span.math  \(S1\)
          |  and take the minimum MIP. This corresponds, at the level of mechanisms, to finding the minimum of the MIPs with respect to the cause
          span
            em  and 
          | the effect repertoires. Therefore a set of elements
          span.math  \(S\)
          |  and its associated constellation is integrated if and only if each subset of elements specifies both selective causes and selective effects about its complement in
          span.math  \(S\)
          | . Similar to integrated information
          span.math  \(\varphi\)
          |  for a mechanism, integrated conceptual information
          span.math  \(\Phi\)
          |  for a set of elements is defined as the distance
          em  D 
          | between the constellation of the whole set and that of the partitioned set:
        p
          span.math  \[\Phi_{\rm MIP}(C|s_0) = D(C\| C_{\rm MIP}^{\rightarrow}),\]
        p
          | where
          span.math  \(C_{\rm MIP}^\rightarrow\)
          |  denotes the constellation of the partitioned set of elements.
        p
          | The extended EMD between the whole and the partitioned constellation corresponds to the minimal cost of transforming
          span.math  \(C\)
          |  into
          span.math  \(C_{\rm MIP}^{\rightarrow}\)
          |  in concept space. Through the partition, concepts of
          span.math  \(C\)
          |  may change location, lose
          span.math  \(\varphi^{{\rm Max}}(P,F|s_0)\)
          | , or disappear. Their
          span.math  \(\varphi^{{\rm Max}}(P,F|s_0)\)
          |  has to be allocated to fill the concepts in
          span.math  \(C_{\rm MIP}^{\rightarrow}\)
          |  with an associated cost of transportation that is proportional to the distance in concept space and the amount of
          span.math  \(\varphi^{{\rm Max}}\)
          |  that is moved. Any residual
          span.math  \( \varphi^{{\rm Max}}\)
          |  is transported to the ``null" concept (<span class="math">\(p^{uc}\)</span>) under the same cost of transportation.
        p
          | Fig. [fig:big
          sub p
          | hi
          sub M
          | IP] shows the conceptual structure for the candidate system
          span.math  \(ABC\)
          |  and its MIP (see Methods S1 for a calculation of
          span.math  \(\Phi_{\rm MIP}(C(ABC)|100)\)
          | ). In this case, 4 of the 6 concepts of
          span.math  \(ABC\)
          |  are lost through the partition; their
          span.math  \( \varphi^{{\rm Max}}(P,F|s_0)\)
          |  is thus transported to the location of the ``null" concept (<span class="math">\(p^{uc}\)</span>). Since
          span.math  \(\Phi\)
          |  will always be evaluated over the MIP, in what follows the subscript
          span.math  \(_{\rm MIP}\)
          |  is dropped, as it was for
          span.math  \(\varphi\)
          | .
        p
          embed(src='Figures/IIT3_Fig13.pdf')
        p
          | [fig:integration
          sub s
          | ystem]
        p
          | The motivation for integration at the system level is illustrated in Fig. [fig:integration
          sub s
          | ystem] (as was done for mechanisms in Fig. [fig:integration]). The set of 6 elements shown in Fig. [fig:integration
          sub s
          | ystem]A can be subdivided into two independent subsets of 3 elements, each with its independent set of concepts. Therefore, a minimum partition between the two subsets makes no difference and integrated conceptual information
          span.math  \(\Phi = 0\)
          | . Since the set is reducible without any loss, it does not exist intrinsically---it can only be treated as ``one“ system from the extrinsic perspective of an observer. By contrast, the set in Fig. [fig:integration
          sub s
          | ystem]B is irreducible because each part specifies both causes and effects in the other part. Two other possibilities are that a subset specifies causes, but not effects, in the rest of the set (Fig.[fig:integration
          sub s
          | ystem]C), or only effects, but not causes (Fig. [fig:integration
          sub s
          | ystem]D). In the case of unidirectional connections the subset is integrated ``weakly” rather than ``strongly“ (in analogy with weak and strong connectedness in graph theory, e.g.
          span.citation
          | ). Therefore, the subset is not really an ``integral” part of the set, but merely an ``appendix“. As an analogy, take the executive board of a company. An employee who transcribes the recording of a board meeting is obviously affected by the board, but if he has no way to provide any feed-back, he should not be considered an ``integral” part of the board, which has no way of knowing that he exists and what he does. The same obtains for an employee who prints the agenda for the board meeting, if the board has no way of giving him feedback about the agenda.
        .anchor#exclusion-a-maximally-irreducible-conceptual-structure-mics-specified-by-a-set-of-elements-a-complex
          h3.unnumbered
          | Exclusion: a maximally irreducible conceptual structure (MICS) specified by a set of elements (a complex)
        p
          | The exclusion postulate at the level of systems of mechanisms says that only a conceptual structure that is
          span
            em  maximally 
          | irreducible can give rise to consciousness---other constellations generated by overlapping elements are excluded. A
          em  complex 
          | is thus defined as a set of elements within a system of mechanisms that generates a local maximum of integrated conceptual information
          span.math  \(\Phi^{\rm Max}\)
          |  (meaning that it has maximal
          span.math  \(\Phi\)
          |  as compared to all overlapping sets of elements). Only a complex exists as an entity from the intrinsic perspective. Consequently, complexes cannot overlap and at each point in time, an element/mechanism can belong to one complex only (complexes should be evaluated as maxima of integrated information not only over elements, but also over spatial and temporal grains
          span.citation
          | , but here it is assumed that the binary elements and time intervals considered in the examples are optimal). Once a complex has been identified, concept space can be called “
          em  qualia space
          | ,” and the constellation of concepts can be called a ``
          em quale ‘sensu lato’ 
          | ". A quale in the broad sense of the word is therefore a
          em  maximally irreducible conceptual structure (MICS) 
          | or, alternatively, an
          em  integrated information structure.
          |
        p
          | To determine whether an integrated set of elements is a complex,
          span.math  \(\Phi\)
          |  must be evaluated for all possible candidate sets (subsets of the system) (Fig. [fig:complex]). As mentioned above, when a set of elements within the system is assessed, the other elements are treated as boundary conditions (see Methods S1). Fig. [fig:complex] shows the values of
          span.math  \(\Phi(C|s_0)\)
          |  for all possible candidate sets that are subsets of
          span.math  \(ABC\)
          | (<span class="math">\(AB\)</span>,
          span.math  \(AC\)
          | ,
          span.math  \(BC\)
          | ,
          span.math  \(ABC\)
          | ) and for one superset (<span class="math">\(ABCD\)</span>). The latter, and all other sets that include elements
          span.math  \(D\)
          | ,
          span.math  \(E\)
          | , or
          span.math  \(F\)
          | , have
          span.math  \(\Phi=0\)
          | . This is because
          span.math  \(D\)
          | ,
          span.math  \(E\)
          | , and
          span.math  \(F\)
          |  are not strongly integrated with the rest of the system. Single elements are not taken into account as candidate sets since they cannot be partitioned and thus cannot be complexes by definition. In this example, the set of elements
          span.math  \(ABC\)
          |  generates the highest value of
          span.math  \(\Phi^{\rm Max}\)
          |  and is therefore the complex. By the exclusion postulate (“of all overlapping sets of elements, only one set can be conscious”), only
          span.math  \(ABC\)
          |  “exists” intrinsically, and other overlapping sets of elements within the system cannot ``exist" intrinsically at the same time (they are excluded).
        p
          embed(src='Figures/IIT3_Fig14.pdf')
        p [fig:complex]
        .anchor#identity-between-an-experience-and-a-maximally-irreducible-conceptual-structure-mics-or-quale-sensu-lato-generated-by-a-complex
          h3.unnumbered
          | Identity between an experience and a maximally irreducible conceptual structure (MICS or quale ``sensu lato") generated by a complex
        p
          | The notions and measures related to the information, integration, and exclusion postulates, both at the level of mechanisms and at the level of systems of mechanisms, are summarized in Table 1. On this basis, it is possible to formulate the central identity proposed by IIT:
          em
            | an experience is identical with the maximally irreducible conceptual structure (MICS, integrated information structure, or quale ``sensu lato") specified by the mechanisms of a complex in a state
          | . Subsets of elements within the complex constitute the concepts that make up the MICS. The maximally irreducible cause-effect repertoire (MICE) of each concept specifies what the concept is about (what it contributes to the quality of the experience, i.e. its
          em  quale ``sensu stricto" 
          | (in the narrow sense of the term)). The value of irreducibility
          span.math  \(\varphi^{\rm Max}\)
          |  of the concepts specifies how much the concept is present in the experience. An experience (i.e. consciousness) is thus an
          em  intrinsic property 
          | of a complex of elements in a state: how they constrain---in a compositional manner---its space of possibilities, in the past and in the future.
        p
          | In Fig. [fig:FancyQuale], this identity is illustrated by showing an isolated system of physical mechanisms
          span ABC
          | in a particular state (bottom left). The above analysis allows one to determine that in this case the system does constitute a complex, and that it specifies a MICS or quale (top right). As before, the constellation of concepts in qualia space is plotted over 3 representative axes separately for past and future states of the system. For clarity, the concepts are also represented as probability distributions over all 16 past and future states (cause-effect repertoires, bottom right).
        p
          | The central identity of IIT can also be formulated to express the classic distinction between
          em  level 
          | and
          em  content 
          | of consciousness
          span.citation
          | : the quantity or level of consciousness corresponds to the
          span.math  \(\Phi^{{\rm Max}}\)
          |  value of the quale; the quality or content of the experience corresponds to the particular constellation of concepts that constitutes the quale---a particular shape in qualia space. Note that, by specifying the quality of an experience, the particular shape of each constellation also distinguishes it from other possible experiences, just like the particular shape of a tetrahedron is what makes it a tetrahedron and distinguishes it from a cube, an icosahedron, and so on.
        p
          | As indicated by the figure, once a phenomenological analysis of the essential properties (axioms) of consciousness has been translated into a set of postulates that the physical mechanisms generating consciousness must satisfy, it becomes possible to invert the process: One can now ask, for any set of physical mechanisms, whether it is associated with phenomenology (is there ``something it is like to be it," from its own intrinsic perspective), how much of it (the quantity or level of consciousness), and of which kind (the quality or content of the experience). As also indicated by the figure, these phenomenological properties should be considered as intrinsic properties of physical mechanisms arranged in a certain way, meaning that a complex of physical mechanisms in a certain state is necessarily associated with its quale.
        p
          embed(src='Figures/IIT3_Fig15.pdf')
        p [fig:FancyQuale]
      section#results-and-discussion
        h1.page-header.unnumbered Results and Discussion
        p
          | The Theory and Models section presented a way of constructing the experience or quale generated by a system of mechanisms in a state in a step-by-step, bottom up manner. The next section explores several implications of the postulates and concepts introduced above using example systems of mechanisms and the conceptual structures they generate.
        .anchor#a-system-may-condense-into-a-main-complex-and-several-minor-complexes
          h2.unnumbered A system may condense into a main complex and several minor complexes
        p
          embed(src='Figures/IIT3_Fig16.pdf')
        p
          | [fig:minor
          sub c
          | omplex]
        p
          | In Fig. [fig:minor
          sub c
          | omplex], the previous example system
          span.math  \(ABC\)
          |  has been embedded within a larger network. In the larger system, elements
          span.math  \(I\)
          | ,
          span.math  \(J\)
          | , and
          span.math  \(L\)
          |  cannot be a part of the complex because they lack either inputs or outputs, or both.
          span.math  \(H\)
          |  and
          span.math  \(K\)
          |  also cannot be part of the complex, since they are connected to the rest of the system in a strictly feed-forward manner. Nevertheless, elements
          span.math  \(H\)
          |  and
          span.math  \(K\)
          |  act as boundary conditions for the rest of the system. The remaining elements
          span.math  \(ABCDEFG\)
          |  cannot form a complex as a whole, since the subset of elements
          span.math  \(FG\)
          |  is not connected to the rest of the system. The subset of elements
          span.math  \(ABCDE\)
          |  does generate a small amount of integrated conceptual information
          span.math  \(\Phi\)
          |  and could thus potentially form a complex. Among the power set of elements
          span.math  \(ABCDE\)
          | , however, it is the smaller subset
          span.math  \(ABC\)
          |  that generates the local maximum of
          span.math  \(\Phi^{\rm Max}\)
          | . This excludes
          span.math  \(ABCDE\)
          |  from being a complex, since an element can participate to only one complex at each point in time. The remaining elements
          span.math  \(DE\)
          | , however, can still form a
          em  minor complex,
          | with lower
          span.math  \(\Phi^{{\rm Max}}\)
          |  than
          span.math  \(ABC\)
          | . Thus,
          span.math  \(ABCDE\)
          |  condenses down to the major complex
          span.math  \(ABC\)
          | , the minor complex
          span.math  \(DE\)
          | , and their residual interactions. Finally,
          span.math  \(FG\)
          |  forms a minor complex that does not interact with the rest of the system.
        p
          | This simple example of ``condensation“ into major and minor complexes may be relevant also for much more complicated systems of interconnected elements. For example, IIT predicts that in the human brain there should be a dominant ``main” complex of high
          span.math  \(\Phi^{{\rm Max}}\)
          | , constituted of neural elements within the cortical system, which satisfies the postulates described above and generates the changing qualia of waking consciousness
          span.citation
          | . The set of neuronal elements constituting this main complex is likely to be dynamic
          span.citation
          | , at times including and at times excluding particular subsets of neurons. Through its interface elements (called “ports-in” and “ports-out”), this main complex receives inputs and provides outputs to a vast number of smaller systems involved in parsing inputs and planning and executing outputs. While interacting with the main complex in both directions, many of these smaller systems may constitute minor complexes specifying little more than a few concepts, which would qualify them as ``minimally conscious" (see below). In the healthy, adult human brain the qualia and
          span.math  \(\Phi^{\rm Max}\)
          |  generated by the dominant main complex are likely to dwarf those specified by the minimally conscious minor complexes. In addition to the fully conscious main complex and minimally conscious minor complexes, there will be a multitude of unconscious processes mediated by purely feed-forward systems (see below) or by the residual interactions between main complex and minor complexes, as in Fig. [fig:minor
          sub c
          | omplex].
        p
          | Under special circumstances, such as after split brain surgery, the main complex may split into two main complexes, both having high
          span.math  \(\Phi^{{\rm Max}}\)
          | . There is solid evidence that in such cases consciousness itself splits in two individual consciousnesses that are unaware of each other
          span.citation
          | . A similar situation may occur in dissociative and conversion disorders, where splits of the main complex may be functional and reversible rather than structural and permanent
          span.citation
          | .
        p
          | An intriguing dilemma is posed by behaviors that would seem to require a substantial amount of cognitive integration, such as semantic judgments (e.g.
          span.citation
          | ). Such behaviors are usually assumed to be mediated by neural systems that are unconscious, because they can be shown to occur under experimental conditions, such as continuous flash suppression, where the speaking subject is not aware of them and cannot report about them. If such behaviors were carried out in a purely feed-forward manner, they would indeed qualify as unconscious in IIT (see below). However, at least some of these behaviors may constitute the output of minor complexes separated from the main one. According to IIT such minor complexes, if endowed with non-trivial values of
          span.math  \(\Phi^{{\rm Max}}\)
          | , should be considered
          em  paraconscious 
          | (i.e. conscious ``on the side" of the conscious subject) rather than unconscious. In principle, the presence of paraconscious minor complexes could be demonstrated by developing experimental paradigms of dual report.
        p
          | In brains substantially different from ours many other scenarios may occur. For example, the nervous system of highly intelligent invertebrates such as the octopus contains a central brain as well as large populations of neurons distributed in the nerve cords of its arms. It is an open question whether such a brain would give rise to a large, distributed main complex, or to multiple major complexes that generate separate consciousnesses. Similar issues apply to systems composed of non-neural elements, such as ant colonies, computer architectures, and so on. While determining rigorously how such systems condense in terms of major and minor complexes, and what kind of MICS they may generate, is not practically feasible, the predictions of IIT are in principle testable and should lead to definite answers.
        .anchor#consciousness-and-connectivity-modular-homogeneous-and-specialized-networks
          h2.unnumbered Consciousness and connectivity: modular, homogeneous, and specialized networks
        p
          | Whether a set of elements as a whole constitutes a complex or decomposes into several complexes depends first of all on the connectivity among its elementary mechanisms. In Fig. [fig:examples] we show the complexes and the associated MICS of three simple networks, representative of a modular, homogeneous, and specialized system architecture.
        p
          | Fig. [fig:examples]A (top) shows a ``modular" network of 3 COPY (<span class="math">\(ACE\)</span>) and 3 AND (<span class="math">\(BDF\)</span>) logic gates. In this network, the system as a whole is not a complex, despite being integrated due to the presence of inter-connections among all elements. Instead, each of the three modules (<span class="math">\(AB\)</span>,
          span.math  \(CD\)
          | , and
          span.math  \(EF\)
          | ) that consist of 1 COPY and 1 AND gate constitutes a complex, because each generates more
          span.math  \(\Phi\)
          |  than the whole system, although each module has just two concepts. The purviews of module
          span.math  \(AB\)
          | ’s concepts are shown in Fig. [fig:examples]A (middle), and their representation in qualia space is displayed in Fig. [fig:examples]A (bottom).
        p
          | Fig. [fig:examples]B shows a ``homogeneous" network of 5 OR gates (<span class="math">\(ABCDE\)</span>), in which every element is connected to every other element including itself. Since all elements in the network specify the same cause-effect repertoire, their 5
          span.math  \(1^{st}\)
          |  order (elementary) concepts are identical. Moreover, there are no higher order concepts, since combining elements yields nothing above the cause-effect information generated by each elementary mechanism. In qualia space, the 5 identical concepts are concentrated on a single point (Fig. [fig:examples]B, bottom). Accordingly, the homogeneous network has a low value of CI and
          span.math  \(\Phi^{\rm Max}\)
          | .
        p
          | Fig. [fig:examples]C shows a ``specialized" network consisting of 5 majority gates, which turn on when the majority of inputs is on. However, each gate has only 3 afferent and efferent connections, which differ for every element. Therefore, each elementary concept specifies a different cause-effect repertoire. For the same reason, there are many higher order concepts (all but the highest order concept of the power set). The specialized network thus gives rise to a rich constellation in qualia space (Fig. [fig:examples]C, bottom) with a high value of CI and
          span.math  \(\Phi^{\rm Max}\)
          | .
        p
          | The example in Fig. [fig:examples]A showing that a network can be interconnected, either directly or indirectly, yet condense into a number of mini-complexes of low
          span.math  \(\Phi^{\rm Max}\)
          |  if its architecture is primarily modular, is potentially consistent with neuropsychological evidence. As mentioned in the Introduction, the cerebellum is a paramount example of a complicated neuronal network, comprising even more neurons than the cerebral cortex, that does not give rise to consciousness or contribute to it
          span.citation
          | . This paradox could be explained by its anatomical and physiological organization, which seems to be such that small cerebellar modules process inputs and produce outputs largely independent of each other
          span.citation
          | . By contrast, a prominent feature of the cerebral cortex, which instead can generate consciousness, is that it is comprised of elements that are functionally specialized and at the same time can interact rapidly and effectively
          span.citation
          | . This is the kind of organization that yields a comparatively high value of
          span.math  \(\Phi^{\rm Max}\)
          |  in the simple example of Fig.[fig:examples]C. Finally, the example in Fig.[fig:examples]B, where connections are abundant but are organized in a homogeneous manner, may also have neurobiological counterparts. For instance, during deep slow wave sleep or in certain states of general anesthesia, the interactions among different cortical regions become highly stereotypical. Due to the characteristic bistability between on and off states of most neurons in the cerebral cortex, even though the anatomical connectivity is unchanged, functional and effective connectivity become virtually homogeneous
          span.citation
          | . Under such conditions, consciousness invariably fades
          span.citation
          | . The examples of Fig.[fig:examples]B and C also suggest that both the richness of concepts and the level of consciousness should increase with the refinement of cortical connections during neural development and the associate increase in functional specialization (e.g.
          span.citation
          | ).
        p
          embed(src='Figures/IIT3_Fig17.pdf')
        p [fig:examples]
        .anchor#consciousness-and-activity-inactive-systems-can-be-conscious
          h2.unnumbered Consciousness and activity: inactive systems can be conscious
        p
          | The conceptual structure generated by a complex depends not only on the connectivity among its elements and the input/output function they perform, but also on their current state. An important corollary of IIT is that both active and inactive elements can contribute to its conceptual structure. In general, elements that are on (1) specify ``positive“ elementary concepts, whereas elements that are off (0) specify ``negative” elementary concepts, both of which contribute to the shape of the MICS. Moreover, high-order concepts will often be specified by subsets including both active and inactive elements.
        p
          | In Fig. [fig:physiol], the system ABCD, comprised of 4 COPY gates, illustrates that a set of elements can form a complex and specify a MICS even though
          span
            em  all 
          | of its elements are in state ‘0’ (off). This is because inactive elements, too, can selectively constrain past and future states of the system (as opposed to “inactivated” or non-functional elements, which cannot change state and thus cannot generate information). For example, element
          span.math  \(A=0\)
          |  specifies an irreducible cause (<span class="math">\(D\)</span>.had to be off at
          span.math  \(t_{-1}\)
          | ) and an irreducible effect (<span class="math">\(B\)</span>.will be on at
          span.math  \(t_{+1}\)
          | ) within the complex. Thus, IIT predicts that, even if all the neurons in a main complex were inactive (or active at a low baseline rate), they would still generate consciousness as long as they are ready to respond to incoming spikes. An intriguing possibility is that a neurophysiological state of near-silence may be approximated through certain meditative practices that aim at reaching a state of ``pure“ awareness without content
          span.citation
          | . This corollary of IIT contrasts with the common assumption that neurons can only contribute to consciousness if they are active in such a way that they can ``signal” or ``broadcast“ the information they represent and ``ignite” fronto-parietal networks
          span.citation
          | . This is because, in IIT, information is not in the message that is broadcasted by an element, but in the shape of the MICS that is specified by a complex.
        p
          | Another corollary of IIT that is relevant to neuroscience is that it is not necessary for the firing state of neurons to percolate or be ``broadcasted“ globally through the entire system generating consciousness for it to contribute to experience. For example, in the system in Fig. [fig:physiol], element
          span.math  \(A\)
          |  does not connect directly to element
          span.math  \(C\)
          | . As a consequence, the activity (or inactivity) of
          span.math  \(A\)
          |  cannot affect
          span.math  \(C\)
          | , and vice versa, within one time step. Nevertheless,
          span.math  \(ABCD\)
          |  still forms a complex and gives rise to a MICS at time
          span.math  \(t_0\)
          | . Thus, according to IIT, the activation or deactivation of a neuron (over the time scale at which integrated information reaches a maximum
          span.citation
          | ) can modify an experience as long as it affects the shape of the MICS specified by the complex to which the neuron belongs, without requiring any global ``broadcast” of signals.
        p
          embed(src='Figures/IIT3_Fig18.pdf')
        p [fig:physiol]
        .anchor#simple-systems-can-be-conscious-a-minimally-conscious-photodiode
          h2.unnumbered Simple systems can be conscious: a ``minimally conscious" photodiode
        p
          | The previous section showed that activations and direct interactions between elements are not necessary to generate a MICS. Taking into account the axioms and postulates of IIT, we can now summarize what it takes to be conscious and give an example of a ``minimally conscious system,“ which will be called a ``minimally conscious” photodiode.
        p
          embed(src='Figures/IIT3_Fig19.pdf')
        p [fig:photo]
        p
          | The ``photodiode“ in Fig. [fig:photo]A consists of two elements: the detector
          span.math  \(D\)
          |  and the predictor
          span.math  \(P\)
          | .
          span.math  \(D\)
          |  receives two external light inputs (and is thus a port-in) and one internal input from
          span.math  \(P\)
          | , all with input strength 1. As illustrated in Fig. [fig:photo]B,
          span.math  \(D\)
          |  turns on if it receives at least two inputs from internal and/or external sources. If
          span.math  \(D\)
          |  has switched on due to sufficiently strong external inputs, it activates element
          span.math  \(P\)
          | , which serves as a ``memory”. At the next time step,
          span.math  \(P\)
          |  acts as a ``predictor" of the next external input to
          span.math  \(D\)
          |  by increasing its sensitivity to light.
        p
          | Simple as it is, the photodiode system satisfies the postulates of IIT: both of its elements specify selective causes and effects within the system (each element about the other one), their cause-effect repertoires are maximally irreducible, and the conceptual structure specified by the two elements is also maximally irreducible. Consequently, the system
          span.math  \(DP=11\)
          |  forms a complex that gives rise to a MICS, albeit one having just two concepts and a
          span.math  \(\Phi^{\rm Max}\)
          |  value of 1 (Fig. [fig:photo]C).
          span.math  \(DP\)
          |  is therefore conscious, albeit minimally so.
        p
          | It is instructive to consider the quality of experience specified by such a minimally conscious photodiode. From an observer’s perspective, the photodiode detects light, but from the intrinsic perspective, the experience is only minimally specified, and in no way can convey the meaning ``light“:
          span.math  \(D\)
          |  says something about
          span.math  \(P\)
          | ’s past and future, and
          span.math  \(P\)
          |  about
          span.math  \(D\)
          | ’s, and that is all. Accordingly, the shape in qualia space is a constellation having just two stars, and is thus minimally specific. This aspect is further emphasized if one considers that different physical systems, say a photodiode activated by blue light (a ``blue” detector), or even a binary thermistor (a ``temperature“ detector) would generate the exact same MICS (Fig.[fig:photo]D) and thus the same minimal experience. Moreover, the symmetry of the MICS implies that the quality of the experience would be the same regardless of the system’s state: the photodiode in state
          span.math  \(DP=00\)
          | ,
          span.math  \(01\)
          | , or
          span.math  \(10\)
          | , receiving one external input, generates exactly the same MICS as
          span.math  \(DP=11\)
          | . In all the above cases, the experience might be described roughly as “it is like this rather than not like this”, with no further qualifications. The photodiode’s experience is thus both quantitatively and qualitatively minimal. Only additional mechanisms that create new concepts and break the symmetries in the shape of the MICS can generate additional meaning. Ultimately, only a set of concepts comparable to that of our main complex can specify the shape of the experience ``light” as it appears to us, and distinguish it from countless other shapes corresponding to different experiences
          span.citation
          | .
        .anchor#complex-systems-can-be-unconscious-a-zombie-feed-forward-network
          h2.unnumbered Complex systems can be unconscious: a ``zombie" feed-forward network
        p
          | Another corollary of IIT is that certain structures do not give rise to consciousness even though they may perform complicated functions. Consider first an ``unconscious" photodiode (Fig.[fig:feedf]A), comprising again the two elements
          span.math  \(D\)
          |  and
          span.math  \(P\)
          | . In this case, however, whether
          span.math  \(D\)
          |  is on or off is determined by external inputs only, and the output of
          span.math  \(P\)
          |  does not feed back into the system. Therefore,
          span.math  \(D\)
          | ’s response to light is just passed through the system, but never comes back to it. Although an observer may describe the two elements
          span.math  \(DP\)
          |  as a system,
          span.math  \(D\)
          |  and
          span.math  \(P\)
          |  do not have both causes and effects within the system
          span.math  \(DP\)
          | , which is thus not a complex, and generates no quale.
        p
          | The same lack of feed-back that disqualifies the unconscious photodiode can be extended, by recursion, to any feed-forward system, no matter how numerous its elements and complicated its connectivity (Fig.[fig:feedf]B). From the viewpoint of an extrinsic observer, the system’s borders can be set arbitrarily. However, the input layer is always determined entirely by external inputs and the output layer does not affect the rest of the system. Consequently, from the intrinsic perspective, both input and output layer cannot be part of the complex. Drawing the system boundaries closer and closer together in a recursive manner, one eventually ends up with just one input and output layer, made up of many “unconscious photodiodes”, and thus generating no quale. Therefore, systems with a purely feed-forward architecture cannot generate consciousness.
        p
          | The idea that ``feed-back“, ``reentry”, or ``recursion“ of some kind may be an essential ingredient of consciousness has many proponents
          span.citation
          | . Recently, it has been suggested that the presence or absence of feed-back could be directly equated with the presence or absence of consciousness
          span.citation
          | . Moreover, several recent studies indicate that an impairment of reentrant interactions over feed-back connections is associated with loss of consciousness during anesthesia
          span.citation
          | and in brain-damaged patients
          span.citation
          | . However, it has been pointed out that the brain (and many other systems) is full of reentrant circuits, many of which do not seem to contribute to consciousness
          span.citation
          | . IIT offers some specific insights with respect to these issues. First, the need for reciprocal interactions within a complex is not merely an empirical observation, but it has theoretical validity because it is derived directly from the phenomenological axiom of (strong) integration. Second, (strong) integration is by no means the only requirement for consciousness, but must be complemented by information and exclusion. Third, for IIT it is the potential for interactions among the parts of a complex that matters and not the actual occurrence of ``feed-back” or ``reentrant“ signaling, as is usually assumed. As was discussed above, a complex can be conscious, at least in principle, even though none of its neurons may be firing, no feed-back or reentrant loop may be activated, and no ``ignition” may have occurred.
        p
          embed(src='Figures/IIT3_Fig20.pdf')
        p [fig:feedf]
        .anchor#conscious-complexes-and-unconscious-zombie-systems-can-be-functionally-equivalent
          h2.unnumbered
          | Conscious complexes and unconscious ``zombie" systems can be functionally equivalent
        p
          | The last section showed that according to IIT feed-forward systems cannot give rise to a quale. However, without restrictions on the number of nodes, feed-forward networks with multiple layers can in principle approximate almost any given function to an arbitrary (but finite) degree
          span.citation
          | . Therefore, it is conceivable that an unconscious system could show the same input-output behavior as a ``conscious" system.
        p
          | An example is shown in Fig.[fig:equivalent]A. A strongly integrated system is compared to a feed-forward network that produces the same input-output behavior over at least 4 time steps (<span class="math">\(9^4\)</span>.input states, Fig.[fig:equivalent]B). To achieve a memory of
          span.math  \(x\)
          |  past time steps in the feed-forward system, the relevant elements were unfolded over time: the state of each element is passed on through a chain of
          span.math  \(x\)
          |  nodes, one node for each of the
          span.math  \(x\)
          |  time steps
          span.citation
          | . In this way, the states of upstream elements in previous time steps can be combined (converge) in a feed-forward manner to determine the state of elements downstream, but can never feed back on elements upstream. As illustrated in the figure, while the recurrent system gives rise to a complex with
          span.math  \(\Phi^{\rm Max} > 0\)
          |  in every state, and would therefore be conscious, the feed-forward system is not a complex and is thus necessarily unconscious.
        p
          | This comparison highlights an important corollary of IIT: whether a system is conscious or not cannot be decided based on its input-output behavior only. In neuroscience, the ability to report is usually considered as the gold standard for assessing the presence of consciousness. Behavior and reportability can be reliable guides under ordinary conditions (typically adult awake humans) and can be employed to evaluate neural correlates of consciousness
          span.citation
          | and to validate theoretical constructs
          span.citation
          | . However, behavior and reportability become problematic for evaluating consciousness in pathological conditions, during development, in animals very different from us, and in machines that may perform sophisticated behaviors
          span.citation
          | . For example, programs running on powerful computers can not only play chess better than humans, but win in difficult question games such as ``Jeopardy“
          span.citation
          | . Moreover, recent advances in machine learning have made it possible to construct simulated networks, primarily feed-forward, that can learn to recognize natural categories such as cats, dogs
          span.citation
          | , pedestrians
          span.citation
          | , and/or faces
          span.citation
          | . Hence, if behavior is the gold standard, it is not clear on what grounds we should deny consciousness to a phone ``assistant” program that can answer many difficult questions, and can even be made to report about her internal feelings, or to a chip that recognizes thousands of different objects as well or better than we do, while granting it to a human who can barely follow an object with his eyes. IIT claims, by contrast, that input-output behavior is not always a reliable guide: one needs to investigate not just ``what“ functions are being performed by a system, but also ``how” they are performed within the system. Thus, IIT admits the possibility of true ``zombies", which may behave more and more like us while lacking subjective experience
          span.citation
          | .
        p
          | The examples of Fig.[fig:equivalent] also suggest that, while it may be possible to build unconscious systems that perform many complex functions, there is an evident evolutionary advantage towards the selection of integrated architectures that can perform the same functions consciously. Among the benefits of integrated architectures are economy of units and wiring, speed, compositionality, context-dependency, memory, and the ability to learn adaptive functions rapidly, flexibly, and building upon previous knowledge
          span.citation
          | . Moreover, in a feed-forward network all system elements are entirely determined by the momentary external input passing through the system. By contrast, a (strongly) integrated system is autonomous, since it can act and react based on its internal states and goals.
        p
          embed(src='Figures/IIT3_Fig21.pdf')
        p [fig:equivalent]
        .anchor#the-concepts-within-a-complex-are-self-generated-self-referential-and-holistic
          h2.unnumbered The concepts within a complex are self-generated, self-referential, and holistic
        p
          embed(src='Figures/IIT3_Fig22.pdf')
        p [fig:segment]
        p
          | The final example (Fig. [fig:segment]A) considers a simple perceptual system---a recurrent segment/dot system. The segment/dot system consists of 10 heavily interconnected elements that, in their current state, form a complex (Fig. [fig:segment]A, blue circle). Elements
          span.math  \(A, B\)
          | , and
          span.math  \(C\)
          |  are the ports-in of the complex: they each receive 2 inputs from an external source in addition to feed-back inputs from within the complex. Elements
          span.math  \(F\)
          |  and
          span.math  \(J\)
          |  are the ports-out of the complex: they output to the external elements
          span.math  \(O1\)
          |  and
          span.math  \(O2\)
          | , respectively, in addition to their outputs within the complex. In this example, the ports-out are XOR logic gates. All other elements inside the segment/dot system are linear threshold units (LTUs). Connections within the complex are excitatory (<span class="math">\(+1\)</span>, black) or inhibitory (<span class="math">\(-1\)</span>, red).
        p
          | The elementary mechanisms comprising the segment/dot system have specialized functions and generate elementary concepts. If an elementary mechanism is in state ``on“(1), its concept is termed “positive”. If the mechanism is in state “off”(0), its concept is termed “negative”. In the segment/dot system, the negative concepts tend to have lower
          span.math  \(\varphi^{\rm Max}\)
          |  values, because the mechanisms tend to be more selective in their ``on” state (see Fig. [fig:selective]). As listed in Fig. [fig:segment]B, in addition to first order concepts, the segment-dot system gives rise to many higher order concepts. Dependent on the state of the system, certain higher order concepts may or may not exist. For instance, in the current state of the segment/dot system, the
          span.math  \(2^{nd}\)
          |  order concept
          span.math  \(DI\)
          |  exists, while
          span.math  \(EG\)
          |  does not because it is reducible (<span class="math">\(\varphi^{\rm Max}=0\)</span>). If the segment/dot system were presented instead with a “right”-segment (inputs 022),
          span.math  \(DI\)
          |  would disappear and
          span.math  \(EG\)
          |  would emerge.
        p
          | From the perspective of an external observer (e.g. a neuroscientist recording the activity of ``neurons“
          span.math  \(A-J\)
          | ), the function of a mechanism is typically described with respect to external inputs (e.g. a ``segment” detector). In the segment/dot system, mechanisms at different hierarchical levels correspond to increasing levels of invariance: element
          span.math  \(D\)
          | , for example, turns on if the two contiguous pixels on the left have been on persistently (with inputs of strength 2); higher up in the system, element
          span.math  \(F\)
          |  turns on if two contiguous pixels have been on either on the left or on the right, thus indicating the presence of the invariant ``segment“. Element
          span.math  \(J\)
          | , on the other hand, detects the invariant ``dot”, either left, right, or center. The excitatory and inhibitory feed-back connections in the segment/dot system serve a predictive function: they temporarily increase/decrease the sensitivity to similar/opposed stimuli, allowing weaker inputs (with a value of 1) to be detected as segments and dots if the weaker external input is in accordance with the feed-back from within the complex.
        p
          | From the intrinsic perspective of the system, instead, the function of each mechanism is given by its concept. Each concept is
          em  self-generated,
          | because it must be specified exclusively by a subset of elements belonging to the complex. It is also
          em  self-referential,
          | because its cause-effect repertoire refers exclusively to elements within the complex. Therefore, each concept is related only indirectly to external inputs. For example, the positive concept of
          span.math  \(D\)
          | , in its current state 1, is about the purview
          span.math  \(D/ABEFJ^p,A^f\)
          | . From the intrinsic perspective, the function of
          span.math  \(D=1\)
          |  is thus to constrain the possible past states of
          span.math  \(A,B,E,F\)
          |  and
          span.math  \(J\)
          | , and to constrain the possible future state of
          span.math  \(A\)
          |  (Fig.[fig:segment]C). Therefore,
          span.math  \(D=1\)
          |  specifies a concept that is exclusively self-referential to the complex to which
          span.math  \(D\)
          |  belongs (note that, in this simple version of a recurrent segment/dot system, feed-forward and feed-back connections have the same absolute strength of 1. In a more realistic neural network, in which the function of the recurrent connections is mostly modulatory, a concept’s past and future purviews would be modified accordingly). Nevertheless, in this case there is a good correspondence between the intrinsic and the extrinsic perspective, since the cause repertoire of
          span.math  \(D=1\)
          |  specifies as potential causes those states in which both ports-in
          span.math  \(A\)
          |  and
          span.math  \(B\)
          |  are 1, which happens when two contiguous pixels on the left are on. Importantly, the concept of
          span.math  \(D=1\)
          |  additionally takes into account the internal context
          span.math  \(E,F,J\)
          |  (blue shaded states in Fig.[fig:segment]C). However, the correspondence between intrinsic and extrinsic perspective breaks down for the ports-in (<span class="math">\(A,B,C\)</span>): even though their state is partly determined by the external inputs, their concept specifies constraints about past and future states of elements higher up in the system, rather than about the environment (Fig.[fig:segment]D).
        p
          | The self-referential property of the concepts specified by ports-in may have some implications with respect to the role of primary areas in consciousness. An influential hypothesis by Crick and Koch
          span.citation
          | suggests that primary visual cortex (V1) and perhaps other primary cortical areas may not contribute directly to consciousness, a hypothesis that is now supported by a large number of experimental results. For example, during binocular rivalry neurons in V1 may fire selectively to horizontal bars that are shown to one eye, even though the subject does not see them and is conscious of a different stimulus presented to the other eye
          span.citation
          | . On the other hand, the firing of units higher up in the visual system correlates tightly with the experience. While these results are compelling, other interpretations are possible if, as illustrated in the segment/dot system, V1 neurons were to constitute ports-in of the main complex. Under this assumption, V1 units would have to specify concepts about other units in the complex---either other V1 units or units in higher areas---rather than about their feed-forward inputs, which would remain outside the complex. V1 concepts could relate for example to Gestalt properties such as spatial continuity, rather than to oriented bars. In that case, what V1 contributes to consciousness during binocular rivalry---namely spatial continuity---would not change substantially between the two rivalrous percepts. Instead, concepts corresponding to oriented bars would be specified by units in higher areas, whose firing is sensitive to perceptual rivalry,
          em over 
          | units in V1. In sum, V1 units would contribute to consciousness not only by generating their own concepts (such as spatial continuity), but also by providing the cause repertoire for concepts specified by units higher up (such as oriented bars). While this possibility may be far-fetched and counterintuitive, it would not be inconsistent with lesion studies that highlight the importance of V1 for most aspects of visual consciousness
          span.citation
          | .
        p
          | The self-referential nature of concepts within a complex has implications with respect to how concepts obtain their
          em  meaning 
          | (Albantakis et al., in preparation). As mentioned above, a (conscious) external observer ``knows“ that element
          span.math  \(F\)
          |  in Fig. [fig:segment]E turns on whenever there is a ``segment” in the input from the environment. However, from the intrinsic perspective of the complex, that meaning cannot be specified by
          span.math  \(F=1\)
          |  in isolation. This is because, while the cause-repertoire of
          span.math  \(F=1\)
          |  specifies that either
          span.math  \(D\)
          |  or
          span.math  \(E\)
          |  must have been on, by itself it cannot specify what
          span.math  \(D\)
          |  and
          span.math  \(E\)
          |  mean in turn. In fact, the full meaning of ``segment“ can only be synthesized through the interlocking of cause-effect repertoires of multiple concepts within a MICS (such as that of element
          span.math  \(F\)
          |  interlocked with those of elements
          span.math  \(D\)
          | ,
          span.math  \(E\)
          | , and so on). In this view, the meaning of a concept depends on the
          em  context 
          | provided by the entire MICS to which it belongs, and corresponds to how it constrains the overall ``shape” of the MICS. Meaning is thus both self-referential (internalistic) and
          em  holistic.
          |
        p
          | While emphasizing the self-referential nature of concepts and meaning, IIT naturally recognizes that in the end most concepts owe their origin to the presence of regularities in the environment, to which they ultimately must refer, albeit only indirectly. This is because the mechanisms specifying the concepts have themselves been honed under selective pressure from the environment during evolution, development, and learning
          span.citation
          | . Nevertheless, at any given time, environmental input can only act as a boundary condition, helping to ``select“ which particular concepts within the MICS will be positive or negative, and their meaning will be defined entirely within the quale. Every waking experience should then be seen as an ``awake dream” selected by the environment. And indeed, once the architecture of the brain has been built and refined, having an experience---with its full complement of intrinsic meaning---does not require the environment at all, as demonstrated every night by the dreams that occur when we are asleep and disconnected from the world.
        .anchor#limitations-and-future-directions
          h2.unnumbered Limitations and future directions
        p
          | In finishing, we point out some limitations and unfinished business. IIT 3.0 starts from key properties of consciousness---the phenomenological axioms---and translates them into postulates that lay out how a system of mechanisms must be constructed to satisfy those axioms and thus generate consciousness. To be able to formulate the postulates in explicit, computable terms, we considered small systems of interconnected mechanisms that are fully characterized by their transition probability matrix (TPM). For each system, mechanisms are discrete in time and space (see also Methods S1) and transition probabilities are available for every possible state. Directly applying this approach to physical systems of interest, such as brains, is unfeasible for several reasons: i) One would need either to discretize the variables of interest or to extend the theoretical treatment to continuous variables. ii) For biological systems, one is usually limited to observable system states, and the exhaustive perturbation of a system as the brain across all its possible states is unfeasible. Nevertheless, systematic perturbations of brain states using naturalistic stimuli such as movies can provide useful approximations. Also, circumscribed regions of the cerebral cortex could be perturbed systematically using optogenetic methods coupled with calcium imaging. iii) Variables recorded in most neurophysiological experiments may not correspond to the spatial and temporal grain at which integrated information reaches a maximum, which is the appropriate level of analysis
          span.citation
          | . iv) The present analysis is unfeasible for systems of more than a dozen elements or so. This is because, to calculate
          span.math  \(\Phi^{\rm max}\)
          |  exhaustively, all possible partitions of every mechanism and of every system of mechanisms should be evaluated, which leads to a combinatorial explosion, not to mention that the analysis should be performed at every spatio-temporal grain. For these reasons, the primary aim of IIT 3.0 is simply to begin characterizing, in a self-consistent and explicit manner, the fundamental properties of consciousness and of the physical systems that can support it. Hopefully, heuristic measures and experimental approaches inspired by this theoretical framework will make it possible to test some of the predictions of the theory
          span.citation
          | . Deriving bounded approximations to the explicit formalism of IIT 3.0 is also crucial for establishing in more complex networks how some of the properties described here scale with system size and as a function of system architecture.
        p
          | The above formulation of IIT 3.0 is also incomplete: i) We did not discuss the relationship between MICS and specific aspects of phenomenology, such as the clustering into modalities and submodalities, and the characteristic ``feel“ of different aspects of experience (space, shape, color and so on; but see
          span.citation
          | ). ii) In the examples above, we assumed that the ``micro” spatio-temporal grain size of elementary logic gates updating every time step was optimal. In general, however, for any given system the optimal grain size needs to be established by examining at which spatio-temporal level integrated information reaches a maximum
          span.citation
          | . In terms of integrated information, then, the macro may emerge over the micro, just like the whole may emerge above the parts. iii) While emphasizing that meaning is always internal to a complex (it is self-generated and self-referential), we did not discuss in any detail how meaning originates through the nesting of concepts within MICS (its holistic nature). This issue will begin to be addressed in future work (Albantakis et al., in preparation). iv) In IIT, the relationship between the MICS generated by a complex of mechanisms, such as a brain, and the environment to which it is adapted, is not one of ``information processing“, but rather one of ``matching” between internal and external causal structures
          span.citation
          | . Matching can be quantified as the distance between the set of MICS generated when a system interacts with its typical environment and those generated when it is exposed to a structureless (``scrambled“) version of it
          span.citation
          | . The notion of matching, and the prediction that adaptation to an environment should lead to an increase in matching and thereby to an increase in consciousness, will be investigated in future work, both by evolving simulated agents in virtual environments (``animats”
          span.citation
          | ), and through neurophysiological experiments (Boly et al., submitted). v) IIT 3.0 explicitly treats integrated information and causation as one and the same thing, but the many implications of this approach need to be explored in depth in future work. For example, IIT implies that each individual consciousness is a local maximum of causal power. Hence, if having causal power is a requirement for existence, then consciousness is maximally real. Moreover, it is real in and of itself---from its own intrinsic perspective---without the need for an external observer to come into being.
      section#supplementary-material
        h1.page-header.unnumbered Supplementary Material
        .anchor#main-differences-between-iit-3
          h2.0-and-earlier-versions.unnumbered Main differences between IIT 3.0 and earlier versions
        ol
          li
            p
              | The axioms and postulates of the theory are presented explicitly. This clarifies many issues and highlights the link between the starting point of IIT, which is phenomenology itself (axioms that are assumed to be self-evident from the intrinsic perspective of a conscious entity) and the postulates that must be satisfied by physical systems in order to support consciousness. Moreover, the postulates are applied explicitly both at the level of individual mechanisms and at that of systems of mechanisms. As illustrated in the main text, axioms and postulates include existence, composition, information, integration, and exclusion.
          li
            p
              | Mechanisms specify both causes and effects. Unlike IIT 2.0
              span.citation
              | , IIT 3.0 considers how mechanisms in a state constrain both the past and the future of a system. In a way this is a return to the state-independent framework of IIT 1.0
              span.citation
              | , which considered both causes and effects, but only for a stationary system at equilibrium. It is also a return to IIT 1.0 in capturing the idea that information is “a difference that makes a difference”, and not simply “a difference”. Indeed, IIT 3.0 postulates that both cause and effect are necessary to generate information intrinsically. This emphasis on both the causes and the effects of mechanisms in a state becomes a starting point for exploring the relationship between information and causation (see Albantakis et. al in prep), which in IIT 3.0 are one and the same thing.
          li
            p
              | The elements of a system are mechanisms in a state. In IIT 3.0, the basic elements that define concepts and constellations of concepts within concept space are mechanisms in a state, rather than the connections among them, as was the case in IIT 2.0. This is because mechanisms in a state (e.g. on/off) can specify ``
              em given 
              | this cause -
              em  then 
              | that effect" conditions, i.e. specify concepts.
          li
            p
              | Complexes are identified by assessing the effects of partitions on their entire conceptual structure. For computational expediency, in IIT 1.0 and 2.0, complexes were identified by assessing the irreducibility of a set of elements through partitions of its highest-order concept only---the concept specified by all its elements together. Only then would one establish the full conceptual structure specified by the set. In IIT 3.0 the irreducibility of a set of elements is assessed by considering how a partition affects its entire conceptual structure---all the concepts specified by its elements in all combinations (power set). In this way all the concepts that are changed or lost due to the partition contribute to
              span.math  \(\Phi\)
              | . For example, even a partition between a single element
              span.math  \(A\)
              |  and the rest of the set can destroy or modify not only the elementary concept specified by
              span.math  \(A\)
              |  by itself, but also the higher-order concepts
              span.math  \(A\)
              |  specifies together with elements on the other side of the partition, as well as all the concepts specified by other elements that include
              span.math  \(A\)
              |  in their purview.
          li
            p
              | The minimum information partition (MIP) is evaluated without normalization. In IIT 1.0 and 2.0, normalization was used to avoid certain inappropriate consequences of identifying complexes based exclusively on partitions at the level of the highest-order concept. In IIT 3.0 this is no longer necessary.
          li
            p
              | Mechanisms specify concepts only if they are irreducible. IIT 3.0 recognizes that concepts can only exist intrinsically if they are irreducible. This important requirement had been overlooked in IIT 2.0.
          li
            p
              | Concept space has a proper metric. In IIT 2.0, the effect of partitions was measured by the Kullback-Leibler divergence (KLD) between distributions, which only takes into account differences in selectivity. IIT 3.0 recognizes the need for a true metric (the earth mover’s distance, EMD) that also takes into account the similarity or dissimilarity of states between whole and partitioned distributions. Moreover, an extended version of EMD is applied to measure the distance between whole and partitioned constellations of concepts in concept space. This development makes all the more explicit the distinction between the notion of information in IIT as “differences that make a difference” from the intrinsic perspective of a system, and the classic notion of information from the perspective of an external observer (see
              span Some differences between integrated information and Shannon information
              | ).
          li
            p
              | The exclusion postulate is applied not only to systems of mechanisms but also to causes and effects specified by individual mechanisms in a system.
          li
            p
              | Elements outside the candidate set under consideration are treated as boundary conditions. Their states are fixed at their ``actual" values, rather than noised (see Boundary conditions).
          li
            p
              | A user-friendly program for calculating exhaustively all the quantities required by IIT in discrete systems is made available alongside the paper
              span.citation
              | .
        .anchor#conditionally-independent-mechanisms
          h2.unnumbered Conditionally independent mechanisms
        p
          | In this paper, it is assumed that mechanisms are conditionally independent. Consider a system consisting of three elements
          span.math  \(ABC\)
          | . Mathematically, conditional independence of mechanisms
          span.math  \(A\)
          | ,
          span.math  \(B\)
          | , and
          span.math  \(C\)
          |  is represented as
        p
          span.math
            | \[p(ABC^{t}|ABC^{t-1}) = p(A^t|ABC^{t-1}) \times p(B^t|ABC^{t-1}) \times p(C^t|ABC^{t-1}).\]
        p
          | In words, given a system state at time
          span.math  \(t_{-1}\)
          | , the probability of
          span.math  \(A\)
          | ,
          span.math  \(B\)
          | , and
          span.math  \(C\)
          |  at time
          span.math  \(t\)
          |  can be determined independently. This corresponds to the assumption that there is no instantaneous interaction between mechanisms and causes must precede their effects.
        .anchor#boundary-conditions
          h2.unnumbered Boundary conditions
        p
          | Choosing a ``candidate set" for IIT analysis means defining a precise border between elements that are inside the candidate set and elements that are outside. From the intrinsic perspective of the candidate set, the outside elements are treated as
          em  boundary conditions.
          | This means that they are not considered as variables internal to the set over which to perform perturbations, but rather as fixed, external constraints. For the purpose of IIT analysis, this means that the connections from the outside elements are not noised and their state is kept fixed. Specifically, when evaluating a cause repertoire in the candidate set, the outside elements are fixed at their past state at
          span.math  \(t_{-1}\)
          | . Similarly, when evaluating an effect repertoire, the outside elements are fixed at their present states at
          span.math  \(t_0\)
          | .
        p
          | As an example, in the candidate set of Fig. 1 (main text), the state of element
          span.math  \(D\)
          |  is taken to be 0 at
          span.math  \(t_{-1}\)
          |  and
          span.math  \(t_0\)
          | . Given these boundary condition, the candidate set
          span.math  \(ABC\)
          |  performs as if
          span.math  \(D\)
          |  did not exist. On the other hand, the transition probability matrix (TPM) of
          span.math  \(ABC\)
          |  would be different for
          span.math  \(D(t_{-1})=1\)
          | . Thus, the conceptual structure (<span class="math">\(C\)</span>) of the candidate set may differ, depending on the boundary conditions, even though the state of the elements within the candidate set is the same (Note that, for all example systems in the Results and Discussion section, the system state at
          span.math  \(t_{-1}\)
          |  is the same as the current state at
          span.math  \(t_0\)
          | . Also, when considering other candidate sets within
          span.math  \(ABC\)
          | , it is assumed that
          span.math  \(ABC(t_{-1})=110\)
          | .)
        p
          | A neurological example of boundary conditions would be the sensory input to the ports-in of a cortical main complex. As was illustrated in the case of the segment/dot system (Fig. 22, main text), the main complex does not include the ``sensory“ elements providing feed-forward input to it. However, their input---some on and some off---constitutes a boundary condition over which cause-effect repertoires internal to the main complex are evaluated. As a more extreme example, the activity of subcortical activating systems with diffuse projections that maintain the excitability of the cortex is likely to constitute an essential boundary or ``enabling” condition
          span.citation
          | for the existence of a cortical main complex. Without the activating input they provide, cortical neurons become bistable and consciousness disintegrates
          span.citation
          | . Nevertheless, the neural elements that provide this essential activating input to cortex are themselves likely to be excluded from the main complex
          span.citation
          | .
        .anchor#cause-effect-repertoire-unconstrained-repertoire-puc-and-partitions
          h2.unnumbered Cause-effect repertoire, unconstrained repertoire
          span.math  \(p^{uc}\)
          | , and partitions
        p
          | To calculate the cause-effect information of a mechanism in a state over a purview, its cause-effect repertoire is compared against the unconstrained repertoire
          span.math  \(p^{uc}\)
          | . The integrated information
          span.math  \(\phi\)
          |  of a mechanism in a state over a purview is assessed by comparing its cause-effect repertoire against that of the partitioned purview. How these probability distributions are derived is illustrated using the example of mechanism
          span.math  \(A=1\)
          |  over the purview
          span.math  \(ABC\)
          |  from Fig. 4 (main text), as well as other mechanisms from the candidate set
          span.math  \(ABC\)
          |  (Fig. 1, main text).
        .anchor#cause-repertoire
          h3.unnumbered Cause repertoire
        p
          | The cause repertoire
          span.math  \(p(ABC^p|A^c=1)\)
          |  is obtained via Bayes’ rule by perturbing the set of elements into all its states with equal likelihood, i.e. assuming a uniform prior distribution
          span.math  \(p^{\rm{per}}(ABC^p)\)
          |  of past states
          span.math  \(ABC^p\)
          | , where the superscript
          span.math  \(^{\rm per}\)
          |  stands for “perturbed”:
        p
          span.math
            | \[p(ABC^p|A^c=1) = \frac{p(A^c=1|ABC^p) p^{\rm{per}}(ABC^p)} {p(A^c=1)}. \label{eq:causerep}\]
        p
          | Here,
          span.math  \(p^{\rm{per}}(ABC^p) = 1/8\)
          |  for each past state, since the set is perturbed into each state with equal probability,
          span.math  \(p(A^c=1)=3/4\)
          | , and
          span.math  \(p(A^c=1|ABC^p)\)
          |  is either 0 (for states 000 and 100) or 1 (for all other states).
        p
          | In general, the cause repertoire can also be assessed over a subset of the candidate set, e.g.
          span.math  \(p(C^p|A^c=1)\)
          | . In this case, one has to
          em  marginalize 
          | over the elements outside of the purview, which remain unconstrained:
        p
          span.math
            | \[p(C^p|A^c=1) =  \frac{ \left( \sum_{AB^p} p(A^c=1|C^p, AB^p) \: p^{\rm{per}}(AB^p) \right)  \: p^{\rm{per}}(C^p)} {p(A^c=1)}, \label{eq:causerep2}\]
        p
          | where
          span.math  \(p^{\rm{per}}(AB^c)\)
          |  denotes the perturbed (uniform) distribution over the past states of the elements
          span.math  \(AB\)
          | . The sum in equation [eq:causerep2] is effectively the average over the probabilities
          span.math  \(p(A^c=1|ABC^p)\)
          |  calculated for
          span.math  \(AB^p = [00, 10, 01, 11]\)
          | .
        p
          | If the cause-repertoire of a higher order mechanism such as
          span.math  \(AB=10\)
          |  is determined over a limited purview, e.g
          span.math  \(AB^p\)
          | , marginalizing over the remaining elements (<span class="math">\(C^p\)</span>) can lead to correlations in
          span.math  \(AB^c\)
          |  if
          span.math  \(C\)
          |  provides common input to both
          span.math  \(A\)
          |  and
          span.math  \(B\)
          | . Since the aim is to assess the cause-information of
          span.math  \(AB^c\)
          |  about
          span.math  \(AB^p\)
          |  independent of
          span.math  \(C^p\)
          | ,
          span.math  \(C^p\)
          |  has to be replaced by ``virtual elements" with independent output to every element, as indicated by the subscript
          span.math  \(_V\)
          | :
        p
          span.math
            | \[p(AB^p|AB^c=10) =  \frac{ \left(  \sum_{C^p_V} p(AB^c=10|AB^p, C^p_V) \: p^{\rm{per}}(C^p_V) \right)  \: p^{\rm{per}}(AB^p)} {p(AB^c=10)}.\]
        p
          | The virtual element
          span.math  \(C^p_V\)
          |  means that the states 0 and 1 are imposed independently over every output connection from
          span.math  \(C\)
          | (Fig. [fig:virtualnodes]A). The mechanisms considered here are
          span.math  \(1^{st}\)
          |  order Markov functions and are thus conditionally independent given their respective inputs. Therefore, the cause repertoire of a higher order mechanism such as
          span.math  \(AB=01\)
          |  can be calculated as the product of the cause-repertoires of its elementary mechanisms,
          span.math  \(A=1\)
          |  and
          span.math  \(B=0\)
          | , obviating the need to add virtual elements in the actual calculations:
        p
          span.math  \[p(AB^p|AB^c=10) = p(AB^p|A^c=1) \times p(AB^p|B^c=0).\]
        .anchor#unconstrained-cause-repertoire
          h3.unnumbered Unconstrained cause repertoire
        p
          | As described in the main text (Fig. 4), the amount of cause information that
          span.math  \(A=1\)
          |  specifies about the past, its cause information (<span class="math">\(ci\)</span>), is measured as the distance
          span.math  \(D\)
          |  between the cause repertoire (eq. [eq:causerep]) and the unconstrained repertoire
          span.math  \(p^{\rm{uc}}\)
          | . For the purview
          span.math  \(ABC^p\)
          | :
        p
          span.math
            | \[ci(ABC^p|A^c=1) = D(p(ABC^p|A^c=1)|| p^{ {\rm uc}}(ABC^p)) = 0.33. \label{eq:ci2}\]
        p
          span.math  \(p^{\rm{uc}}(ABC^p)\)
          |  corresponds to the cause repertoire in the absence of any mechanism. The unconstrained past distributions is thus the uniform distribution, since without the constraints of a mechanism in a state
          span.math  \(p^{\rm{uc}}(ABC^p)\)
          | =
          span.math  \(p^{\rm{per}}(ABC^p)\)
          | .
        .anchor#effect-repertoire
          h3.unnumbered Effect repertoire
        p
          | The effect repertoire
          span.math  \(p(ABC^f|A^c=1)\)
          |  (Fig. 4, main text) is computed by fixing the current state of
          span.math  \(A\)
          |  to 1, while the remaining elements
          span.math  \(B\)
          |  and
          span.math  \(C\)
          |  are independently perturbed into all their possible states with equal likelihood:
        p
          span.math
            | \[p(ABC^f|A^c=1) = \sum_{BC^c_V} p(ABC^f|A^c=1, BC^c_V) p^{\rm{per}}(BC^c_V). \label{eq:effectrep}\]
        p
          | Again, common inputs from
          span.math  \(B\)
          |  or
          span.math  \(C\)
          |  can lead to correlations between
          span.math  \(A^f\)
          | ,
          span.math  \(B^f\)
          | , and
          span.math  \(C^f\)
          | . To avoid counting these correlations as effects of element
          span.math  \(A\)
          | , it is important to replace elements
          span.math  \(B\)
          |  and
          span.math  \(C\)
          |  by virtual elements with independent outputs to every element (Fig. [fig:virtualnodes]B). Since all mechanisms under consideration are conditionally independent, in practice the effect repertoire can be calculated as:
        p
          span.math  \[p(ABC^f|A^c=1) = p(A^f|A^c=1) \times p(B^f|A^c=1) \times p(C^f|A^c=1),\]
        p
          | where the effect repertoire of a single future element, e.g.
          span.math  \(p(A^f|A^c=1)\)
          |  is simply:
        p
          span.math
            | \[p(A^f|A^c=1) = \sum_{BC^c} p(ABC^f|A^c=1, BC^c) p^{\rm{per}}(BC^c). \label{eq:effectrep2}\]
        .anchor#unconstrained-effect-repertoire
          h3.unnumbered Unconstrained effect repertoire
        p
          | Like the cause information (<span class="math">\(ci\)</span>), the effect information (<span class="math">\(ei\)</span>) of
          span.math  \(A=1\)
          |  is quantified as the distance
          em  D 
          | between the effect repertoire of
          span.math  \(A\)
          |  and the unconstrained future distribution
          span.math  \(p^{\rm{uc}}(ABC^f)\)
          | :
        p
          span.math  \[ei(ABC^f|A^c=1)  =  D(p(ABC^f|A^c=1)|| p^{{\rm uc}}(ABC^f)) = 0.25.\]
        p
          | Without any constrains from a mechanism in a state, the unconstrained effect repertoire
          span.math  \(p^{{\rm uc}}(ABC^f)\)
          |  is given by:
        p
          span.math
            | \[p^{{\rm uc}}(ABC^f) = \sum_{ABC^c_V} p(ABC^f|ABC^c_V) p^{{\rm per}}(ABC^c_V), \label{eq:future_ref}\]
        p
          | where virtual elements are again used to avoid including effects arising from correlations due to common inputs. For conditionally independent mechanisms, this is identical to:
        p
          span.math
            | \[\begin{aligned}
            | p^{{\rm uc}}(ABC^f) =& \sum_{ABC^c} p(A^f|ABC^c) p^{{\rm per}}(ABC^c) \times \sum_{ABC^c} p(B^f|ABC^c) p^{{\rm per}}(ABC^c) \times \\ \times & \sum_{ABC^c} p(C^f|ABC^c) p^{{\rm per}}(ABC^c), \nonumber \label{eq:future_ref2}\end{aligned}\]
        p
          | the product of the effect probability distributions of each element given unconstrained inputs.
          span.math  \(p^{{\rm uc}}(ABC^f)\)
          |  is thus not simply the uniform distribution of future states of the candidate set. In the example set
          span.math  \(ABC\)
          | , the unconstrained effect repertoire for the OR-gate
          span.math  \(A\)
          |  is
          span.math  \(p(A=0)=0.25\)
          |  and
          span.math  \(p(A=1)=0.75\)
          | , for the AND-gate
          span.math  \(B\)
          | :
          span.math  \(p(B=0)=0.75\)
          |  and
          span.math  \(p(B=1)=0.25\)
          | , and for the XOR-gate
          span.math  \(C\)
          | :
          span.math  \(p(C=0)=0.5\)
          |  and
          span.math  \(p(C=1)=0.5\)
          | , obtained by perturbing their inputs to the states [00, 01, 10, 11] with equal probability.
        p
          embed(src='Figures/IIT3_FigS1.pdf')
        p [fig:virtualnodes]
        .anchor#partitions
          h3.unnumbered Partitions
        p
          | As illustrated above, when the cause/effect repertoire of a mechanism is computed over a particular purview, the elements outside of the purview, but within the candidate set, remain unconstrained. Similarly, if a purview is partitioned, elements outside of the part under consideration become unconstrained and thus effectively act as independent noise sources (they are ``injected with noise"). This renders the connections across the partition causally inactive.
        p
          | In the example shown in Fig. [fig:partition], the purview
          span.math  \(ABC^c/ABC^f\)
          |  is partitioned into
          span.math  \(AB^c/AB^f \times C^c/C^f\)
          | . To obtain the partitioned effect repertoire, the effect repertoires of the purviews
          span.math  \(AB^c/AB^f\)
          |  and
          span.math  \(C^c/C^f\)
          |  are calculated independently and then multiplied. Considering the purview of
          span.math  \(AB^c/AB^f\)
          | , the element
          span.math  \(C\)
          |  is outside of the purview and thus unconstrained:
        p
          span.math  \[p(AB^f|AB^c=10) = \sum_{C^c_V} p(AB^f|AB^c=10, C^c_V) p^{\rm{per}}(C^c_V).\]
        p
          | To causally disconnect
          span.math  \(C\)
          |  from
          span.math  \(AB\)
          | , it is again important to introduce virtual elements, which ensure independent noise in the connections across the partition. Even if
          span.math  \(A\)
          |  and
          span.math  \(B\)
          |  receive a common input from
          span.math  \(C\)
          | , such a common input is ignored in the purview of
          span.math  \(AB^c/AB^f\)
          | . Since
          span.math  \(A\)
          | ,
          span.math  \(B\)
          | , and
          span.math  \(C\)
          |  are conditionally independent
          span.math  \(p(AB^f|AB^c=10)\)
          |  can be calculated without explicitly introducing virtual units:
        p
          span.math
            | \[p(AB^f|AB^c=10) = \sum_{C^c} p(A^f|AB^c=10, C^c) p^{\rm{per}}(C^c) \times \sum_{C^c} p(B^f|AB^c=10, C^c) p^{\rm{per}}(C^c).\]
        p
          | Similarly, the purview of
          span.math  \(C^c/C^f\)
          |  is the computed as:
        p
          span.math  \[p(C^f|C^c=0) = \sum_{AB^c} p(C^f|C^c=0, AB^c) p^{\rm{per}}(AB^c).\]
        p
          | The partitioned effect repertoire,
          span.math  \(AB^c/AB^f \times C^c/C^f\)
          | , is the product of
          span.math  \(p(AB^f|AB^c)\)
          |  and
          span.math  \(p(C^f|C^c)\)
          | . Partitioned cause repertoires are calculated in the same way.
        p
          embed(src='Figures/IIT3_FigS2.pdf')
        p [fig:partition]
        .anchor#earth-movers-distance
          h2.unnumbered Earth mover’s distance
        p
          embed(src='Figures/IIT3_FigS3.pdf')
        p [fig:KLDEMD]
        .anchor#distance-for-probability-distributions
          h3.unnumbered Distance for probability distributions
        p
          | Integrated information
          span.math  \(\varphi\)
          |  measures the difference between two probability distributions, a partitioned distribution and an unpartitioned distribution. In previous work, the Kullback-Leibler divergence (KLD) was used to compare distributions. KLD has several useful properties, but it is not a true metric (it is not symmetric) and it is unbounded. Moreover, KLD only measures how ``sharp" a distribution is compared to the other, without taking into account whether some states of the system are closer than others (e.g. that [0 0] is closer to [1 0] than to [1 1]).
        p
          | A more appropriate measure that corresponds better to the IIT notion of information as ``differences that make a difference“ is the earth mover’s distance (EMD). As indicated by its name, an intuitive interpretation of the EMD is that it measures the minimum cost of transportation that arises when one probability distribution has to be transformed into another
          span.citation
          | . In this view, a probability value is associated with a certain amount of ``earth” that is moved across a certain distance, the distance from one state to another. The cost of transportation is then the amount of ``earth" moved times the distance by which it is moved. The distance between binary states is measured by the Hamming distance, which counts the number of places by which two strings differ. For instance, the Hamming distance between the states
          span.math  \(ABC=000\)
          |  and
          span.math  \(ABC=111\)
          |  is 3; the distance between
          span.math  \(ABC=010\)
          |  and
          span.math  \(ABC=100\)
          |  is 2. The EMD is in principle extendable to account for non-binary states, as long as a distance between the individual states is given, which is an intrinsic property of the mechanisms under consideration.
        p
          | EMD is symmetric, is bounded (by the number of elements
          span.math  \(N\)
          |  for binary mechanisms) and takes into account the distance between states. Fig. [fig:KLDEMD] shows a cause repertoire over two elements with two of its possible partitioned repertoires. In the intact cause repertoire, the only possible cause is
          span.math  \(00\)
          | . Partitions 1 and 2 both have state
          span.math  \(00\)
          |  as a possible cause, but add state
          span.math  \(10\)
          | (Partition 1) and state
          span.math  \(11\)
          | (Partition 2), with equal probability. Since state
          span.math  \(00\)
          |  is closer to
          span.math  \(10\)
          |  than to
          span.math  \(11\)
          | , the intact distribution is closer to Partition 1 than to the Partition 2, as captured with EMD. From the intrinsic perspective of the system, then, Partition 2 makes more of a difference, since the state of
          em  two 
          | system mechanisms becomes undetermined instead of just one. By contrast, KLD assigns the same distance to both partitions, since it only measures the “reduction of uncertainty.”
        h3#distance-for-constellations-of-concepts.unnumbered Distance for constellations of concepts
        p
          embed(src='Figures/IIT3_FigS4.pdf')
        p [fig:genEMD]
        p
          | Integrated conceptual information
          span.math  \(\Phi^{\rm Max}\)
          |  measures the difference between the intact constellation of a set of elements
          span.math  \(C\)
          |  and that of its minimum information partition
          span.math  \(C^{\rm MIP}_\rightarrow\)
          | . Like the difference between probability distributions
          span.math  \(\varphi^{\rm}\)
          | , the difference between constellations is assessed by an extended version of the earth mover’s distance (EMD). The extended EMD measures the minimal cost of transforming one constellation into another. Instead of probabilities, in the extended EMD it is the
          span.math  \(\varphi^{\rm}\)
          |  value of the concepts that corresponds to the “earth” that is redistributed from constellation
          span.math  \(C\)
          |  to
          span.math  \(C^{\rm MIP}_\rightarrow\)
          | . Instead of the Hamming distance, the distance between the concepts of
          span.math  \(C\)
          |  and
          span.math  \(C^{\rm MIP}_\rightarrow\)
          |  is given by their distance in concept space. Since
          span.math  \(\sum (\varphi^{\rm Max})\)
          |  of all concepts of
          span.math  \(C\)
          |  is usually higher than that of
          span.math  \(C^{\rm MIP}_\rightarrow\)
          | , any residual
          span.math  \(\varphi^{\rm Max}\)
          |  is assigned to the ``null" concept (the unconstrained distribution
          span.math  \(p^{uc}\)
          | ), which is included as an additional location in
          span.math  \(C^{\rm MIP}_\rightarrow\)
          | .
        p
          | Fig. [fig:genEMD] shows how the
          span.math  \(\Phi^{\rm Max}\)
          |  value of candidate set
          span.math  \(ABC\)
          |  (Fig. 12, main text) is calculated. To illustrate the analogy between the standard EMD for probability distributions and the extended EMD for constellations, the
          span.math  \(\varphi^{\rm Max}\)
          |  values of the concepts of
          span.math  \(C\)
          |  and
          span.math  \(C^{\rm MIP}_\rightarrow\)
          |  are displayed as two distributions. In this example, the concept of
          span.math  \(A\)
          |  and
          span.math  \(B\)
          |  are unaffected by the partition, while the other 4 concepts are destroyed. The optimal way to transform
          span.math  \(C\)
          |  into
          span.math  \(C^{\rm MIP}_\rightarrow\)
          |  is thus to move the
          span.math  \(\varphi^{\rm Max}\)
          |  values of the concepts
          span.math  \(C\)
          | ,
          span.math  \(AB\)
          | ,
          span.math  \(BC\)
          | , and
          span.math  \(ABC\)
          |  to the ``null“ concept
          span.math  \(p^{uc}\)
          | . The distance between two concepts in concept space is measured by the EMD distance of their cause-effect repertoires. The distance from concept
          span.math  \(C\)
          |  to the ``null” concept, for example (Fig. [fig:genEMD]B), is the sum of the standard EMD distance of the two cause repertoires and the two effect repertoires of
          span.math  \(C\)
          |  and the
          span.math  \(p^{uc}\)
          | . To obtain
          span.math  \(\Phi^{\rm Max}\)
          |  for this example, the distances of concepts
          span.math  \(C\)
          | ,
          span.math  \(AB\)
          | ,
          span.math  \(BC\)
          | , and
          span.math  \(ABC\)
          |  to the ``null" concept are multiplied by the
          span.math  \(\varphi^{\rm Max}\)
          |  value of each concept and then summed up. In the general case, the optimal way to redistribute
          span.math  \(\varphi^{\rm Max}\)
          |  form
          span.math  \(C\)
          |  to
          span.math  \(C^{\rm MIP}_\rightarrow\)
          |  must be found using an optimization algorithm
          span.citation
          | .
        h4#section
          span.header-section-number 0.0.0.1
        p
          | Custom-made MATLAB software was used for all calculations. The program to calculate the complex of a small system of logic gates and its constellation of concepts is available under:
          span.citation
          | . EMD calculations were performed using the open source fast MATLAB code of Pele and Werman
          span.citation
          | .
        .anchor#motivation-for-exclusion-at-the-level-of-mechanisms
          h2.unnumbered Motivation for exclusion at the level of mechanisms
        p
          | Fig. S5 is described in the main text, section ``Exclusion: A maximally irreducible cause-effect repertoire (MICE) specified by a subset of elements (a concept)".
        p
          embed(src='Figures/IIT3_FigS5.pdf')
        p [fig:Neuron]
        .anchor#some-differences-between-integrated-information-and-shannon-information
          h2.unnumbered Some differences between integrated information and Shannon information
        p
          | While the notion of information plays a central role in IIT, it is distinctively different from that developed in Shannon’s communication theory
          span.citation
          | .
        p
          | Information as defined in information theory quantifies how accurately input signals can be decoded by knowing output signals transmitted across a (noisy) channel. (Mutual) information is the average statistical dependence between two sets of variables, such as the inputs and the outputs of a channel. In general, this depends on the entropy of the inputs and on how well the inputs predict the outputs. The (self)-information contained in a single system state is simply the logarithm of its probability of occurrence. As clearly recognized by Shannon, the information of information theory is divorced from meaning: the sender and the receiver are assumed to know what the messages mean, and information theory is only concerned with how reliably and efficiently they can be transmitted across a channel. While Shannon information can be evaluated between past states and current states of the same system, it is always assessed from the extrinsic perspective of an observer who assesses the statistical dependence between inputs and outputs. Therefore, Shannon information could be called
          em  extrinsic 
          | information.
        p
          | In IIT, by contrast, information is
          em  intrinsic 
          | : it is assessed from the intrinsic perspective of a system in terms of the differences that make a difference to it. Intrinsic information is
          em  causal,
          | and it must be evaluated by perturbing a set of elements in all possible ways, not just by observing them. It is also
          em  compositional,
          | in that different combinations of elements within a system can simultaneously specify different constraints. Crucially, in IIT information must also be
          em  integrated 
          | : if partitioning a system makes no difference to it, there is no system to begin with. Moreover, IIT only considers information that is maximally integrated or
          em  exclusive 
          | because, from the intrinsic perspective, there can only be one set of causes. None of these requirements matter from the extrinsic perspective of an observer who measures the overall statistical dependence between inputs and outputs. Note also that within the framework of Shannon information, the difference between two probability distributions is assessed based on the Kullback-Leibler divergence (KLD), which measures the loss of Shannon information from the perspective of an external observer if one probability distribution is approximated by another. In the case that a distribution is evaluated against maximum entropy (all states equally likely), KLD measures reduction of uncertainty. Since information in IIT aims to capture differences that make a difference from the intrinsic perspective of a system, it uses a true metric (EMD) that is sensitive to the relative distance among system states (Text S5).
        p
          | Most importantly, classical information theory does not deal with meaning, but only with how well messages are communicated and stored. In IIT, instead, information
          em  is 
          | meaning: more precisely, intrinsic integrated information is a maximally irreducible information structure (MICS). This integrated information structure captures all the constrains over the past and future of a complex as determined by the state of its mechanisms. It is thus a “shape” in concept space, not a message transmitted across a channel. Accordingly, the meaning is the MICS itself, and the meaning of each individual concept within the MICS is self-generated, self-referential, and holistic: it is constructed by the elements of the complex, over the elements of the complex, and in the context provided by other concepts within the same MICS.
        p
          | As an example, consider the firing of a “face” neuron deep inside the brain that is part of the main complex. From the extrinsic perspective of an observer, such a face neuron can certainly convey extrinsic, Shannon information about some events in the environment, given that its firing is tightly correlated with the presence of faces. From the intrinsic perspective, however, the firing of that neuron is meaningful because of the way it modifies the shape of the MICS generated by the main complex (which it would modify in a different way if it were silent). As a corollary, dreams have as much meaning as awake experiences: the firing of a “face” neuron in a dream specifies the same intrinsic information even though it conveys no extrinsic information about the environment.
        p
          | Ultimately, of course, the circuits that generate meaning originate, develop, and refine through a long process of evolution, neural development, and learning, under the selective pressure of a complex environment. According to IIT, the goal of these circuits is not so much to “process” extrinsic information from the environment. Instead, it is to generate integrated information structures that ``match“ regularities in the environment in such a way that the ``right dream” occurs at the right time
          span.citation
          | . This matching, in turn, can act as a driving force for the evolution of consciousness
          span.citation
          | .
      section#acknowledgments
        h1.page-header.unnumbered Acknowledgments
        p
          | We thank Chiara Cirelli, Lice Ghilardi, Melanie Boly, Christof Koch, and Marcello Massimini for many invaluable discussions concerning the concepts presented here. We also thank Brad Postle, Barry van Veen, Virgil Griffiths, Atif Hashmi, Erik Hoel, Matteo Mainetti, Andy Nere, Umberto Olcese, and Puneet Rana. We are especially grateful to V. Griffith for his contribution to characterizing the concept of synergy and its relation to integrated information; to M. Mainetti for his help in characterizing the proper metric for conceptual spaces. For developing the software used to compute maximally irreducible integrated conceptual structures we are indebted to B. Shababo, A. Nere, A. Hashmi, U. Olcese, and P. Rana. This work was supported by a Paul Allen Family Foundation grant and by the McDonnell Foundation.
        //TODO: make links
        //TODO: link to reference pdfs?
        h2#references References
        ol.references
          li.reference
            | Le Q V. et al. (2011) Building high-level features using large scale unsupervised learning. In: ICML2012.
          li.reference
            | The DeepQA Research Team (2013) Available: http://researcher.ibm.com/researcher/view\_project.php?id=2099. Accessed October 21, 2013.
          li.reference
            | Thompson C (2010) Smarter Than You Think---I.B.M.’s Supercomputer to Challenge “Jeopardy!” Champions. N Y Times Mag.
          li.reference
            | Tononi G (2004) An information integration theory of consciousness. BMC Neurosci 5: 42.
          li.reference
            | Tononi G (2008) Consciousness as integrated information: a provisional manifesto. Biol Bull 215: 216-242.
          li.reference
            | Tononi G (2012) Integrated information theory of consciousness: an updated account. Arch Ital Biol 150: 56-90.
          li.reference
            | Baars BJ (1988) A Cognitive Theory of Consciousness (Cambridge University Press).
          li.reference
            | Crick F, Koch C (2003) A framework for consciousness. Nat Neurosci 6: 119-126.
          li.reference
            | Koch C (2004) The Quest for Consciousness: A Neurobiological Approach (Roberts and Co.).
          li.reference
            | Dehaene S, Changeux JP (2011) Experimental and theoretical approaches to conscious processing. Neuron 70: 200–27. Chalmers DJ (1996) The Conscious Mind: In Search of a Fundamental Theory (Oxford University Press).
          li.reference
            | Tononi G, Koch C (2008) The neural correlates of consciousness: an update. Ann N Y Acad Sci 1124: 239-61.
          li.reference
            | Tononi G, Laureys S (2009) The neurology of consciousness: an overview. The neurology of consciousness, 375-412.
          li.reference
            | Casali AG, Gosseries O, Rosanova M, Boly M, Sarasso S, et al. (2013) A theoretically based index of consciousness independent of sensory processing and behavior. Science translational medicine 5(198): 198ra105-198ra105.
          li.reference
            | Oizumi et al. (in prep)
          li.reference
            | Tononi G (2001) Information measures for conscious experience. Arch Ital Biol 139:367-71.
          li.reference
            | Balduzzi D, Tononi G (2008) Integrated information in discrete dynamical systems: Motivation and theoretical framework. PLoS Comput Biol 4: e1000091.
          li.reference
            | Balduzzi D, Tononi G (2009) Qualia: the geometry of integrated information. PLoS Comput Biol 5: e1000462.
          li.reference
            | Hoel E, Albantakis L, Tononi G (2013) Quantifying causal emergence shows that ``macro“ can beat ``micro”. Proc Natl Acad Sci: In press.
          li.reference
            | Ay N, Polani D (2008) Information Flows in Causal Networks. Adv Complex Syst 11:17–41.
          li.reference
            | Korb KB, Nyberg EP, Hope L (2011) in Causality in the Sciences (Oxford University Press, Oxford).
          li.reference
            | Griffith V, Koch C (2012) Quantifying synergistic mutual information. arXiv preprint arXiv:1205.4265.
          li.reference
            | Wilson RJ (1985) Introduction to Graph Theory, 3/e (Longman Scientific & Technical).
          li.reference
            | Plum F, Posner JB (1982) The Diagnosis of Stupor and Coma (Oxford University Press).
          li.reference
            | Tononi G, Edelman GM (1998) Consciousness and complexity. Science 282: 1846-1851.
          li.reference
            | Gazzaniga MS (2005) Forty-five years of split-brain research and still going strong. Nat Rev Neurosci 6:653–9.
          li.reference
            | Lynn S, Rhue J (1994) Dissociation: Clinical and theoretical perspectives (Guilford Press).
          li.reference
            | Mudrik L, Breska A, Lamy D, Deouell LY (2011) Integration without awareness: expanding the limits of unconscious processing.
            em  Psychol Sci 
            | 22: 764–70.
          li.reference
            | Mudrik, L, Faivre N, Koch S (2013) Information integration in the absence of awareness. Trends in Cognitive Sciences, in press.
          li.reference
            | Glickstein M (2007) What does the cerebellum really do? Curr Biol 17:R824–R827.
          li.reference
            | Schmahmann JD, Weilburg JB, Sherman JC (2007) The neuropsychiatry of the cerebellum---insights from the clinic. Cerebellum 6:254–67.
          li.reference
            | Boyd CAR (2010) Cerebellar agenesis revisited. Brain 133:941–4.
          li.reference
            | Cohen D (1998) Patches of synchronized activity in the cerebellar cortex evoked by mossy-fiber stimulation: Questioning the role of parallel fibers. Proc Natl Acad Sci 95:15032–15036.
          li.reference
            | Bower JM (2002) The Organization of Cerebellar Cortical Circuitry Revisited. Implications for Function. Ann N Y Acad Sci 978:135–155.
          li.reference
            | Sporns O (2010) Networks of the Brain (MIT Press).
          li.reference
            | van den Heuvel MP, Sporns O (2013) An anatomical substrate for integration among functional networks in human cortex. J Neurosci 33:14489–500.
          li.reference
            | Massimini M et al. (2005) Breakdown of cortical effective connectivity during sleep. Science 309:2228–32.
          li.reference
            | 1. Ferrarelli F et al. (2010) Breakdown in cortical effective connectivity during midazolam-induced loss of consciousness. Proc Natl Acad Sci U S A 107:2681–6.
          li.reference
            | Sanes DH, Reh TA, Harris WA (2011) Development of the Nervous System (Academic Press).
          li.reference
            | Sullivan PR (1995) Contentless Consciousness and Information-Processing Theories of Mind. Philos Psychiatry, Psychol 2:51–59.
          li.reference
            | Edelman GM (1989) The Remembered Present: A Biological Theory of Consciousness (Basic Books).
          li.reference
            | Harth E (1993) The creative loop: How the brain makes a mind (Addison-Wesley, Reading, MA).
          li.reference
            | Hofstadter DR (2007) I Am a Strange Loop (Basic Books).
          li.reference
            | Lamme VAF (2003) Why visual attention and awareness are different. Trends Cogn Sci 7:12–18.
          li.reference
            | Imas OA, Ropella KM, Ward BD, Wood JD, Hudetz AG (2005) Volatile anesthetics disrupt frontal-posterior recurrent information transfer at gamma frequencies in rat. Neurosci Lett 387:145–150.
          li.reference
            | Boly M et al. (2012) Connectivity changes underlying spectral EEG changes during propofol-induced loss of consciousness. J Neurosci 32:7082–90.
          li.reference
            | Mashour GA (2013) Cognitive unbinding: A neuroscientific paradigm of general anesthesia and related states of unconsciousness. Neurosci Biobehav Rev.
          li.reference
            | Koch C, Crick F (2001) The zombie within. Nature 411: 893.
          li.reference
            | Boly M et al. (2011) Preserved feedforward but impaired top-down processes in the vegetative state. Science 332:858–62.
          li.reference
            | Cybenko G (1989) Approximation by superpositions of a sigmoidal function. Math Control Signals Syst 2: 303-314.
          li.reference
            | Hornik K, Stinchcombe M, White H (1989) Multilayer feedforward networks are universal approximators. Neural Networks 2: 359-366.
          li.reference
            | Rumelhart D, Hinton G, Williams R (1986) Learning internal representations by error propagation, Parallel distributed processing, 1986. Cambridge, MA.
          li.reference
            | Goldman M (2009) Memory without feedback in a neural network. Neuron 61: 499–501.
          li.reference
            | Dalal N, Triggs B (2005) in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) (IEEE), pp 886–893.
          li.reference
            | Serre T, Wolf L, Bileschi S, Riesenhuber M, Poggio T (2007) Robust object recognition with cortex-like mechanisms. IEEE Trans Pattern Anal Mach Intell 29:411–26.
          li.reference
            | Sung K-K, Poggio T (1998) Example-based learning for view-based human face detection. IEEE Trans Pattern Anal Mach Intell 20:39–51.
          li.reference
            | Zhao W, Chellappa R, Phillips PJ, Rosenfeld A (2003) Face recognition. ACM Comput Surv 35:399–458.
          li.reference
            | Poggio T, Ullman S (2013) Vision: are models of object recognition catching up with the brain? Ann N Y Acad Sci.
          li.reference
            | Crick F, Koch C (1995) Are we aware of neural activity in primary visual cortex? Nature 375: 121–123.
          li.reference
            | Blake R, Logothetis NK (2002) Visual competition. Nat Rev Neurosci 3: 13–21.
          li.reference
            | Tong F (2003) Primary visual cortex and visual awareness. Nat Rev Neurosci 4:219–29.
          li.reference
            | Pollen DA (2008) Fundamental requirements for primary visual perception. Cereb Cortex 18:1991–8.
          li.reference
            | Tononi G, Sporns O, Edelman GM (1996) A complexity measure for selective matching of signals by the brain. Proc Natl Acad Sci U S A 93:3422–3427.
          li.reference
            | Friston K, Kiebel S (2009) Predictive coding under the free-energy principle. Philos Trans R Soc Lond B Biol Sci 364:1211–21.
          li.reference
            | Friston K (2010) The free-energy principle: a unified brain theory? Nat Rev Neurosci 11:127–38.
          li.reference
            | Barrett AB, Seth AK (2011) Practical measures of integrated information for time-series data. PLoS Comput Biol 7:e1001052.
          li.reference
            | Hashmi A, Nere A, Tononi G (2013) Sleep-Dependent Synaptic Down-Selection (II): Single-Neuron Level Benefits for Matching, Selectivity, and Specificity. Front Neurol 4:148.
          li.reference
            | Albantakis L, Hintze A, Koch, C, Adami C, Tononi G (2013) Information Matching---Environment dependent increase in integrated information (<span class="math">\(\Phi\)</span>). European Conference on Complex Systems (ECCS13).
          li.reference
            | Edlund J a et al. (2011) Integrated information increases with fitness in the evolution of animats. PLoS Comput Biol 7:e1002236.
          li.reference
            | Joshi NJ, Tononi G, Koch C (2013) The minimal complexity of adapting agents increases with fitness. PLoS Comput Biol 9:e1003111.
          li.reference
            | https://github.com/Albantakis/iit/tree/IIT-3.0-Program
          li.reference
            | Mormann F, Koch C (2007) Neural Correlates of Consciousness. Scholarpedia 2:1740.
          li.reference
            | Pele O, Werman M (2008) A linear time histogram metric for improved sift matching. Computer Vision-ECCV 2008, 495-508. Springer Berlin Heidelberg.
          li.reference
            | Pele O, Werman M (2009) Fast and robust earth mover’s distances. Computer vision, 2009 IEEE 12th international conference on, 460-467. IEEE.
          li.reference
            | Shannon CE, Weaver W (1949) The mathematical theory of communication (University of Illinois press, Urbana, IL).
          li.reference
            | Cover TM, Thomas JA (2006) Elements of information theory (Wiley-interscience).
        section#figure-legends
          h1.page-header.unnumbered Figure Legends
